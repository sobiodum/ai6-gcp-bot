{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Actions(Enum):\n",
    "    NO_POSITION = 0\n",
    "    LONG = 1\n",
    "    SHORT = 2\n",
    "\n",
    "\n",
    "class MarketSession(Enum):\n",
    "    TOKYO = 0\n",
    "    LONDON = 1\n",
    "    NEW_YORK = 2\n",
    "    OFF_HOURS = 3\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Position:\n",
    "    \"\"\"Represents an open trading position.\"\"\"\n",
    "    type: str  # 'long' or 'short'\n",
    "    entry_price: float\n",
    "    size: float\n",
    "    entry_time: pd.Timestamp\n",
    "    base_currency: str\n",
    "    quote_currency: str\n",
    "    take_profit: Optional[float] = None\n",
    "    stop_loss: Optional[float] = None\n",
    "\n",
    "@dataclass\n",
    "class RewardParams:\n",
    "    \"\"\"Parameters controlling the reward function behavior.\"\"\"\n",
    "    realized_pnl_weight: float = 0.1\n",
    "    unrealized_pnl_weight: float = 0.8\n",
    "    holding_time_threshold: int = 7*12  # hours ok\n",
    "    holding_penalty_factor: float = -0.00001\n",
    "    max_trades_per_day: int = 6 \n",
    "    overtrading_penalty_factor: float = -0.0001\n",
    "    win_rate_threshold: float = 0.4\n",
    "    win_rate_bonus_factor: float = 0.0005\n",
    "    drawdown_penalty_factor: float = -0.0001\n",
    "\n",
    "class ForexTradingEnv(gym.Env):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        pair: str,\n",
    "        initial_balance: float = 1_000_000.0,\n",
    "        trade_size: float = 100_000.0,\n",
    "        max_position_size: float = 1.0,\n",
    "        transaction_cost: float = 0.0001,\n",
    "        reward_scaling: float = 1e-4,\n",
    "        sequence_length: int = 10,\n",
    "        random_start: bool = True,\n",
    "        margin_rate_pct:float = 0.01,\n",
    "        trading_history_size: int = 50,  # Keep track of last 50 trades\n",
    "        reward_params: Optional[RewardParams] = None,\n",
    "    ):\n",
    "        super(ForexTradingEnv, self).__init__()\n",
    "\n",
    "        self.df = df\n",
    "        self.trade_size = trade_size\n",
    "        self.pair = pair\n",
    "        self.base_currency = pair.split('_')[0]\n",
    "        self.quote_currency = pair.split('_')[1]\n",
    "        self.initial_balance = initial_balance\n",
    "        self.balance = self.initial_balance\n",
    "        self.max_position_size = max_position_size\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.reward_scaling = reward_scaling\n",
    "        self.sequence_length = sequence_length\n",
    "        self.random_start = random_start\n",
    "        self.margin_rate_pct = margin_rate_pct\n",
    "        self._last_trade_info = None\n",
    "\n",
    "        # Initialize reward parameters\n",
    "        self.reward_params = reward_params or RewardParams()\n",
    "\n",
    "        # Additional tracking for enhanced observations\n",
    "        self.trading_history_size = trading_history_size\n",
    "        self.trade_history = []  # List of past trade results\n",
    "        self.session_trades = {session: [] for session in MarketSession}\n",
    "        self.peak_balance = initial_balance\n",
    "        self.session_start_balance = initial_balance\n",
    "\n",
    "        # Calculate observation space size including account state\n",
    "        self.feature_columns = [col for col in df.columns\n",
    "                                if col not in ['timestamp', 'volume']]\n",
    "        # Enhanced observation space\n",
    "        self.market_features = len(self.feature_columns)\n",
    "        # Basic account features (balance, position type, size)\n",
    "        self.account_features = 7\n",
    "        # Time in pos, drawdown, dist to SL/TP, ATR ratio, unrealized PnL\n",
    "        self.risk_features = 5\n",
    "        # Hour sin/cos, day sin/cos, session one-hot (3)\n",
    "        self.context_features = 7\n",
    "        # Win ratio, avg PnL, drawdown, trade count, session success\n",
    "        self.history_features = 5\n",
    "\n",
    "        # Define action space (NO_POSITION, LONG, SHORT)\n",
    "        self.action_space = spaces.Discrete(len(Actions))\n",
    "\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'market': spaces.Box(\n",
    "                low=-np.inf,\n",
    "                high=np.inf,\n",
    "                shape=(sequence_length, self.market_features),\n",
    "                dtype=np.float32\n",
    "            ),\n",
    "            'account': spaces.Box(\n",
    "                low=-np.inf,\n",
    "                high=np.inf,\n",
    "                shape=(self.account_features,),\n",
    "                dtype=np.float32\n",
    "            ),\n",
    "            'risk': spaces.Box(\n",
    "                low=-np.inf,\n",
    "                high=np.inf,\n",
    "                shape=(self.risk_features,),\n",
    "                dtype=np.float32\n",
    "            ),\n",
    "            'context': spaces.Box(\n",
    "                low=-np.inf,\n",
    "                high=np.inf,\n",
    "                shape=(self.context_features,),\n",
    "                dtype=np.float32\n",
    "            ),\n",
    "            'history': spaces.Box(\n",
    "                low=-np.inf,\n",
    "                high=np.inf,\n",
    "                shape=(self.history_features,),\n",
    "                dtype=np.float32\n",
    "            )\n",
    "        })\n",
    "\n",
    "        # Initialize state\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None) -> Tuple[Dict[str, np.ndarray], Dict]:\n",
    "        \"\"\"Reset the environment to initial state.\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.balance = self.initial_balance\n",
    "        self.position: Optional[Position] = None\n",
    "        self.current_step = self.sequence_length\n",
    "        self.trade_history = [] \n",
    "\n",
    "        if self.random_start and len(self.df) > self.sequence_length + 100:\n",
    "            self.current_step = np.random.randint(\n",
    "                self.sequence_length,\n",
    "                len(self.df) - 100\n",
    "            )\n",
    "\n",
    "        self.total_pnl = 0.0\n",
    "        self.total_trades = 0\n",
    "        self.winning_trades = 0\n",
    "        self.trade_history = []\n",
    "\n",
    "        return self._get_observation(), self._get_info()\n",
    "\n",
    "    def _print_after_episode(self):\n",
    "        \"\"\"Print episode summary with corrected metrics.\"\"\"\n",
    "        total_return = ((self.balance / self.initial_balance) - 1) * 100\n",
    "        win_rate = (self.winning_trades / max(1, self.total_trades)) * 100\n",
    "        \n",
    "        print(\"\\nEpisode Summary:\")\n",
    "        print(f\"Final Return: {total_return:.2f}%\")\n",
    "        print(f\"Total PnL: {self.total_pnl:.2f}\")\n",
    "        print(f\"Total Trades: {self.total_trades}\")\n",
    "        print(f\"Winning Trades: {self.winning_trades}\")\n",
    "        print(f\"Win Rate: {win_rate:.2f}%\")\n",
    "        print(f\"Initial Balance: {self.initial_balance:.2f}\")\n",
    "        print(f\"Final Balance: {self.balance:.2f}\")\n",
    "        print(\"-\" * 50)\n",
    "        pass \n",
    "\n",
    "    def step(self, action: int) -> Tuple[Dict[str, np.ndarray], float, bool, bool, Dict]:\n",
    "        \"\"\"Execute one step in the environment.\"\"\"\n",
    "        action = Actions(action)\n",
    "        reward = 0.0\n",
    "        # Move to next step / get next price\n",
    "        current_price = self.df.iloc[self.current_step]['close']\n",
    "        self.current_step += 1\n",
    "        prev_price = self.df.iloc[self.current_step-1]['close']\n",
    "        if self.balance == 0 or self.initial_balance == 0:\n",
    "            print(f\"0 Value balance: {self.balance} self.initial_balance: {self.initial_balance} at step: {self.current_step}\")\n",
    "\n",
    "        # Handle position transitions\n",
    "        if action == Actions.NO_POSITION and self.position is not None:\n",
    "            # Close current position\n",
    "            reward = self._calculate_reward(self._close_position(current_price))\n",
    "\n",
    "        elif action == Actions.LONG:\n",
    "            if self.position is None:\n",
    "                # Open long position\n",
    "                self._open_position('long', current_price)\n",
    "                reward = self._calculate_reward()\n",
    "            \n",
    "            elif self.position.type == 'short':\n",
    "                # Close short and open long\n",
    "                reward = self._calculate_reward(self._close_position(current_price))\n",
    "                self._open_position('long', current_price)\n",
    "\n",
    "            elif self.position.type == 'long':\n",
    "                # Maintain long position, calculate reward based on holding\n",
    "                reward = self._calculate_reward()\n",
    "\n",
    "        elif action == Actions.SHORT:\n",
    "            if self.position is None:\n",
    "                # Open short position\n",
    "                self._open_position('short', current_price)\n",
    "                reward = self._calculate_reward()\n",
    "            \n",
    "            elif self.position.type == 'long':\n",
    "                # Close long and open short\n",
    "                reward = self._calculate_reward(self._close_position(current_price))\n",
    "                self._open_position('short', current_price)\n",
    "            \n",
    "            elif self.position.type == 'short':\n",
    "                # Maintain short position, calculate reward based on holding\n",
    "                reward = self._calculate_reward()\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "        # Check if episode is done\n",
    "        terminated = self.current_step >= len(self.df) - 1 or self.balance <= 0\n",
    "        truncated = False\n",
    "        if terminated or truncated:\n",
    "            self._print_after_episode()\n",
    "\n",
    "        return self._get_observation(), reward, terminated, truncated, self._get_info()\n",
    "    \n",
    "    def _get_market_sequence(self) -> np.ndarray:\n",
    "        \"\"\"Get the market data sequence with padding if needed.\"\"\"\n",
    "        if self.current_step >= len(self.df):\n",
    "            raise IndexError(\"Current step exceeds dataset length\")\n",
    "        start_idx = self.current_step - self.sequence_length\n",
    "        end_idx = self.current_step\n",
    "        \n",
    "        # Handle the case where we don't have enough history\n",
    "        if start_idx < 0:\n",
    "            # Create padding\n",
    "            pad_length = abs(start_idx)\n",
    "            market_data = self.df.iloc[0:end_idx][self.feature_columns].values\n",
    "            padding = np.zeros((pad_length, len(self.feature_columns)))\n",
    "            market_obs = np.vstack([padding, market_data])\n",
    "        else:\n",
    "            market_obs = self.df.iloc[start_idx:end_idx][self.feature_columns].values\n",
    "            \n",
    "        return market_obs\n",
    "\n",
    "    def _get_account_state(self) -> np.ndarray:\n",
    "        \"\"\"Calculate the current account state features.\"\"\"\n",
    "        # Initialize with zeros\n",
    "        position_type = 0.0  # No position\n",
    "        position_size = 0.0\n",
    "        unrealized_pnl = 0.0\n",
    "        \n",
    "        # Update if position exists\n",
    "        if self.position is not None:\n",
    "            # Position type: 1 for long, -1 for short\n",
    "            position_type = 1.0 if self.position.type == 'long' else -1.0\n",
    "            \n",
    "            # Normalized position size\n",
    "            position_size = self.position.size / self.initial_balance\n",
    "            \n",
    "            # Calculate unrealized PnL\n",
    "            current_price = self.df.iloc[self.current_step]['close']\n",
    "            unrealized_pnl = self._calculate_pnl(\n",
    "                self.position.type,\n",
    "                self.position.entry_price,\n",
    "                current_price,\n",
    "                self.position.size\n",
    "            ) / self.initial_balance  # Normalize by initial balance\n",
    "        \n",
    "        return np.array([\n",
    "            self.balance / self.initial_balance,  # Normalized balance\n",
    "            position_type,  # Position direction\n",
    "            position_size,  # Normalized position size\n",
    "            unrealized_pnl,  # Normalized unrealized PnL\n",
    "            self.total_pnl / self.initial_balance,  # Normalized total PnL\n",
    "            self.total_trades / 1000.0,  # Normalized trade count (assuming max 1000 trades)\n",
    "            self.winning_trades / max(1, self.total_trades)  # Win rate\n",
    "        ])\n",
    "\n",
    "    def _get_observation(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Construct enhanced observation with additional features.\"\"\"\n",
    "        current_time = self.df.index[self.current_step]\n",
    "        current_price = self.df.iloc[self.current_step]['close']\n",
    "\n",
    "        # 1. Market data sequence (with padding if needed)\n",
    "        market_obs = self._get_market_sequence()\n",
    "\n",
    "        # 2. Account state\n",
    "        account_obs = self._get_account_state()\n",
    "\n",
    "        # 3. Risk metrics\n",
    "        risk_obs = self._get_risk_metrics(current_price)\n",
    "\n",
    "        # 4. Market context\n",
    "        context_obs = self._get_market_context(current_time)\n",
    "\n",
    "        # 5. Trading history\n",
    "        history_obs = self._get_trading_history()\n",
    "\n",
    "        return {\n",
    "            'market': market_obs.astype(np.float32),\n",
    "            'account': account_obs.astype(np.float32),\n",
    "            'risk': risk_obs.astype(np.float32),\n",
    "            'context': context_obs.astype(np.float32),\n",
    "            'history': history_obs.astype(np.float32)\n",
    "        }\n",
    "\n",
    "    def _get_risk_metrics(self, current_price: float) -> np.ndarray:\n",
    "        \"\"\"Calculate risk-related metrics.\"\"\"\n",
    "        if self.position is None:\n",
    "            return np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "        # Time in position (normalized by typical holding period, e.g., 24 hours)\n",
    "        time_in_pos = (self.df.index[self.current_step] -\n",
    "                       self.position.entry_time).total_seconds() / (24 * 3600)\n",
    "\n",
    "        # Current drawdown from peak balance\n",
    "        drawdown = (self.peak_balance - self.balance) / self.peak_balance\n",
    "\n",
    "        # Distance to stop loss/take profit (if set)\n",
    "        if self.position.stop_loss:\n",
    "            dist_to_sl = abs(\n",
    "                current_price - self.position.stop_loss) / current_price\n",
    "        else:\n",
    "            dist_to_sl = 1.0  # No stop loss set\n",
    "\n",
    "        # ATR ratio to position size\n",
    "        atr = self.df.iloc[self.current_step]['atr']\n",
    "        if self.balance > 0:\n",
    "            atr_ratio = atr * self.position.size / self.balance\n",
    "        else:\n",
    "            atr_ratio = 0.0\n",
    "\n",
    "        # Unrealized PnL (normalized by position size)\n",
    "        unrealized_pnl = self._calculate_pnl(\n",
    "            self.position.type,\n",
    "            self.position.entry_price,\n",
    "            current_price,\n",
    "            self.position.size\n",
    "        ) / self.position.size\n",
    "\n",
    "        return np.array([\n",
    "            time_in_pos,\n",
    "            drawdown,\n",
    "            dist_to_sl,\n",
    "            atr_ratio,\n",
    "            unrealized_pnl\n",
    "        ])\n",
    "\n",
    "    def _get_market_context(self, current_time: pd.Timestamp) -> np.ndarray:\n",
    "        \"\"\"Calculate market context features.\"\"\"\n",
    "        # Hour encoding (sin/cos for cyclical nature)\n",
    "        hour = current_time.hour + current_time.minute / 60.0\n",
    "        hour_sin = np.sin(2 * np.pi * hour / 24.0)\n",
    "        hour_cos = np.cos(2 * np.pi * hour / 24.0)\n",
    "\n",
    "        # Day of week encoding\n",
    "        day = current_time.weekday()\n",
    "        day_sin = np.sin(2 * np.pi * day / 7.0)\n",
    "        day_cos = np.cos(2 * np.pi * day / 7.0)\n",
    "\n",
    "        # Market session one-hot encoding\n",
    "        session = self._get_market_session(current_time)\n",
    "        session_encoding = np.zeros(3)  # Tokyo, London, NY\n",
    "        if session != MarketSession.OFF_HOURS:\n",
    "            session_encoding[session.value] = 1.0\n",
    "\n",
    "        return np.concatenate([\n",
    "            [hour_sin, hour_cos, day_sin, day_cos],\n",
    "            session_encoding\n",
    "        ])\n",
    "\n",
    "    def _get_trading_history(self) -> np.ndarray:\n",
    "        \"\"\"Calculate trading history metrics.\"\"\"\n",
    "        if not self.trade_history:\n",
    "            return np.zeros(5)\n",
    "\n",
    "        recent_trades = self.trade_history[-self.trading_history_size:]\n",
    "\n",
    "        # Overall win ratio - check PnL field in trade dictionaries\n",
    "        win_ratio = sum(1 for t in recent_trades if t['pnl'] > 0) / len(recent_trades)\n",
    "\n",
    "        # Average PnL\n",
    "        avg_pnl = np.mean([t['pnl'] for t in recent_trades]) / self.initial_balance\n",
    "\n",
    "        # Maximum drawdown in current session\n",
    "        session_drawdown = (self.session_start_balance - \n",
    "                            self.balance) / self.session_start_balance\n",
    "\n",
    "        # Number of trades in current session (normalized)\n",
    "        current_session = self._get_market_session(\n",
    "            self.df.index[self.current_step])\n",
    "        # Normalize by expected max trades per session\n",
    "        session_trade_count = len(self.session_trades[current_session]) / 20.0\n",
    "\n",
    "        # Success rate in current session type\n",
    "        session_trades = self.session_trades[current_session]\n",
    "        if session_trades:\n",
    "            session_success = sum(1 for t in session_trades \n",
    "                                if t['pnl'] > 0) / len(session_trades)\n",
    "        else:\n",
    "            session_success = 0.0\n",
    "\n",
    "        return np.array([\n",
    "            win_ratio,\n",
    "            avg_pnl,\n",
    "            session_drawdown,\n",
    "            session_trade_count,\n",
    "            session_success\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def _get_market_session(self, timestamp: pd.Timestamp) -> MarketSession:\n",
    "        \"\"\"Determine current market session.\"\"\"\n",
    "        hour = timestamp.hour\n",
    "\n",
    "        # Convert to UTC+9 for Tokyo\n",
    "        tokyo_hour = (hour + 9) % 24\n",
    "        if 9 <= tokyo_hour < 15:\n",
    "            return MarketSession.TOKYO\n",
    "\n",
    "        # London session (UTC+0)\n",
    "        if 8 <= hour < 16:\n",
    "            return MarketSession.LONDON\n",
    "\n",
    "        # New York session (UTC-4)\n",
    "        ny_hour = (hour - 4) % 24\n",
    "        if 8 <= ny_hour < 17:\n",
    "            return MarketSession.NEW_YORK\n",
    "\n",
    "        return MarketSession.OFF_HOURS\n",
    "\n",
    "    def _on_trade_closed(self, pnl: float) -> None:\n",
    "        \"\"\"Update trade history when a position is closed.\"\"\"\n",
    "        if self.position is None:\n",
    "            return\n",
    "            \n",
    "        current_time = self.df.index[self.current_step]\n",
    "        current_price = self.df.iloc[self.current_step]['close']\n",
    "        \n",
    "        trade_info = {\n",
    "            'pnl': pnl,\n",
    "            'type': self.position.type,\n",
    "            'entry_price': self.position.entry_price,\n",
    "            'exit_price': current_price,\n",
    "            'trade_closed': True,\n",
    "            'size': self.position.size,\n",
    "            'entry_time': self.position.entry_time,\n",
    "            'exit_time': current_time,\n",
    "            'duration': (current_time - self.position.entry_time).total_seconds() / 3600,\n",
    "            'session': self._get_market_session(current_time),\n",
    "  \n",
    "        }\n",
    "        \n",
    "        self.trade_history.append(trade_info)\n",
    "        if len(self.trade_history) > self.trading_history_size:\n",
    "            self.trade_history.pop(0)\n",
    "\n",
    "        current_session = self._get_market_session(current_time)\n",
    "        self.session_trades[current_session].append(trade_info)\n",
    "\n",
    "        # Update peak balance\n",
    "        self.peak_balance = max(self.peak_balance, self.balance)\n",
    "\n",
    "    def _calculate_pnl(\n",
    "        self,\n",
    "        position_type: str,\n",
    "        entry_price: float,\n",
    "        exit_price: float,\n",
    "        position_size: float\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Calculate PnL in base currency terms.\n",
    "\n",
    "        For example:\n",
    "        - EUR/USD: PnL in EUR\n",
    "        - USD/JPY: PnL in USD\n",
    "        \"\"\"\n",
    "        if position_type == 'long':\n",
    "            # Convert PnL to base currency\n",
    "            if self.quote_currency == 'USD':\n",
    "                # For pairs like EUR/USD, convert USD PnL to base currency (EUR)\n",
    "                pnl = (exit_price - entry_price) * position_size / exit_price\n",
    "            else:\n",
    "                # For pairs like USD/JPY, PnL is already in base currency (USD)\n",
    "                pnl = (exit_price - entry_price) * position_size\n",
    "        else:  # short\n",
    "            if self.quote_currency == 'USD':\n",
    "                pnl = (entry_price - exit_price) * position_size / exit_price\n",
    "            else:\n",
    "                pnl = (entry_price - exit_price) * position_size\n",
    "\n",
    "        return pnl\n",
    "\n",
    "    def _open_position(self, position_type: str, current_price: float) -> None:\n",
    "        \"\"\"Open a new position.\"\"\"\n",
    "        position_size = self.balance * self.max_position_size\n",
    "        entry_price = current_price\n",
    "\n",
    "        # Add transaction costs\n",
    "        if position_type == 'long':\n",
    "            entry_price += self.transaction_cost\n",
    "        else:\n",
    "            entry_price -= self.transaction_cost\n",
    "\n",
    "        self.position = Position(\n",
    "            type=position_type,\n",
    "            entry_price=entry_price,\n",
    "            size=self.trade_size,\n",
    "            entry_time=self.df.index[self.current_step],\n",
    "            base_currency=self.base_currency,\n",
    "            quote_currency=self.quote_currency\n",
    "        )\n",
    "\n",
    "        required_margin = self.trade_size * self.margin_rate_pct  # 1% margin requirement\n",
    "      \n",
    "\n",
    "    def _close_position(self, current_price: float) -> float:\n",
    "        \"\"\"Close current position and return reward.\"\"\"\n",
    "        if not self.position:\n",
    "            return 0.0\n",
    "\n",
    "        # Calculate PnL with transaction costs\n",
    "        exit_price = current_price\n",
    "        if self.position.type == 'long':\n",
    "            exit_price -= self.transaction_cost\n",
    "        else:\n",
    "            exit_price += self.transaction_cost\n",
    "\n",
    "        pnl = self._calculate_pnl(\n",
    "            self.position.type,\n",
    "            self.position.entry_price,\n",
    "            exit_price,\n",
    "            self.position.size\n",
    "        )\n",
    "\n",
    "        self._last_trade_info = {\n",
    "            'trade_closed': True,  # Must be True to trigger trade recording\n",
    "            'trade_pnl': pnl,\n",
    "            'entry_time': self.position.entry_time,\n",
    "            'exit_time': self.df.index[self.current_step],\n",
    "            'entry_price': self.position.entry_price,\n",
    "            'exit_price': exit_price,\n",
    "            'position_type': self.position.type,\n",
    "            'position_size': self.position.size,\n",
    "            'market_state': {\n",
    "                'session': self._get_market_session(self.df.index[self.current_step]).name,\n",
    "                'balance': self.balance,\n",
    "                'total_trades': self.total_trades,\n",
    "                'win_rate': self.winning_trades / max(1, self.total_trades)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Update metrics\n",
    "        self.total_pnl += pnl\n",
    "        self.balance += pnl\n",
    "        self.total_trades += 1\n",
    "        if pnl > 0:\n",
    "            self.winning_trades += 1\n",
    "\n",
    "        # Call _on_trade_closed before clearing position\n",
    "        self._on_trade_closed(pnl)  \n",
    "        # Clear position\n",
    "        self.position = None\n",
    "\n",
    "        return pnl * self.reward_scaling\n",
    "    \n",
    "\n",
    " \n",
    "    def _calculate_reward(self, realized_pnl: float = 0.0) -> float:\n",
    "        \"\"\"\n",
    "        Calculate reward based on multiple factors:\n",
    "        1. Realized PnL from closed trades\n",
    "        2. Unrealized PnL from open positions\n",
    "        3. Risk-adjusted returns (Sharpe-like ratio)\n",
    "        4. Position holding costs\n",
    "        5. Trade efficiency metrics\n",
    "        \n",
    "        Returns:\n",
    "            float: Calculated reward\n",
    "        \"\"\"\n",
    "        reward = 0.0\n",
    "        current_price = self.df.iloc[self.current_step]['close']\n",
    "        \n",
    "        # 1. Realized PnL component\n",
    "        if realized_pnl != 0:\n",
    "            normalized_pnl = realized_pnl / self.trade_size\n",
    "            reward += normalized_pnl * (1 + (self.reward_params.realized_pnl_weight if realized_pnl > 0 else 0))\n",
    "            \n",
    "      \n",
    "            \n",
    "            # Calculate win rate bonus\n",
    "            # if self.total_trades > 0:\n",
    "            #     win_rate = self.winning_trades / self.total_trades\n",
    "            #     reward += win_rate * 0.1  # Small bonus for maintaining good win rate\n",
    "        \n",
    "        # 2. Unrealized PnL component for open positions\n",
    "        if self.position is not None:\n",
    "            unrealized_pnl = self._calculate_pnl(\n",
    "                self.position.type,\n",
    "                self.position.entry_price,\n",
    "                current_price,\n",
    "                self.position.size\n",
    "            )\n",
    "            \n",
    "            normalized_unrealized = unrealized_pnl / self.trade_size\n",
    "        \n",
    "            # Add scaled unrealized PnL (smaller weight than realized)\n",
    "            reward += normalized_unrealized * self.reward_params.unrealized_pnl_weight\n",
    "            \n",
    "            # Add holding cost penalty (larger for longer-held positions)\n",
    "            holding_hours = (self.df.index[self.current_step] - \n",
    "                        self.position.entry_time).total_seconds() / 3600  # in hours\n",
    "            # Stronger penalty for very long holds\n",
    "   \n",
    "            if holding_hours > self.reward_params.holding_time_threshold:  # Penalize holds over x hours\n",
    "                holding_penalty = self.reward_params.holding_penalty_factor * (holding_hours - self.reward_params.holding_time_threshold)\n",
    "                reward += holding_penalty\n",
    "        \n",
    "            # 3. Anti-overtrading penalty\n",
    "            if self.total_trades > 0:\n",
    "                # Calculate trades per day\n",
    "                total_days = (self.df.index[self.current_step] - \n",
    "                            self.df.index[0]).total_seconds() / (24 * 3600)\n",
    "                trades_per_day = self.total_trades / max(1, total_days)\n",
    "                \n",
    "                # Penalty for excessive trading (more than 6 trades per day)\n",
    "                if trades_per_day > self.reward_params.max_trades_per_day:\n",
    "                    overtrading_penalty = self.reward_params.overtrading_penalty_factor * (trades_per_day - self.reward_params.max_trades_per_day)\n",
    "                    reward += overtrading_penalty\n",
    "   \n",
    "            # 4. Win rate Linear increase in bonus above 40% win rate\n",
    "            min_trades_required = 10\n",
    "            if self.total_trades >= min_trades_required:\n",
    "                win_rate = self.winning_trades / self.total_trades\n",
    "                # Linear scaling between 40% and 60% win rate\n",
    "                win_rate_bonus = max(0, (win_rate - self.reward_params.win_rate_threshold) * self.reward_params.win_rate_bonus_factor)\n",
    "                reward += win_rate_bonus\n",
    "                \n",
    "            # 5. Risk management penalty (progressive with drawdown)\n",
    "            if self.balance < self.initial_balance:\n",
    "                drawdown_pct = (self.initial_balance - self.balance) / self.initial_balance\n",
    "                # Linear penalty that increases with drawdown\n",
    "                risk_penalty = self.reward_params.drawdown_penalty_factor * (drawdown_pct * 100) ** 2\n",
    "                reward += risk_penalty\n",
    "\n",
    "        return float(reward)\n",
    "    \n",
    "    def _get_info(self) -> Dict:\n",
    "        \"\"\"Get current state information and performance metrics.\"\"\"\n",
    "        current_price = self.df.iloc[self.current_step]['close']\n",
    "        \n",
    "        # Calculate unrealized PnL if position exists\n",
    "        unrealized_pnl = 0.0\n",
    "        position_duration = 0\n",
    "        position_type = 'none'\n",
    "        \n",
    "        if self.position is not None:\n",
    "            position_type = self.position.type\n",
    "            unrealized_pnl = self._calculate_pnl(\n",
    "                self.position.type,\n",
    "                self.position.entry_price,\n",
    "                current_price,\n",
    "                self.position.size\n",
    "            )\n",
    "            position_duration = (self.df.index[self.current_step] - \n",
    "                            self.position.entry_time).total_seconds() / 3600  # Convert to hours\n",
    "        \n",
    "        # Calculate drawdown\n",
    "        peak_balance = max(self.peak_balance, self.balance + unrealized_pnl)\n",
    "        current_balance = self.balance + unrealized_pnl\n",
    "        drawdown = (peak_balance - current_balance) / peak_balance if peak_balance > 0 else 0.0\n",
    "        info= {\n",
    "            # Account metrics\n",
    "            'balance': self.balance,\n",
    "            'total_pnl': self.total_pnl,\n",
    "            'unrealized_pnl': unrealized_pnl,\n",
    "            'total_trades': self.total_trades,\n",
    "            'trade_count': self.total_trades,\n",
    "            'win_rate': self.winning_trades / max(1, self.total_trades),\n",
    "            'drawdown': drawdown,\n",
    "            \n",
    "            # Position info\n",
    "            'position_type': position_type,\n",
    "            'position_size': self.position.size if self.position else 0.0,\n",
    "            'position_duration': position_duration,\n",
    "            \n",
    "            # Trading costs and metrics\n",
    "            'trading_costs': self.transaction_cost * (self.position.size if self.position else 0.0),\n",
    "            'avg_trade_pnl': self.total_pnl / max(1, self.total_trades),\n",
    "            \n",
    "            # Episode progress\n",
    "            'current_step': self.current_step,\n",
    "            'total_steps': len(self.df),\n",
    "            'timestamp': self.df.index[self.current_step],\n",
    "            \n",
    "            # Market info\n",
    "            'current_price': current_price,\n",
    "            'spread': self.df.iloc[self.current_step].get('spread', self.transaction_cost)\n",
    "        }\n",
    "        if self._last_trade_info is not None:\n",
    "            info.update(self._last_trade_info)\n",
    "            self._last_trade_info = None\n",
    "        return info\n",
    "    \n",
    "    @property\n",
    "    def win_rate(self) -> float:\n",
    "        \"\"\"Calculate win rate.\"\"\"\n",
    "        return self.winning_trades / max(1, self.total_trades)\n",
    "\n",
    "    @property\n",
    "    def avg_trade_duration(self) -> float:\n",
    "        \"\"\"Calculate average trade duration.\"\"\"\n",
    "        if not self.trade_history:\n",
    "            return 0.0\n",
    "        return sum(t['duration'] for t in self.trade_history) / len(self.trade_history)\n",
    "\n",
    "    @property\n",
    "    def max_drawdown(self) -> float:\n",
    "        \"\"\"Calculate maximum drawdown.\"\"\"\n",
    "        if self.peak_balance <= 0:\n",
    "            return 0.0\n",
    "        return (self.peak_balance - self.balance) / self.peak_balance\n",
    "\n",
    "    @property\n",
    "    def position_ratios(self) -> Dict[str, float]:\n",
    "        \"\"\"Calculate position type ratios.\"\"\"\n",
    "        if not self.trade_history:\n",
    "            return {'long': 0.0, 'short': 0.0, 'none': 1.0}\n",
    "        \n",
    "        total = len(self.trade_history)\n",
    "        longs = sum(1 for t in self.trade_history if t.get('type') == 'long')\n",
    "        shorts = sum(1 for t in self.trade_history if t.get('type') == 'short')\n",
    "        \n",
    "        return {\n",
    "            'long': longs / total,\n",
    "            'short': shorts / total,\n",
    "            'none': (total - longs - shorts) / total\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('./EUR_USD.parquet')\n",
    "\n",
    "def split_dataset(\n",
    "        df: pd.DataFrame, \n",
    "        train_ratio: float = 0.7,\n",
    "        val_ratio: float = 0.15,\n",
    "        test_ratio: float = 0.15,\n",
    "        shuffle: bool = False\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Split dataset into train, validation and test sets.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            train_ratio: Proportion for training (default: 0.7)\n",
    "            val_ratio: Proportion for validation (default: 0.15)\n",
    "            test_ratio: Proportion for testing (default: 0.15)\n",
    "            shuffle: Whether to shuffle before splitting (default: False for time series)\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (train_df, val_df, test_df)\n",
    "        \"\"\"\n",
    "        assert np.isclose(train_ratio + val_ratio + test_ratio, 1.0), \"Ratios must sum to 1\"\n",
    "        \n",
    "        n = len(df)\n",
    "        indices = np.arange(n)\n",
    "        \n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        \n",
    "        train_idx = int(n * train_ratio)\n",
    "        val_idx = int(n * (train_ratio + val_ratio))\n",
    "        \n",
    "        train_df = df.iloc[indices[:train_idx]]\n",
    "        val_df = df.iloc[indices[train_idx:val_idx]]\n",
    "        test_df = df.iloc[indices[val_idx:]]\n",
    "        \n",
    "        # Sort by index again if shuffled\n",
    "        if shuffle:\n",
    "            train_df = train_df.sort_index()\n",
    "            val_df = val_df.sort_index()\n",
    "            test_df = test_df.sort_index()\n",
    "        \n",
    "        print(f\"Dataset split sizes:\")\n",
    "        print(f\"Training: {len(train_df)} samples ({len(train_df)/n:.1%})\")\n",
    "        print(f\"Validation: {len(val_df)} samples ({len(val_df)/n:.1%})\")\n",
    "        print(f\"Test: {len(test_df)} samples ({len(test_df)/n:.1%})\")\n",
    "        \n",
    "        return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from dataclasses import dataclass\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import os, sys\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "log_file = \"optuna_trials.log\"  # Path to log file\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()  # For console output\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RewardParams:\n",
    "    \"\"\"Parameters controlling the reward function behavior.\"\"\"\n",
    "    realized_pnl_weight: float = 1.1\n",
    "    unrealized_pnl_weight: float = 0.8\n",
    "    holding_time_threshold: int = 7*12  # hours\n",
    "    holding_penalty_factor: float = -0.00001\n",
    "    max_trades_per_day: int = 6 \n",
    "    overtrading_penalty_factor: float = -0.0001\n",
    "    win_rate_threshold: float = 0.4\n",
    "    win_rate_bonus_factor: float = 0.0005\n",
    "    drawdown_penalty_factor: float = -0.0001\n",
    "\n",
    "@dataclass\n",
    "class OptimizationResult:\n",
    "    \"\"\"Stores results of a single trial.\"\"\"\n",
    "    trial_number: int\n",
    "    params: Dict\n",
    "    final_balance: float\n",
    "    total_trades: int\n",
    "    win_rate: float\n",
    "    max_drawdown: float\n",
    "    training_time: float\n",
    "\n",
    "class RewardOptimizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        val_df: pd.DataFrame,\n",
    "        study_name: str = \"forex_reward_optimization1\",\n",
    "        n_timesteps: int = 500_000\n",
    "    ):\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.study_name = study_name\n",
    "        self.n_timesteps = n_timesteps\n",
    "        \n",
    "        # Setup study with TPE sampler and Median pruner\n",
    "        self.study = optuna.create_study(\n",
    "            study_name=study_name,\n",
    "            storage=\"sqlite:///optuna_trials.db\",\n",
    "            load_if_exists=True,\n",
    "            sampler=TPESampler(seed=42),\n",
    "            pruner=MedianPruner(\n",
    "                n_startup_trials=5,  # Wait for at least 5 trials to complete before pruning\n",
    "                n_warmup_steps=50_000,  # Let each trial run for 50k timesteps before pruning\n",
    "                interval_steps=50_000\n",
    "            ),\n",
    "            direction=\"maximize\"\n",
    "        )\n",
    "\n",
    "    def _create_env(self, df: pd.DataFrame, params: Dict, is_eval: bool = False) -> VecNormalize:\n",
    "        \"\"\"Create vectorized and normalized environment.\"\"\"\n",
    "        def make_env():\n",
    "            def _init():\n",
    "                env = ForexTradingEnv(\n",
    "                    df=df.copy(),\n",
    "                    pair='EUR_USD',\n",
    "                    initial_balance=1_000_000,\n",
    "                    trade_size=100_000,\n",
    "                    reward_params=RewardParams(**params)\n",
    "                )\n",
    "                return Monitor(env)\n",
    "            return _init\n",
    "\n",
    "        vec_env = DummyVecEnv([make_env()])\n",
    "        env = VecNormalize(\n",
    "            vec_env,\n",
    "            norm_obs=True,\n",
    "            norm_reward=not is_eval,\n",
    "            clip_obs=10.,\n",
    "            clip_reward=10.,\n",
    "            gamma=0.99,\n",
    "            epsilon=1e-08\n",
    "        )\n",
    "\n",
    "        return env\n",
    "\n",
    "    def objective(self, trial: optuna.Trial) -> float:\n",
    "        \"\"\"Optimization objective function.\"\"\"\n",
    "        try:\n",
    "            # Sample parameters\n",
    "            params = self._sample_parameters(trial)\n",
    "            \n",
    "            # Create environments\n",
    "            train_env = self._create_env(self.train_df, params)\n",
    "            eval_env = self._create_env(self.val_df, params, is_eval=True)\n",
    "            \n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            # Create model\n",
    "            model = PPO(\n",
    "                \"MultiInputPolicy\",\n",
    "                train_env,\n",
    "                verbose=0,\n",
    "                tensorboard_log=f\"./tensorboard/trial_{trial.number}\"\n",
    "            )\n",
    "\n",
    "            # Setup evaluation callback\n",
    "            eval_callback = EvalCallback(\n",
    "                eval_env,\n",
    "                best_model_save_path=f\"./models/trial_{trial.number}\",\n",
    "                log_path=f\"./logs/trial_{trial.number}\",\n",
    "                eval_freq=25_000,\n",
    "                deterministic=True,\n",
    "                render=False\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            model.learn(\n",
    "                total_timesteps=self.n_timesteps,\n",
    "                callback=eval_callback\n",
    "            )\n",
    "            train_env.save(f'./optuna/best_model_trial_{trial.number}/vecnormalize.pkl')\n",
    "            training_time = (datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "            # Get final balance from eval environment\n",
    "            final_balance = eval_env.get_attr('balance')[0]\n",
    "            total_trades = eval_env.get_attr('total_trades')[0]\n",
    "            win_rate = eval_env.get_attr('winning_trades')[0] / max(1, total_trades)\n",
    "            \n",
    "            # Ca# Log trial results\n",
    "            logging.info(f\"Trial {trial.number} completed:\")\n",
    "            logging.info(f\"Final Balance: ${final_balance:,.2f}\")\n",
    "            logging.info(f\"Parameters: {params}\")\n",
    "            logging.info(f\"Total Trades: {total_trades}\")\n",
    "            logging.info(f\"Win Rate: {win_rate:.2%}\")\n",
    "            logging.info(f\"Training Time: {training_time:.1f}s\")\n",
    "            logging.info(\"-\" * 80)\n",
    "\n",
    "            # Print trial results\n",
    "            print(f\"\\nTrial {trial.number} completed:\")\n",
    "            print(f\"Final Balance: ${final_balance:,.2f}\")\n",
    "            print(f\"Parameters:\")\n",
    "            for key, value in params.items():\n",
    "                print(f\"    {key}: {value}\")\n",
    "            print(f\"Total Trades: {total_trades}\")\n",
    "            print(f\"Win Rate: {win_rate:.2%}\")\n",
    "            print(f\"Training Time: {training_time:.1f}s\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            return final_balance\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Trial {trial.number} failed: {str(e)}\")\n",
    "            return float('-inf')\n",
    "\n",
    "    def _sample_parameters(self, trial: optuna.Trial) -> Dict:\n",
    "        \"\"\"Sample reward parameters for trial.\"\"\"\n",
    "        return {\n",
    "            'realized_pnl_weight': trial.suggest_float('realized_pnl_weight', 0.5, 2.0),\n",
    "            'unrealized_pnl_weight': trial.suggest_float('unrealized_pnl_weight', 0.3, 1.0),\n",
    "            'holding_time_threshold': trial.suggest_int('holding_time_threshold', 24, 96),\n",
    "            'holding_penalty_factor': trial.suggest_float('holding_penalty_factor', -0.0001, 0.0),\n",
    "            'max_trades_per_day': trial.suggest_int('max_trades_per_day', 3, 12),\n",
    "            'overtrading_penalty_factor': trial.suggest_float('overtrading_penalty_factor', -0.001, 0.0),\n",
    "            'win_rate_threshold': trial.suggest_float('win_rate_threshold', 0.3, 0.5),\n",
    "            'win_rate_bonus_factor': trial.suggest_float('win_rate_bonus_factor', 0.0001, 0.001, log=True),\n",
    "            'drawdown_penalty_factor': trial.suggest_float('drawdown_penalty_factor', -0.001, 0.0)\n",
    "        }\n",
    "\n",
    "    def optimize(self, n_trials: int = 100, n_jobs: int = 6) -> None:\n",
    "        \"\"\"Run optimization using Optuna's built-in parallelization.\"\"\"\n",
    "        self.study.optimize(\n",
    "            self.objective,\n",
    "            n_trials=n_trials,\n",
    "            n_jobs=n_jobs,  # Number of parallel jobs\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "            \n",
    "        # Print best trial after completion\n",
    "        print(\"\\nOptimization completed!\")\n",
    "        print(\"\\nBest trial:\")\n",
    "        trial = self.study.best_trial\n",
    "        print(f\"Value: ${trial.value:,.2f}\")\n",
    "        print(\"Best parameters:\")\n",
    "        for key, value in trial.params.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "\n",
    "\n",
    "optimizer = RewardOptimizer(\n",
    "    train_df=train_df,\n",
    "    val_df=val_df,\n",
    "    n_timesteps=300_000\n",
    ")\n",
    "\n",
    "optimizer.optimize(n_trials=100, n_jobs=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SB5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
