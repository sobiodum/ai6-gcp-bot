{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split sizes:\n",
      "Training: 101768 samples (70.0%)\n",
      "Validation: 21808 samples (15.0%)\n",
      "Test: 21808 samples (15.0%)\n",
      "Data splits:\n",
      "Training: 101768 samples\n",
      "Validation: 21808 samples\n",
      "Test: 21808 samples\n",
      "\n",
      "Starting model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/floriankockler/Code/anaconda_env/SB5/lib/python3.9/site-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce4b7c2881c48cb95304817f63010d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "from trading.model_manager import ModelManager\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "\n",
    "project_dir = Path.cwd() / \"forex_models\"  # Create in current working directory\n",
    "project_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 1. Load and prepare data\n",
    "dataset_manager = DatasetManager()\n",
    "\n",
    "pair = \"EUR_USD\"\n",
    "parquet_path = Path(\"/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h\") / f\"{pair}.parquet\"\n",
    "df = pd.read_parquet(parquet_path)\n",
    "\n",
    "\n",
    "train_df, val_df, test_df = dataset_manager.split_dataset(df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)\n",
    "\n",
    "print(f\"Data splits:\")\n",
    "print(f\"Training: {len(train_df)} samples\")\n",
    "print(f\"Validation: {len(val_df)} samples\")\n",
    "print(f\"Test: {len(test_df)} samples\")\n",
    "\n",
    "# 3. Initialize model manager\n",
    "model_manager = ModelManager(\n",
    "    base_path=str(project_dir),\n",
    "    n_envs=1,\n",
    "    verbose=0\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# 5. Train model\n",
    "\n",
    "print(\"\\nStarting model training...\")\n",
    "model, metrics = model_manager.train_model(\n",
    "    df=train_df,\n",
    "    pair=pair,\n",
    "    total_timesteps=1_000_000,  # Start with smaller number for testing\n",
    "    eval_freq=50_000\n",
    ")\n",
    "\n",
    "# 6. Evaluate on validation set\n",
    "print(\"\\nEvaluating on validation set...\")\n",
    "val_metrics = model_manager.evaluate_model(\n",
    "    model=model,\n",
    "    df=val_df,\n",
    "    pair=pair,\n",
    "    n_evaluations=5\n",
    ")\n",
    "\n",
    "print(\"\\nValidation Metrics:\")\n",
    "print(f\"Total PnL: {val_metrics.total_pnl:.2f}\")\n",
    "print(f\"Win Rate: {val_metrics.win_rate:.2%}\")\n",
    "print(f\"Sharpe Ratio: {val_metrics.sharpe_ratio:.2f}\")\n",
    "print(f\"Max Drawdown: {val_metrics.max_drawdown:.2%}\")\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "from trading.environments.forex_env import ForexTradingEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Training mainly to check if ENV works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split sizes:\n",
      "Training: 101768 samples (70.0%)\n",
      "Validation: 21808 samples (15.0%)\n",
      "Test: 21808 samples (15.0%)\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -5.36%\n",
      "Total PnL: -53590.60\n",
      "Total Trades: 3308\n",
      "Winning Trades: 1218\n",
      "Win Rate: 36.82%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 946409.40\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -6.97%\n",
      "Total PnL: -69664.18\n",
      "Total Trades: 12919\n",
      "Winning Trades: 5147\n",
      "Win Rate: 39.84%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 930335.82\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -8.76%\n",
      "Total PnL: -87585.20\n",
      "Total Trades: 14464\n",
      "Winning Trades: 5842\n",
      "Win Rate: 40.39%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 912414.80\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -0.19%\n",
      "Total PnL: -1928.39\n",
      "Total Trades: 373\n",
      "Winning Trades: 151\n",
      "Win Rate: 40.48%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 998071.61\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -0.83%\n",
      "Total PnL: -8275.67\n",
      "Total Trades: 589\n",
      "Winning Trades: 207\n",
      "Win Rate: 35.14%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 991724.33\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -0.03%\n",
      "Total PnL: -303.61\n",
      "Total Trades: 116\n",
      "Winning Trades: 46\n",
      "Win Rate: 39.66%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 999696.39\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -0.92%\n",
      "Total PnL: -9204.43\n",
      "Total Trades: 536\n",
      "Winning Trades: 201\n",
      "Win Rate: 37.50%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 990795.57\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -0.80%\n",
      "Total PnL: -7999.71\n",
      "Total Trades: 497\n",
      "Winning Trades: 187\n",
      "Win Rate: 37.63%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 992000.29\n",
      "--------------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=0.56 +/- 0.33\n",
      "Episode length: 10951.60 +/- 3456.35\n",
      "New best mean reward!\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -15.61%\n",
      "Total PnL: -156114.27\n",
      "Total Trades: 16474\n",
      "Winning Trades: 6406\n",
      "Win Rate: 38.89%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 843885.73\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -12.37%\n",
      "Total PnL: -123744.90\n",
      "Total Trades: 10060\n",
      "Winning Trades: 3634\n",
      "Win Rate: 36.12%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 876255.10\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -1.12%\n",
      "Total PnL: -11229.46\n",
      "Total Trades: 777\n",
      "Winning Trades: 302\n",
      "Win Rate: 38.87%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 988770.54\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -1.47%\n",
      "Total PnL: -14663.97\n",
      "Total Trades: 814\n",
      "Winning Trades: 316\n",
      "Win Rate: 38.82%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 985336.03\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -1.16%\n",
      "Total PnL: -11574.94\n",
      "Total Trades: 776\n",
      "Winning Trades: 295\n",
      "Win Rate: 38.02%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 988425.06\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -1.12%\n",
      "Total PnL: -11215.60\n",
      "Total Trades: 758\n",
      "Winning Trades: 289\n",
      "Win Rate: 38.13%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 988784.40\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -1.25%\n",
      "Total PnL: -12482.81\n",
      "Total Trades: 783\n",
      "Winning Trades: 291\n",
      "Win Rate: 37.16%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 987517.19\n",
      "--------------------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=0.60 +/- 0.61\n",
      "Episode length: 14124.00 +/- 2883.20\n",
      "New best mean reward!\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -22.77%\n",
      "Total PnL: -227699.50\n",
      "Total Trades: 17324\n",
      "Winning Trades: 6610\n",
      "Win Rate: 38.16%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 772300.50\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -73.53%\n",
      "Total PnL: -735308.30\n",
      "Total Trades: 44626\n",
      "Winning Trades: 17199\n",
      "Win Rate: 38.54%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 264691.70\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -5.66%\n",
      "Total PnL: -56600.59\n",
      "Total Trades: 3127\n",
      "Winning Trades: 1077\n",
      "Win Rate: 34.44%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 943399.41\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -7.15%\n",
      "Total PnL: -71501.45\n",
      "Total Trades: 3933\n",
      "Winning Trades: 1375\n",
      "Win Rate: 34.96%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 928498.55\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -1.14%\n",
      "Total PnL: -11412.35\n",
      "Total Trades: 671\n",
      "Winning Trades: 229\n",
      "Win Rate: 34.13%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 988587.65\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -6.05%\n",
      "Total PnL: -60523.28\n",
      "Total Trades: 3272\n",
      "Winning Trades: 1145\n",
      "Win Rate: 34.99%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 939476.72\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -33.69%\n",
      "Total PnL: -336946.23\n",
      "Total Trades: 18304\n",
      "Winning Trades: 6040\n",
      "Win Rate: 33.00%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 663053.77\n",
      "--------------------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-5.77 +/- 11.52\n",
      "Episode length: 6048.80 +/- 6526.57\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -45.84%\n",
      "Total PnL: -458402.90\n",
      "Total Trades: 29079\n",
      "Winning Trades: 11045\n",
      "Win Rate: 37.98%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 541597.10\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -15.47%\n",
      "Total PnL: -154744.46\n",
      "Total Trades: 8900\n",
      "Winning Trades: 2893\n",
      "Win Rate: 32.51%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 845255.54\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -19.28%\n",
      "Total PnL: -192837.75\n",
      "Total Trades: 9941\n",
      "Winning Trades: 3242\n",
      "Win Rate: 32.61%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 807162.25\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -13.80%\n",
      "Total PnL: -138024.32\n",
      "Total Trades: 7479\n",
      "Winning Trades: 2492\n",
      "Win Rate: 33.32%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 861975.68\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -5.90%\n",
      "Total PnL: -59035.50\n",
      "Total Trades: 3049\n",
      "Winning Trades: 1071\n",
      "Win Rate: 35.13%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 940964.50\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -11.04%\n",
      "Total PnL: -110384.30\n",
      "Total Trades: 6132\n",
      "Winning Trades: 2035\n",
      "Win Rate: 33.19%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 889615.70\n",
      "--------------------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-67.13 +/- 45.04\n",
      "Episode length: 10923.60 +/- 4209.80\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -53.32%\n",
      "Total PnL: -533163.95\n",
      "Total Trades: 34216\n",
      "Winning Trades: 12919\n",
      "Win Rate: 37.76%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 466836.05\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -4.19%\n",
      "Total PnL: -41884.92\n",
      "Total Trades: 2062\n",
      "Winning Trades: 688\n",
      "Win Rate: 33.37%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 958115.08\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -26.23%\n",
      "Total PnL: -262266.75\n",
      "Total Trades: 15941\n",
      "Winning Trades: 5355\n",
      "Win Rate: 33.59%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 737733.25\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -11.35%\n",
      "Total PnL: -113495.63\n",
      "Total Trades: 7315\n",
      "Winning Trades: 2472\n",
      "Win Rate: 33.79%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 886504.37\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -17.31%\n",
      "Total PnL: -173056.67\n",
      "Total Trades: 10404\n",
      "Winning Trades: 3402\n",
      "Win Rate: 32.70%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 826943.33\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -13.88%\n",
      "Total PnL: -138781.82\n",
      "Total Trades: 8514\n",
      "Winning Trades: 2827\n",
      "Win Rate: 33.20%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 861218.18\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -7.26%\n",
      "Total PnL: -72578.68\n",
      "Total Trades: 4540\n",
      "Winning Trades: 1600\n",
      "Win Rate: 35.24%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 927421.32\n",
      "--------------------------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=-12.25 +/- 17.48\n",
      "Episode length: 14501.20 +/- 4555.22\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "from trading.environments.forex_env2 import ForexTradingEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "\n",
    "pair = \"EUR_USD\"\n",
    "# parquet_path = Path(\"/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h\") / f\"{pair}.parquet\"\n",
    "# parquet_path = Path(\"/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h\") / f\"{pair}.parquet\"\n",
    "df = pd.read_parquet('/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/normalized/eur_norm_robut.parquet')\n",
    "\n",
    "dataset_manager = DatasetManager()\n",
    "train_df, val_df, test_df = dataset_manager.split_dataset(df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)\n",
    "\n",
    "\n",
    "\n",
    "saving_path = f'./logs/20nov/'\n",
    "os.makedirs(saving_path, exist_ok=True)\n",
    "\n",
    "def make_train_env():\n",
    "    env = ForexTradingEnv(\n",
    "        df=train_df,\n",
    "        pair='EUR_USD',\n",
    "\n",
    "    )\n",
    "    env = Monitor(env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=True)\n",
    "    return env\n",
    "\n",
    "def make_eval_env():\n",
    "    env = ForexTradingEnv(\n",
    "\n",
    "        df=val_df,\n",
    "        pair='EUR_USD',\n",
    "        # resample_interval='1h'\n",
    "    )\n",
    "    env = Monitor(env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=False)\n",
    "    env.training = False\n",
    "    return env\n",
    "\n",
    "train_env = make_train_env()\n",
    "eval_env = make_eval_env()\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=saving_path,\n",
    "    log_path=saving_path,\n",
    "    eval_freq=100_000,  # Adjust as needed\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "model = PPO(\n",
    "    'MultiInputPolicy',\n",
    "    train_env,\n",
    "    verbose=0,\n",
    "    tensorboard_log=f'{saving_path}tensorboard/',\n",
    ")\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=500_000,  # Adjust as needed\n",
    "    callback=eval_callback\n",
    ")\n",
    "\n",
    "model.save(f'{saving_path}best_model.zip')\n",
    "train_env.save(f'{saving_path}vec_normalize.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split sizes:\n",
      "Training: 101768 samples (70.0%)\n",
      "Validation: 21808 samples (15.0%)\n",
      "Test: 21808 samples (15.0%)\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import os, sys\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "from trading.model_manager import ModelManager\n",
    "from trading.agents.evaluate_model import ModelEvaluator, TradeLedger\n",
    "from pathlib import Path\n",
    "from trading.environments.forex_env import ForexTradingEnv\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv, VecNormalize\n",
    "from stable_baselines3 import PPO\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "eval_path = Path(\"model_evaluations\")\n",
    "eval_path.mkdir(exist_ok=True)\n",
    "evaluator = ModelEvaluator(base_path=eval_path)\n",
    "\n",
    "\n",
    "dataset_manager = DatasetManager()\n",
    "pair = \"EUR_USD\"\n",
    "df = dataset_manager.load_parquet_dataset(pair)\n",
    "train_df, val_df, test_df = dataset_manager.split_dataset(df)\n",
    "\n",
    "def load_model_for_evaluation(model_path: Path, env_path: Path) -> Tuple[PPO, VecNormalize]:\n",
    "    \"\"\"\n",
    "    Load a trained model and its normalization parameters.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model\n",
    "        env_path: Path to the saved VecNormalize stats\n",
    "    \"\"\"\n",
    "    # Create a dummy environment (required for loading)\n",
    "    def make_env():\n",
    "        def _init():\n",
    "            env = ForexTradingEnv(\n",
    "                df=test_df,  # Empty DataFrame for now\n",
    "                pair=\"EUR_USD\"\n",
    "            )\n",
    "            return env\n",
    "        return _init\n",
    "    \n",
    "    # Create vectorized environment\n",
    "    vec_env = DummyVecEnv([make_env()])\n",
    "    \n",
    "    # Load the saved normalization statistics\n",
    "    vec_env = VecNormalize.load(\n",
    "        env_path,\n",
    "        vec_env\n",
    "    )\n",
    "    \n",
    "    # Don't update normalization statistics during evaluation\n",
    "    vec_env.training = False\n",
    "    vec_env.norm_reward = False\n",
    "    \n",
    "    # Load the model\n",
    "    model = PPO.load(model_path)\n",
    "    \n",
    "    # Set the correct environment\n",
    "    model.set_env(vec_env)\n",
    "    \n",
    "    return model, vec_env\n",
    "\n",
    "model, vec_env = load_model_for_evaluation(\n",
    "    model_path= Path('/Users/floriankockler/Code/GitHub.nosync/ai6-gcp-bot/forex_trading_system/notebooks/logs/20nov/best_model'), \n",
    "    env_path= Path('/Users/floriankockler/Code/GitHub.nosync/ai6-gcp-bot/forex_trading_system/notebooks/logs/20nov/vec_normalize.pkl')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode Summary:\n",
      "Final Return: -36.57%\n",
      "Total PnL: -365657.64\n",
      "Total Trades: 19809\n",
      "Winning Trades: 6775\n",
      "Win Rate: 34.20%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 634342.36\n",
      "--------------------------------------------------\n",
      "Total trades recorded: 19809\n",
      "Ledger metrics: {'total_trades': 19809, 'winning_trades': 6775, 'losing_trades': 11938, 'win_rate': 0.3420162552375183, 'total_pnl': -365657.6364306434, 'average_pnl': -18.459166865093817, 'max_drawdown': 366323.447455183, 'avg_trade_duration': Timedelta('0 days 01:33:09.095865515'), 'best_trade': 1641.1270390509203, 'worst_trade': -2871.1286830462177, 'long_trades': 9904, 'short_trades': 9905, 'profit_factor': 0.5634835058529779}\n",
      "\n",
      "Evaluation Results:\n",
      "Total Trades: 19809\n",
      "Win Rate: 34.20%\n",
      "Total PnL: -365657.64\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "from trading.model_manager import ModelManager\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "from trading.agents.evaluate_model import ModelEvaluator, TradeLedger\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Create evaluation directory\n",
    "result_dir = Path(\"model_evaluation\") / f\"EUR_USD_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "result_dir.mkdir(parents=True)\n",
    "\n",
    "# Initialize evaluator \n",
    "evaluator = ModelEvaluator(base_path=Path(\"model_evaluation\"))\n",
    "\n",
    "# Evaluate model\n",
    "metrics, ledger = evaluator._evaluate_single_dataset(\n",
    "    model=model,\n",
    "    df=test_df,\n",
    "    pair=\"EUR_USD\",\n",
    "    save_dir=result_dir\n",
    ")\n",
    "\n",
    "# Get trade analysis\n",
    "trades_df = ledger.to_dataframe()\n",
    "trades_df.to_csv(result_dir / \"trades.csv\")\n",
    "\n",
    "print(f\"\\nEvaluation Results:\")\n",
    "print(f\"Total Trades: {len(trades_df)}\")\n",
    "print(f\"Win Rate: {metrics['win_rate']:.2%}\")\n",
    "print(f\"Total PnL: {metrics['total_pnl']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_trades': 19809,\n",
       " 'winning_trades': 6775,\n",
       " 'losing_trades': 11938,\n",
       " 'win_rate': 0.3420162552375183,\n",
       " 'total_pnl': -365657.6364306434,\n",
       " 'average_pnl': -18.459166865093817,\n",
       " 'max_drawdown': 366323.447455183,\n",
       " 'avg_trade_duration': Timedelta('0 days 01:33:09.095865515'),\n",
       " 'best_trade': 1641.1270390509203,\n",
       " 'worst_trade': -2871.1286830462177,\n",
       " 'long_trades': 9904,\n",
       " 'short_trades': 9905,\n",
       " 'profit_factor': 0.5634835058529779}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Out reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor reward distributions\n",
    "#! env not unwraped\n",
    "rewards = []\n",
    "realized_pnls = []\n",
    "unrealized_pnls = []\n",
    "\n",
    "for episode in range(10):\n",
    "    obs = train_env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = model.predict(obs, deterministic=True)[0]\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        if info.get('trade_closed'):\n",
    "            realized_pnls.append(info['trade_pnl'])\n",
    "        if info.get('unrealized_pnl'):\n",
    "            unrealized_pnls.append(info['unrealized_pnl'])\n",
    "\n",
    "print(f\"Reward stats:\")\n",
    "print(f\"Mean: {np.mean(rewards):.4f}\")\n",
    "print(f\"Std: {np.std(rewards):.4f}\")\n",
    "print(f\"Min: {np.min(rewards):.4f}\")\n",
    "print(f\"Max: {np.max(rewards):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward distribution monitoring\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def monitor_rewards(model, env, n_episodes=10):\n",
    "    \"\"\"\n",
    "    Monitor reward distributions and trading behavior with proper env unwrapping.\n",
    "    \"\"\"\n",
    "    # Unwrap to get the base environment\n",
    "    if hasattr(env, 'envs'):\n",
    "        # VecEnv unwrapping\n",
    "        base_env = env.envs[0]\n",
    "        if hasattr(base_env, 'env'):\n",
    "            # Possible Monitor wrapper\n",
    "            base_env = base_env.env\n",
    "        if hasattr(base_env, 'env'):\n",
    "            # Possible other wrappers\n",
    "            base_env = base_env.env\n",
    "    else:\n",
    "        base_env = env\n",
    "        \n",
    "    # Storage for metrics\n",
    "    metrics = {\n",
    "        'rewards': [],\n",
    "        'realized_pnls': [],\n",
    "        'unrealized_pnls': [],\n",
    "        'trade_durations': [],\n",
    "        'drawdowns': [],\n",
    "        'balance_trajectory': []\n",
    "    }\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        episode_rewards = []\n",
    "        \n",
    "        print(f\"\\nStarting episode {episode + 1}/{n_episodes}\")\n",
    "        \n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Store raw reward\n",
    "            metrics['rewards'].append(reward[0])  # Unwrap from vectorized form\n",
    "            \n",
    "            # Get info from base environment\n",
    "            metrics['balance_trajectory'].append(base_env.balance)\n",
    "            \n",
    "            if base_env.position is not None:\n",
    "                unrealized_pnl = base_env._calculate_pnl(\n",
    "                    base_env.position.type,\n",
    "                    base_env.position.entry_price,\n",
    "                    base_env.df.iloc[base_env.current_step]['close'],\n",
    "                    base_env.position.size\n",
    "                )\n",
    "                metrics['unrealized_pnls'].append(unrealized_pnl)\n",
    "            \n",
    "            # Store trade information when a trade is closed\n",
    "            info = info[0] if isinstance(info, tuple) else info  # Unwrap info\n",
    "            # Store available metrics from info dict\n",
    "            metrics['balances'].append(info['balance'])\n",
    "            metrics['total_pnl'].append(info['total_pnl'])\n",
    "            metrics['unrealized_pnl'].append(info['unrealized_pnl'])\n",
    "            metrics['win_rates'].append(info['win_rate'])\n",
    "            metrics['drawdowns'].append(info['drawdown'])\n",
    "        \n",
    "        # Episode summary\n",
    "        print(f\"Episode {episode + 1} completed:\")\n",
    "        print(f\"Final Balance: {info['balance']:.2f}\")\n",
    "        print(f\"Total PnL: {info['total_pnl']:.2f}\")\n",
    "        print(f\"Win Rate: {info['win_rate']:.2%}\")\n",
    "        print(f\"Max Drawdown: {info['drawdown']:.2%}\")\n",
    "\n",
    "    # Create analysis plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Reward distribution\n",
    "    sns.histplot(metrics['rewards'], bins=50, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Reward Distribution')\n",
    "    axes[0, 0].set_xlabel('Reward')\n",
    "    \n",
    "    # PnL trajectory\n",
    "    axes[0, 1].plot(metrics['total_pnl'])\n",
    "    axes[0, 1].set_title('Total PnL Trajectory')\n",
    "    axes[0, 1].set_xlabel('Step')\n",
    "    axes[0, 1].set_ylabel('PnL')\n",
    "    \n",
    "    # Balance trajectory\n",
    "    axes[1, 0].plot(metrics['balances'])\n",
    "    axes[1, 0].set_title('Balance Trajectory')\n",
    "    axes[1, 0].set_xlabel('Step')\n",
    "    axes[1, 0].set_ylabel('Balance')\n",
    "    \n",
    "    # Win rate trajectory\n",
    "    axes[1, 1].plot(metrics['win_rates'])\n",
    "    axes[1, 1].set_title('Win Rate Trajectory')\n",
    "    axes[1, 1].set_xlabel('Step')\n",
    "    axes[1, 1].set_ylabel('Win Rate')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print statistics\n",
    "    print(\"\\nReward Statistics:\")\n",
    "    print(f\"Mean Reward: {np.mean(metrics['rewards']):.4f}\")\n",
    "    print(f\"Std Reward: {np.std(metrics['rewards']):.4f}\")\n",
    "    print(f\"Min Reward: {np.min(metrics['rewards']):.4f}\")\n",
    "    print(f\"Max Reward: {np.max(metrics['rewards']):.4f}\")\n",
    "    \n",
    "    print(\"\\nTrading Statistics:\")\n",
    "    print(f\"Final Total PnL: {metrics['total_pnl'][-1]:.2f}\")\n",
    "    print(f\"Final Win Rate: {metrics['win_rates'][-1]:.2%}\")\n",
    "    print(f\"Max Drawdown: {max(metrics['drawdowns']):.2%}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "eval_freq = 50000\n",
    "total_timesteps = 100_0000\n",
    "\n",
    "model_dir = ''\n",
    "vec_normalize_path = ''\n",
    "model = PPO.load(model_dir)\n",
    "\n",
    "# Create and normalize evaluation environment\n",
    "def make_env():\n",
    "    return ForexTradingEnv(\n",
    "        df=test_df,  # Your test DataFrame\n",
    "        pair='EUR_USD',\n",
    "        initial_balance=1_000_000.0,\n",
    "        trade_size=100_000.0,\n",
    "        random_start=False\n",
    "    )\n",
    "\n",
    "# Create vectorized environment\n",
    "env = DummyVecEnv([make_env])\n",
    "env = VecNormalize(\n",
    "    env,\n",
    "    norm_obs=True,\n",
    "    norm_reward=False,  # Disable reward normalization for evaluation\n",
    "\n",
    ")\n",
    "\n",
    "# Load saved normalization stats\n",
    "env = VecNormalize.load(vec_normalize_path, env)\n",
    "env.training = False  # Disable training mode\n",
    "env.norm_reward = False  # Make sure reward normalization is disabled\n",
    "\n",
    "# Now run the monitoring\n",
    "reward_metrics = monitor_rewards(\n",
    "    model=model,\n",
    "    env=env,\n",
    "    n_episodes=1  # Number of episodes to evaluate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the model incl using trade ledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from trading.agents.evaluate_model import ModelEvaluator\n",
    "from trading.agents.trade_ledger import TradeLedger\n",
    "from trading.environments.forex_env import ForexTradingEnv\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "from trading.model_manager import ModelManager\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "# Setup paths\n",
    "eval_path = Path(\"model_evaluation\")\n",
    "eval_path.mkdir(exist_ok=True)\n",
    "\n",
    "dataset_manager = DatasetManager()\n",
    "train_df, val_df, test_df = dataset_manager.split_dataset(df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)\n",
    "\n",
    "\n",
    "\n",
    "saving_path = f'./logs/20nov/'\n",
    "\n",
    "def create_eval_env(df: pd.DataFrame, pair: str) -> VecNormalize:\n",
    "    \"\"\"Create and normalize evaluation environment.\"\"\"\n",
    "    def make_env():\n",
    "        return ForexTradingEnv(\n",
    "            df=df,\n",
    "            pair='EUR_USD',\n",
    "            initial_balance=1_000_000.0,\n",
    "            trade_size=100_000.0,\n",
    "            random_start=False  # Important for evaluation\n",
    "        )\n",
    "    \n",
    "    env = DummyVecEnv([make_env])\n",
    "    env = VecNormalize(\n",
    "        env,\n",
    "        norm_obs=True,\n",
    "        norm_reward=False,\n",
    "     \n",
    "    )\n",
    "    return env\n",
    "\n",
    "# Setup paths\n",
    "# model_dir = Path(f\"{saving_path}/best_model.zip\")\n",
    "eval_path = Path(\"model_evaluation\")\n",
    "eval_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator(base_path=eval_path)\n",
    "\n",
    "# Load model and normalization\n",
    "pair = \"EUR_USD\"\n",
    "model_path = f\"{saving_path}/best_model.zip\"\n",
    "vec_normalize_path = f\"{saving_path}/best_model.zp\"\n",
    "\n",
    "# Load the model\n",
    "model = PPO.load(model_path)\n",
    "\n",
    "# Create evaluation environment and load normalization stats\n",
    "eval_env = create_eval_env(test_df, pair)\n",
    "eval_env = VecNormalize.load(vec_normalize_path, eval_env)\n",
    "# Important: don't update normalization stats during evaluation\n",
    "eval_env.training = False\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Running model evaluation...\")\n",
    "results, trade_ledger = evaluator.evaluate_model(\n",
    "    model=model,\n",
    "    env=eval_env,  # Pass the normalized environment\n",
    "    df=test_df,\n",
    "    pair=pair,\n",
    "    version_id=\"v1\"\n",
    ")\n",
    "\n",
    "# Save detailed results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_dir = eval_path / f\"{pair}_evaluation_20nov_{timestamp}\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save trade log to Excel\n",
    "output_file = results_dir / \"trade_analysis.xlsx\"\n",
    "trade_ledger['test'].export_to_excel(output_file)\n",
    "\n",
    "# Convert trade ledger to DataFrame for analysis\n",
    "trades_df = trade_ledger['test'].to_dataframe()\n",
    "\n",
    "# Print summary metrics\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(f\"Total Trades: {len(trades_df)}\")\n",
    "print(f\"Win Rate: {(trades_df['pnl'] > 0).mean():.2%}\")\n",
    "print(f\"Total PnL: {trades_df['pnl'].sum():.2f}\")\n",
    "print(f\"Average PnL per Trade: {trades_df['pnl'].mean():.2f}\")\n",
    "print(f\"Sharpe Ratio: {results['test']['sharpe_ratio']:.2f}\")\n",
    "print(f\"Max Drawdown: {results['test']['max_drawdown']:.2%}\")\n",
    "\n",
    "# Analyze trades by session\n",
    "session_analysis = trades_df.groupby('session').agg({\n",
    "    'pnl': ['count', 'mean', 'sum'],\n",
    "    'duration': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nSession Analysis:\")\n",
    "display(session_analysis)\n",
    "\n",
    "# Analyze trades by holding period\n",
    "duration_bins = [0, 1, 4, 8, 24, float('inf')]\n",
    "duration_labels = ['0-1h', '1-4h', '4-8h', '8-24h', '>24h']\n",
    "trades_df['duration_category'] = pd.cut(\n",
    "    trades_df['duration'], \n",
    "    bins=duration_bins, \n",
    "    labels=duration_labels\n",
    ")\n",
    "\n",
    "duration_analysis = trades_df.groupby('duration_category').agg({\n",
    "    'pnl': ['count', 'mean', lambda x: (x > 0).mean()],\n",
    "    'duration': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "print(\"\\nDuration Analysis:\")\n",
    "display(duration_analysis)\n",
    "\n",
    "# Save trades to CSV\n",
    "trades_df.to_csv(results_dir / \"trades.csv\", index=True)\n",
    "\n",
    "# Create visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Cumulative PnL\n",
    "cumulative_pnl = trades_df['pnl'].cumsum()\n",
    "axes[0, 0].plot(cumulative_pnl.index, cumulative_pnl.values)\n",
    "axes[0, 0].set_title('Cumulative PnL')\n",
    "axes[0, 0].set_xlabel('Trade Number')\n",
    "axes[0, 0].set_ylabel('Cumulative PnL')\n",
    "\n",
    "# PnL Distribution\n",
    "sns.histplot(data=trades_df, x='pnl', bins=50, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('PnL Distribution')\n",
    "\n",
    "# Win Rate by Session\n",
    "session_win_rates = trades_df.groupby('session')['pnl'].apply(lambda x: (x > 0).mean())\n",
    "session_win_rates.plot(kind='bar', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Win Rate by Session')\n",
    "axes[1, 0].set_ylabel('Win Rate')\n",
    "\n",
    "# Trade Duration Distribution\n",
    "sns.histplot(data=trades_df, x='duration', bins=50, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Trade Duration Distribution')\n",
    "axes[1, 1].set_xlabel('Duration (hours)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / \"analysis_plots.png\")\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nResults saved to: {results_dir}\")\n",
    "print(f\"Trade log: {output_file}\")\n",
    "print(f\"Trades CSV: {results_dir / 'trades.csv'}\")\n",
    "print(f\"Analysis plots: {results_dir / 'analysis_plots.png'}\")\n",
    "\n",
    "# Close environment\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split sizes:\n",
      "Training: 101768 samples (70.0%)\n",
      "Validation: 21808 samples (15.0%)\n",
      "Test: 21808 samples (15.0%)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The number of environments to be set is different from the number of environments in the model: (1 != 3), whereas `set_env` requires them to be the same. To load a model with a different number of environments, you must use `PPO.load(path, env)` instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 68\u001b[0m\n\u001b[1;32m     64\u001b[0m     model\u001b[38;5;241m.\u001b[39mset_env(vec_env)\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, vec_env\n\u001b[0;32m---> 68\u001b[0m \u001b[43mload_model_for_evaluation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/floriankockler/Code/GitHub.nosync/ai6-gcp-bot/forex_trading_system/notebooks/forex_models/deployed/EUR_USD/model.zip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/floriankockler/Code/GitHub.nosync/ai6-gcp-bot/forex_trading_system/notebooks/forex_models/deployed/EUR_USD/vec_normalize.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 64\u001b[0m, in \u001b[0;36mload_model_for_evaluation\u001b[0;34m(model_path, env_path)\u001b[0m\n\u001b[1;32m     61\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO\u001b[38;5;241m.\u001b[39mload(model_path)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Set the correct environment\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvec_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, vec_env\n",
      "File \u001b[0;32m~/Code/anaconda_env/SB5/lib/python3.9/site-packages/stable_baselines3/common/base_class.py:492\u001b[0m, in \u001b[0;36mBaseAlgorithm.set_env\u001b[0;34m(self, env, force_reset)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;66;03m# if it is not a VecEnv, make it a VecEnv\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# and do other transformations (dict obs, image transpose) if needed\u001b[39;00m\n\u001b[1;32m    491\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_env(env, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m--> 492\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m env\u001b[38;5;241m.\u001b[39mnum_envs \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_envs, (\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of environments to be set is different from the number of environments in the model: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mnum_envs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_envs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), whereas `set_env` requires them to be the same. To load a model with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma different number of environments, you must use `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.load(path, env)` instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    496\u001b[0m )\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# Check that the observation spaces match\u001b[39;00m\n\u001b[1;32m    498\u001b[0m check_for_correct_spaces(env, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space)\n",
      "\u001b[0;31mAssertionError\u001b[0m: The number of environments to be set is different from the number of environments in the model: (1 != 3), whereas `set_env` requires them to be the same. To load a model with a different number of environments, you must use `PPO.load(path, env)` instead"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import os, sys\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "from trading.model_manager import ModelManager\n",
    "from trading.agents.evaluate_model import ModelEvaluator, TradeLedger\n",
    "from pathlib import Path\n",
    "from trading.environments.forex_env import ForexTradingEnv\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv, VecNormalize\n",
    "from stable_baselines3 import PPO\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "eval_path = Path(\"model_evaluations\")\n",
    "eval_path.mkdir(exist_ok=True)\n",
    "evaluator = ModelEvaluator(base_path=eval_path)\n",
    "\n",
    "\n",
    "dataset_manager = DatasetManager()\n",
    "pair = \"EUR_USD\"\n",
    "df = dataset_manager.load_parquet_dataset(pair)\n",
    "train_df, val_df, test_df = dataset_manager.split_dataset(df)\n",
    "\n",
    "def load_model_for_evaluation(model_path: Path, env_path: Path) -> Tuple[PPO, VecNormalize]:\n",
    "    \"\"\"\n",
    "    Load a trained model and its normalization parameters.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model\n",
    "        env_path: Path to the saved VecNormalize stats\n",
    "    \"\"\"\n",
    "    # Create a dummy environment (required for loading)\n",
    "    def make_env():\n",
    "        def _init():\n",
    "            env = ForexTradingEnv(\n",
    "                df=test_df,  # Empty DataFrame for now\n",
    "                pair=\"EUR_USD\"\n",
    "            )\n",
    "            return env\n",
    "        return _init\n",
    "    \n",
    "    # Create vectorized environment\n",
    "    vec_env = DummyVecEnv([make_env()])\n",
    "    \n",
    "    # Load the saved normalization statistics\n",
    "    vec_env = VecNormalize.load(\n",
    "        env_path,\n",
    "        vec_env\n",
    "    )\n",
    "    \n",
    "    # Don't update normalization statistics during evaluation\n",
    "    vec_env.training = False\n",
    "    vec_env.norm_reward = False\n",
    "    \n",
    "    # Load the model\n",
    "    model = PPO.load(model_path)\n",
    "    \n",
    "    # Set the correct environment\n",
    "    model.set_env(vec_env)\n",
    "    \n",
    "    return model, vec_env\n",
    "\n",
    "load_model_for_evaluation(\n",
    "    model_path= Path('/Users/floriankockler/Code/GitHub.nosync/ai6-gcp-bot/forex_trading_system/notebooks/forex_models/deployed/EUR_USD/model.zip'), \n",
    "    env_path= Path('/Users/floriankockler/Code/GitHub.nosync/ai6-gcp-bot/forex_trading_system/notebooks/forex_models/deployed/EUR_USD/vec_normalize.pkl')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_evaluation/evaluation_results'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrading\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluate_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelEvaluator, TradeLedger\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m---> 15\u001b[0m model_eval \u001b[38;5;241m=\u001b[39m \u001b[43mModelEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_evaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Setup paths\u001b[39;00m\n\u001b[1;32m     17\u001b[0m base_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Code/GitHub.nosync/ai6-gcp-bot/forex_trading_system/trading/agents/evaluate_model.py:25\u001b[0m, in \u001b[0;36mModelEvaluator.__init__\u001b[0;34m(self, base_path)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_path \u001b[38;5;241m=\u001b[39m base_path\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults_path \u001b[38;5;241m=\u001b[39m base_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_results\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/anaconda_env/SB5/lib/python3.9/pathlib.py:1323\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;124;03mCreate a new directory at this given path.\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1323\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_evaluation/evaluation_results'"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import os, sys\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "from trading.model_manager import ModelManager\n",
    "\n",
    "# Import the evaluation code\n",
    "from trading.agents.evaluate_model import ModelEvaluator, TradeLedger\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "model_eval = ModelEvaluator(base_path=Path(\"model_evaluation\"))\n",
    "# Setup paths\n",
    "base_path = Path(\"model_evaluation\")\n",
    "base_path.mkdir(exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "eval_path = base_path / f\"evaluation_{timestamp}\"\n",
    "eval_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Load your trained model\n",
    "model_manager = ModelManager()\n",
    "dataset_manager = DatasetManager()\n",
    "pair = \"EUR_USD\"\n",
    "\n",
    "model, version = model_manager.get_deployed_model(pair)\n",
    "\n",
    "# Load and split your data\n",
    "df = dataset_manager.load_parquet_dataset(pair)\n",
    "train_df, val_df, test_df = dataset_manager.split_dataset(df)\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "test_metrics, test_ledger = model_eval.evaluate_model(\n",
    "    model=model,\n",
    "    df=test_df,\n",
    "    pair=pair,\n",
    "    output_path=eval_path / 'test'\n",
    ")\n",
    "\n",
    "# Print key metrics\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"Total Trades: {test_metrics['total_trades']}\")\n",
    "print(f\"Win Rate: {test_metrics['win_rate']:.2%}\")\n",
    "print(f\"Total PnL: {test_metrics['total_pnl']:.2f}\")\n",
    "print(f\"Sharpe Ratio: {test_metrics['sharpe_ratio']:.2f}\")\n",
    "print(f\"Max Drawdown: {test_metrics['max_drawdown']:.2%}\")\n",
    "\n",
    "# Analyze trade patterns\n",
    "test_df = test_ledger.to_dataframe()\n",
    "print(\"\\nTrade Analysis:\")\n",
    "print(\"\\nAverage PnL by Hour:\")\n",
    "print(test_df.groupby(test_df['entry_time'].dt.hour)['pnl'].mean().round(2))\n",
    "\n",
    "print(\"\\nPosition Type Performance:\")\n",
    "print(test_df.groupby('position_type').agg({\n",
    "    'pnl': ['count', 'mean', 'sum'],\n",
    "    'holding_period': 'mean'\n",
    "}).round(2))\n",
    "\n",
    "# Show the plots\n",
    "test_ledger.plot_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SB5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
