{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "from trading.environments.forex_env2_flat import ForexTradingEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "from data_management.indicator_manager import IndicatorManager\n",
    "from visualization.chart_manager import ChartManager\n",
    "from data_management.preprocessor import DataPreprocessor\n",
    "\n",
    "processor = DataPreprocessor()\n",
    "dataset_manager = DatasetManager()\n",
    "chart_manager = ChartManager()\n",
    "indicator_manager = IndicatorManager()\n",
    "\n",
    "currencies = [\n",
    "            'EUR_USD'\n",
    "        ]\n",
    "# currencies = [\n",
    "#             'GBP_CHF', 'GBP_JPY', 'EUR_CHF', 'EUR_JPY', 'USD_CHF',\n",
    "#             'EUR_CAD', 'EUR_USD', 'GBP_USD', 'EUR_GBP', 'USD_JPY',\n",
    "#             'USD_CAD', 'AUD_USD', 'CHF_JPY', 'AUD_JPY', 'NZD_USD',\n",
    "#             'NZD_JPY', 'XAU_USD', 'XAG_USD'\n",
    "#         ]\n",
    "\n",
    "def prepare_unbiased_dataset(df_5min):\n",
    "    \"\"\"\n",
    "    Prepare dataset without look-ahead bias by calculating indicators\n",
    "    using only data available at each point in time.\n",
    "    \n",
    "    Args:\n",
    "        df_5min: DataFrame with 5-minute OHLC data and UTC DatetimeIndex\n",
    "    \"\"\"\n",
    "    # Validate input DataFrame has proper UTC DatetimeIndex\n",
    "    if not isinstance(df_5min.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"DataFrame index must be DatetimeIndex\")\n",
    "    if df_5min.index.tz is None:\n",
    "        raise ValueError(\"DataFrame index must be timezone-aware (UTC)\")\n",
    "    \n",
    "    # Initialize result DataFrame with the same structure as input\n",
    "    result_df = df_5min.copy()\n",
    "    \n",
    "    # Calculate indicators for the first day to get the actual columns that will be available\n",
    "    first_day_data = df_5min.iloc[:288]  # First day of data (288 5-min candles in a day)\n",
    "    first_day_daily = first_day_data.resample('D').agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last'\n",
    "    }).dropna()\n",
    "    \n",
    "    # Get actual indicator columns from first calculation\n",
    "    initial_indicators = indicator_manager.calculate_indicators(first_day_daily, indicator_timeframe='1h')\n",
    "    indicator_columns = initial_indicators.columns.tolist()\n",
    "    \n",
    "    print(f\"Available indicators: {indicator_columns}\")\n",
    "    \n",
    "    # Initialize indicator columns\n",
    "    for col in indicator_columns:\n",
    "        result_df[col] = np.nan\n",
    "    \n",
    "    # Get unique dates in UTC\n",
    "    dates = pd.Series(df_5min.index.tz_localize(None).date).unique()\n",
    "    total_dates = len(dates)\n",
    "    \n",
    "    for date_idx, date in enumerate(dates):\n",
    "        print(f\"Processing date: {date} ({date_idx + 1}/{total_dates})\")\n",
    "        \n",
    "        # Create UTC-aware datetime bounds for the day\n",
    "        day_start = pd.Timestamp(date).tz_localize('UTC')\n",
    "        day_end = (day_start + pd.Timedelta(days=1))\n",
    "        \n",
    "        # Get data for this day\n",
    "        day_mask = (df_5min.index >= day_start) & (df_5min.index < day_end)\n",
    "        day_data = df_5min[day_mask]\n",
    "        \n",
    "        # For each 5-minute candle in the day\n",
    "        for timestamp in day_data.index:\n",
    "            # Get all data available up to this point in time\n",
    "            available_data = df_5min[df_5min.index <= timestamp].copy()\n",
    "            \n",
    "            # Calculate the daily candle data using only available information\n",
    "            available_daily = available_data.resample('D').agg({\n",
    "                'open': 'first',\n",
    "                'high': 'max',\n",
    "                'low': 'min',\n",
    "                'close': 'last'\n",
    "            }).dropna()\n",
    "            \n",
    "            try:\n",
    "                # Calculate indicators using only available data\n",
    "                daily_indicators = indicator_manager.calculate_indicators(available_daily, indicator_timeframe='1h')\n",
    "                \n",
    "                # Get the most recent indicator values\n",
    "                if not daily_indicators.empty:\n",
    "                    current_indicators = daily_indicators.iloc[-1]\n",
    "                    \n",
    "                    # Store these indicator values for this specific timestamp\n",
    "                    # Only use columns that exist in both DataFrames\n",
    "                    available_columns = current_indicators.index.intersection(indicator_columns)\n",
    "                    print(f\"Available columns: {available_columns}\")\n",
    "                    result_df.loc[timestamp, available_columns] = current_indicators[available_columns]\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating indicators for {timestamp}: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "            # Progress logging\n",
    "            if timestamp.minute % 60 == 0:\n",
    "                print(f\"Processing: {timestamp}\")\n",
    "    \n",
    "    # Verify no NaN values in result\n",
    "    nan_counts = result_df[indicator_columns].isna().sum()\n",
    "    if nan_counts.any():\n",
    "        print(\"Warning: NaN values found in indicators:\")\n",
    "        print(nan_counts[nan_counts > 0])\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Main processing loop\n",
    "for ccy in currencies:\n",
    "    print(f\"\\nProcessing {ccy}...\")\n",
    "    source = f'/Volumes/ssd_fat2/ai6_trading_bot/datasets/1min/{ccy}.parquet'\n",
    "    \n",
    "    try:\n",
    "        # Read data and ensure UTC timezone\n",
    "        df = pd.read_parquet(source)\n",
    "        if df.index.tz is None:\n",
    "            df.index = df.index.tz_localize('UTC')\n",
    "        elif df.index.tz != pytz.UTC:\n",
    "            df.index = df.index.tz_convert('UTC')\n",
    "            \n",
    "        # Resample to 5 minutes while preserving UTC timezone\n",
    "        df_5min = df.resample('5min').agg({\n",
    "            'open': 'first',\n",
    "            'high': 'max',\n",
    "            'low': 'min',\n",
    "            'close': 'last',\n",
    "        }).dropna()\n",
    "        \n",
    "        print(\"Calculating unbiased indicators...\")\n",
    "        df_with_indicators = prepare_unbiased_dataset(df_5min)\n",
    "        \n",
    "        print(\"Normalizing data...\")\n",
    "        df_norm = processor.normalize_simple(df=df_with_indicators)\n",
    "        \n",
    "        output_path = f'/Volumes/ssd_fat2/ai6_trading_bot/datasets/5min/unbiased/{ccy}_5T_indics_1D_norm_ubiased.parquet'\n",
    "        print(f\"Saving to {output_path}\")\n",
    "        df_norm.to_parquet(output_path)\n",
    "        \n",
    "        print(f\"Completed processing {ccy}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {ccy}: {str(e)}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "from trading.environments.forex_env2_flat import ForexTradingEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "from data_management.indicator_manager import IndicatorManager\n",
    "from visualization.chart_manager import ChartManager\n",
    "from data_management.preprocessor import DataPreprocessor\n",
    "\n",
    "processor = DataPreprocessor()\n",
    "dataset_manager = DatasetManager()\n",
    "chart_manager = ChartManager()\n",
    "indicator_manager = IndicatorManager()\n",
    "\n",
    "source = '/Volumes/ssd_fat2/ai6_trading_bot/datasets/1min/EUR_USD.parquet'\n",
    "df = pd.read_parquet(source)\n",
    "\n",
    "\n",
    "minute_5 = df.resample('5min').agg({\n",
    "    'open': 'first',\n",
    "    'high': 'max',\n",
    "    'low': 'min',\n",
    "    'close': 'last'\n",
    "}).dropna()\n",
    "\n",
    "# Get actual indicator columns from first calculation\n",
    "df_with_indicators = indicator_manager.calculate_indicators(minute_5, indicator_timeframe='1h')\n",
    "df_norm = processor.normalize_simple(df=df_with_indicators)\n",
    "df_norm.columns\n",
    "df_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pytz\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "from data_management.indicator_manager import IndicatorManager\n",
    "\n",
    "from data_management.preprocessor import DataPreprocessor\n",
    "\n",
    "processor = DataPreprocessor()\n",
    "\n",
    "indicator_manager = IndicatorManager()\n",
    "\n",
    "def prepare_unbiased_dataset(\n",
    "    df: pd.DataFrame, \n",
    "    indicator_manager,\n",
    "    indicator_timeframe: str = '1H',\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare dataset with technical indicators calculated without look-ahead bias,\n",
    "    using rolling window aggregation.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with 1-minute OHLC data and UTC timezone index\n",
    "        indicator_manager: IndicatorManager instance\n",
    "        indicator_timeframe: Timeframe for indicator calculation (e.g., '1H', '4H', '1D')\n",
    "        verbose: Whether to print progress information\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with 5-minute candles and indicators calculated at specified timeframe\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Starting data preparation...\")\n",
    "    \n",
    "    # Ensure UTC timezone\n",
    "    if df.index.tz is None:\n",
    "        df.index = df.index.tz_localize('UTC')\n",
    "    elif df.index.tz != pytz.UTC:\n",
    "        df.index = df.index.tz_convert('UTC')\n",
    "    \n",
    "    # Create 5-minute OHLC data\n",
    "    df_5min = df.resample('5T').agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last'\n",
    "    }).dropna()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Resampled to 5-minute candles. Shape: {df_5min.shape}\")\n",
    "    \n",
    "    # Define the number of periods for the indicator timeframe\n",
    "    timeframe_minutes = pd.Timedelta(indicator_timeframe).total_seconds() / 60\n",
    "    periods = int(timeframe_minutes / 5)  # Number of 5-minute periods in the indicator timeframe\n",
    "\n",
    "    if periods < 1:\n",
    "        raise ValueError(\"Indicator timeframe is shorter than the data frequency.\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Timeframe minutes: {timeframe_minutes}\")\n",
    "        print(f\"Periods: {periods}\")\n",
    "    \n",
    "    # Rolling window aggregation functions\n",
    "    def rolling_agg(rolling_obj):\n",
    "        return pd.DataFrame({\n",
    "            'open': rolling_obj['open'].apply(lambda x: x.iloc[0]),\n",
    "            'high': rolling_obj['high'].max(),\n",
    "            'low': rolling_obj['low'].min(),\n",
    "            'close': rolling_obj['close'].apply(lambda x: x.iloc[-1])\n",
    "        })\n",
    "    \n",
    "    # Perform rolling window aggregation\n",
    "    rolling_windows = df_5min.rolling(window=periods, min_periods=periods)\n",
    "\n",
    "    # Apply the rolling aggregation\n",
    "    period_data = rolling_windows.apply(\n",
    "        lambda x: pd.Series({\n",
    "            'open': x['open'].iloc[0],\n",
    "            'high': x['high'].max(),\n",
    "            'low': x['low'].min(),\n",
    "            'close': x['close'].iloc[-1]\n",
    "        }),\n",
    "        raw=False\n",
    "    )\n",
    "\n",
    "    # Drop initial NaNs\n",
    "    period_data.dropna(inplace=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Aggregated period data. Shape: {period_data.shape}\")\n",
    "    \n",
    "    # Calculate indicators on the rolling aggregated data\n",
    "    indicators_df = indicator_manager.calculate_indicators(\n",
    "        period_data,\n",
    "        indicator_timeframe=None  # Since data is already aggregated\n",
    "    )\n",
    "\n",
    "    # Combine indicators with the 5-minute data\n",
    "    result_df = df_5min.join(indicators_df, how='inner')\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Final dataset prepared. Shape: {result_df.shape}\")\n",
    "        print(f\"Date range: {result_df.index[0]} to {result_df.index[-1]}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Main processing loop\n",
    "def process_currency_pairs(\n",
    "    currencies: List[str],\n",
    "    base_path: str = '/Volumes/ssd_fat2/ai6_trading_bot/datasets',\n",
    "    indicator_timeframe: str = '1H'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Process multiple currency pairs with unbiased indicator calculation.\n",
    "    \n",
    "    Args:\n",
    "        currencies: List of currency pairs to process\n",
    "        base_path: Base path for data storage\n",
    "        indicator_timeframe: Timeframe for indicator calculation\n",
    "    \"\"\"\n",
    "    for ccy in currencies:\n",
    "        print(f\"\\nProcessing {ccy}...\")\n",
    "        source = f'{base_path}/1min/{ccy}.parquet'\n",
    "        \n",
    "        try:\n",
    "            # Read source data\n",
    "            df = pd.read_parquet(source)\n",
    "            print(df.head())\n",
    "            \n",
    "            # Prepare dataset with unbiased indicators\n",
    "            df_with_indicators = prepare_unbiased_dataset(\n",
    "                df=df,\n",
    "                indicator_manager=indicator_manager,\n",
    "                indicator_timeframe=indicator_timeframe,\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            # Normalize the data\n",
    "            print(\"\\nNormalizing data...\")\n",
    "            df_norm = processor.normalize_simple(df=df_with_indicators)\n",
    "            \n",
    "            # Save results\n",
    "            output_path = f'{base_path}/5min/unbiased/{ccy}_5T_indics_{indicator_timeframe}_norm_unbiased.parquet'\n",
    "            print(f\"Saving to {output_path}\")\n",
    "            df_norm.to_parquet(output_path)\n",
    "            \n",
    "            print(f\"Completed processing {ccy}\")\n",
    "            print(df_norm)\n",
    "            return df_norm  # Return for inspection\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ccy}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Example usage\n",
    "currencies = ['EUR_USD']  # Add more pairs as needed\n",
    "\n",
    "df = process_currency_pairs(\n",
    "    currencies=currencies,\n",
    "    indicator_timeframe='1H'  # or '1D' for daily indicators\n",
    ")\n",
    "\n",
    "df  # For inspection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row-by-row approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pytz\n",
    "\n",
    "def prepare_unbiased_dataset_row_by_row(\n",
    "    df: pd.DataFrame, \n",
    "    indicator_manager,\n",
    "    indicator_timeframe: str = '1h',\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare dataset with technical indicators calculated without look-ahead bias,\n",
    "    processing data row by row.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with 1-minute OHLC data and UTC timezone index\n",
    "        indicator_manager: IndicatorManager instance\n",
    "        indicator_timeframe: Timeframe to aggregate data for indicator calculation (e.g., '1h', '4h', '1d')\n",
    "        verbose: Whether to print progress information\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with 5-minute candles and indicators calculated at specified timeframe\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Starting data preparation using row-by-row method...\")\n",
    "\n",
    "    # Ensure UTC timezone\n",
    "    if df.index.tz is None:\n",
    "        df.index = df.index.tz_localize('UTC')\n",
    "    elif df.index.tz != pytz.UTC:\n",
    "        df.index = df.index.tz_convert('UTC')\n",
    "\n",
    "    # Create 5-minute OHLC data\n",
    "    df_5min = df.resample('5min').agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last'\n",
    "    }).dropna()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Resampled to 5-minute candles. Shape: {df_5min.shape}\")\n",
    "\n",
    "    # Convert indicator_timeframe to minutes\n",
    "    timeframe_minutes = int(pd.Timedelta(indicator_timeframe).total_seconds() / 60)\n",
    "\n",
    "    # Get maximum periods required by indicators\n",
    "    max_indicator_periods = 100_000\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Maximum indicator periods required: {max_indicator_periods}\")\n",
    "\n",
    "    # Initialize list to collect results\n",
    "    results = []\n",
    "\n",
    "    # Progress bar setup\n",
    "    if verbose:\n",
    "        iterator = tqdm(df_5min.iterrows(), total=len(df_5min), desc='Processing rows')\n",
    "    else:\n",
    "        iterator = df_5min.iterrows()\n",
    "\n",
    "    # Initialize a DataFrame to cache data for the rolling window\n",
    "    data_cache = pd.DataFrame(columns=['open', 'high', 'low', 'close'])\n",
    "\n",
    "    for idx, row in iterator:\n",
    "        # Append the current row to the cache\n",
    "        data_cache.loc[idx] = row\n",
    "\n",
    "        # Remove data older than necessary for the indicator calculations\n",
    "        earliest_time = idx - pd.Timedelta(minutes=(timeframe_minutes * max_indicator_periods))\n",
    "        print(f\"Earliest time: {earliest_time}\")\n",
    "        data_cache = data_cache.loc[data_cache.index >= earliest_time]\n",
    "\n",
    "        # Get data up to the current time for indicator calculation\n",
    "        data_up_to_now = data_cache.loc[:idx]\n",
    "\n",
    "        # Aggregate data to the indicator timeframe up to the current time\n",
    "        period_data = data_up_to_now.resample(indicator_timeframe, closed='right', label='right').agg({\n",
    "            'open': 'first',\n",
    "            'high': 'max',\n",
    "            'low': 'min',\n",
    "            'close': 'last'\n",
    "        }).dropna()\n",
    "\n",
    "        # Check if we have enough periods in period_data for indicators\n",
    "        if len(period_data) < max_indicator_periods:\n",
    "            # Not enough data yet to calculate indicators\n",
    "            continue\n",
    "\n",
    "        # Calculate indicators on period_data\n",
    "        indicators_df = indicator_manager.calculate_indicators(\n",
    "            period_data,\n",
    "            indicator_timeframe=None  # Data is already aggregated\n",
    "        )\n",
    "\n",
    "        if indicators_df.empty:\n",
    "            # Not enough data, skip\n",
    "            continue\n",
    "\n",
    "        # Get the last row of indicators\n",
    "        try:\n",
    "            indicators = indicators_df.iloc[-1]\n",
    "        except IndexError:\n",
    "            # indicators_df is empty\n",
    "            continue\n",
    "\n",
    "        # Combine the current 5-minute data with indicators\n",
    "        combined_row = pd.concat([row, indicators])\n",
    "\n",
    "        # Append combined_row to results\n",
    "        results.append(combined_row)\n",
    "\n",
    "    # Create final DataFrame\n",
    "    result_df = pd.DataFrame(results)\n",
    "\n",
    "    # Drop any rows with NaN values in the indicators (e.g., initial periods)\n",
    "    result_df.dropna(inplace=True)\n",
    "\n",
    "    if verbose and not result_df.empty:\n",
    "        print(f\"\\nFinal dataset prepared. Shape: {result_df.shape}\")\n",
    "        print(f\"Date range: {result_df.index[0]} to {result_df.index[-1]}\")\n",
    "    elif verbose:\n",
    "        print(\"\\nNo data was processed. Please check if there is sufficient data for indicator calculation.\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def process_currency_pairs(\n",
    "    currencies: List[str],\n",
    "    base_path: str = '/Volumes/ssd_fat2/ai6_trading_bot/datasets',\n",
    "    indicator_timeframe: str = '1h'\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process multiple currency pairs with unbiased indicator calculation using row-by-row method.\n",
    "\n",
    "    Args:\n",
    "        currencies: List of currency pairs to process\n",
    "        base_path: Base path for data storage\n",
    "        indicator_timeframe: Timeframe for indicator calculation\n",
    "\n",
    "    Returns:\n",
    "        Processed DataFrame for inspection\n",
    "    \"\"\"\n",
    "    for ccy in currencies:\n",
    "        print(f\"\\nProcessing {ccy}...\")\n",
    "        source = f'{base_path}/1min/{ccy}.parquet'\n",
    "\n",
    "        try:\n",
    "            # Read source data\n",
    "            df = pd.read_parquet(source)\n",
    "\n",
    "            # Prepare dataset with unbiased indicators\n",
    "            df_with_indicators = prepare_unbiased_dataset_row_by_row(\n",
    "                df=df,\n",
    "                indicator_manager=indicator_manager,\n",
    "                indicator_timeframe=indicator_timeframe,\n",
    "                verbose=True\n",
    "            )\n",
    "\n",
    "            if df_with_indicators.empty:\n",
    "                print(f\"No data processed for {ccy}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Normalize the data\n",
    "            print(\"\\nNormalizing data...\")\n",
    "            df_norm = processor.normalize_simple(df=df_with_indicators)\n",
    "\n",
    "            # Save results\n",
    "            output_path = f'{base_path}/5min/unbiased/{ccy}_5min_indics_{indicator_timeframe}_norm_unbiased.parquet'\n",
    "            print(f\"Saving to {output_path}\")\n",
    "            df_norm.to_parquet(output_path)\n",
    "\n",
    "            print(f\"Completed processing {ccy}\")\n",
    "            return df_norm  # Return for inspection\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ccy}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Now you can run the processing function\n",
    "currencies = ['EUR_USD']\n",
    "df = process_currency_pairs(\n",
    "    currencies=currencies,\n",
    "    indicator_timeframe='1h'\n",
    ")\n",
    "\n",
    "df  # For inspection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_indicator_periods(indicator_params):\n",
    "    \"\"\"Calculate the maximum number of periods required by all indicators.\"\"\"\n",
    "    max_period = 0\n",
    "    for indicator, params in indicator_params.items():\n",
    "        if indicator == 'sma':\n",
    "            max_period = max(max_period, max(params.get('periods', [0])))\n",
    "        elif indicator == 'rsi':\n",
    "            max_period = max(max_period, params.get('period', 0))\n",
    "        elif indicator == 'macd':\n",
    "            max_period = max(max_period, params.get('slowperiod', 0))\n",
    "        elif indicator == 'bollinger':\n",
    "            max_period = max(max_period, params.get('timeperiod', 0))\n",
    "        elif indicator == 'atr':\n",
    "            max_period = max(max_period, params.get('period', 0))\n",
    "        elif indicator == 'adx':\n",
    "            max_period = max(max_period, params.get('period', 0))\n",
    "        elif indicator == 'dmi':\n",
    "            max_period = max(max_period, params.get('period', 0))\n",
    "        elif indicator == 'ichimoku':\n",
    "            # Ichimoku uses standard periods of 9, 26, and 52\n",
    "            max_period = max(max_period, 52)\n",
    "    return max_period\n",
    "\n",
    "max_indicator_periods = get_max_indicator_periods(indicator_manager.indicator_params)\n",
    "\n",
    "max_indicator_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "from trading.environments.forex_env2_flat import ForexTradingEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "from data_management.indicator_manager import IndicatorManager\n",
    "from visualization.chart_manager import ChartManager\n",
    "from data_management.preprocessor import DataPreprocessor\n",
    "\n",
    "processor = DataPreprocessor()\n",
    "dataset_manager = DatasetManager()\n",
    "chart_manager = ChartManager()\n",
    "indicator_manager = IndicatorManager()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read your 1-minute data and resample to 5-minute candles\n",
    "source = '/Volumes/ssd_fat2/ai6_trading_bot/datasets/1min/EUR_USD.parquet'\n",
    "df = pd.read_parquet(source)\n",
    "\n",
    "if df.index.tz is None:\n",
    "    df.index = df.index.tz_localize('UTC')\n",
    "\n",
    "minute_5 = df.resample('5min').agg({\n",
    "    'open': 'first',\n",
    "    'high': 'max',\n",
    "    'low': 'min',\n",
    "    'close': 'last'\n",
    "}).dropna()\n",
    "\n",
    "# Resample the 5-minute data into hourly candles, up to each time t\n",
    "hourly_data = minute_5.resample('1H', closed='right', label='right').agg({\n",
    "    'open': 'first',\n",
    "    'high': 'max',\n",
    "    'low': 'min',\n",
    "    'close': 'last'\n",
    "}).dropna()\n",
    "\n",
    "# Calculate technical indicators on the hourly data\n",
    "# (Assuming `indicator_manager.calculate_indicators` returns a DataFrame or Series)\n",
    "indicators_df = indicator_manager.calculate_indicators(hourly_data, indicator_timeframe='1h')\n",
    "\n",
    "# Map the indicator values to the 5-minute DataFrame, starting from time t onwards\n",
    "# This avoids using future data and eliminates forward-looking bias\n",
    "# Initialize result DataFrame\n",
    "result_df = df_5min.copy()\n",
    "\n",
    "# Map each indicator back to 5-minute data\n",
    "for column in indicators_df.columns:\n",
    "    result_df[column] = indicators_df[column].reindex(\n",
    "        df_5min.index, method='ffill'\n",
    "    )\n",
    "    \n",
    "    # Mask values before their calculation time\n",
    "    result_df[column] = result_df.apply(\n",
    "        lambda row: row[column] if row.name >= indicators_df.index.min() else np.nan,\n",
    "        axis=1\n",
    "        )\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pytz\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "def prepare_unbiased_dataset(\n",
    "    df: pd.DataFrame, \n",
    "    indicator_manager,\n",
    "    timeframe: str = '1H',\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare dataset with technical indicators calculated without look-ahead bias.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 1-minute OHLC data and UTC timezone index\n",
    "        indicator_manager: IndicatorManager instance\n",
    "        timeframe: Timeframe for indicator calculation (e.g., '1H', '4H', '1D')\n",
    "        verbose: Whether to print progress information\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with 5-minute candles and indicators calculated at specified timeframe\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Starting data preparation...\")\n",
    "    \n",
    "    # Ensure UTC timezone\n",
    "    if df.index.tz is None:\n",
    "        df.index = df.index.tz_localize('UTC')\n",
    "    elif df.index.tz != pytz.UTC:\n",
    "        df.index = df.index.tz_convert('UTC')\n",
    "    \n",
    "    # Create 5-minute OHLC data\n",
    "    df_5min = df.resample('5min').agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last'\n",
    "    }).dropna()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Resampled to 5-minute candles. Shape: {df_5min.shape}\")\n",
    "    \n",
    "    # Create timeframe data for indicator calculation (e.g., hourly)\n",
    "    period_data = df_5min.resample(timeframe, closed='right', label='right').agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last'\n",
    "    }).dropna()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Resampled to {timeframe} for indicator calculation. Shape: {period_data.shape}\")\n",
    "    \n",
    "    # Calculate indicators\n",
    "    indicators_df = indicator_manager.calculate_indicators(\n",
    "        period_data, \n",
    "        indicator_timeframe=timeframe\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Calculated indicators. Available columns: {indicators_df.columns.tolist()}\")\n",
    "    \n",
    "    # Initialize result DataFrame\n",
    "    result_df = df_5min.copy()\n",
    "    \n",
    "    # Forward fill indicators to 5-minute data\n",
    "    for column in indicators_df.columns:\n",
    "        # Reindex and forward fill\n",
    "        result_df[column] = indicators_df[column].reindex(\n",
    "            df_5min.index, \n",
    "            method='ffill'\n",
    "        )\n",
    "        \n",
    "        # Mask values before their first calculation time\n",
    "        mask = result_df.index >= indicators_df.index.min()\n",
    "        result_df.loc[~mask, column] = np.nan\n",
    "    \n",
    "    # Validate results\n",
    "    if verbose:\n",
    "        nan_counts = result_df[indicators_df.columns].isna().sum()\n",
    "        if nan_counts.any():\n",
    "            print(\"\\nNaN values in indicators:\")\n",
    "            print(nan_counts[nan_counts > 0])\n",
    "        \n",
    "        print(f\"\\nFinal shape: {result_df.shape}\")\n",
    "        print(f\"Date range: {result_df.index[0]} to {result_df.index[-1]}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Main processing loop\n",
    "def process_currency_pairs(\n",
    "    currencies: List[str],\n",
    "    base_path: str = '/Volumes/ssd_fat2/ai6_trading_bot/datasets',\n",
    "    indicator_timeframe: str = '1H'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Process multiple currency pairs with unbiased indicator calculation.\n",
    "    \n",
    "    Args:\n",
    "        currencies: List of currency pairs to process\n",
    "        base_path: Base path for data storage\n",
    "        indicator_timeframe: Timeframe for indicator calculation\n",
    "    \"\"\"\n",
    "    for ccy in currencies:\n",
    "        print(f\"\\nProcessing {ccy}...\")\n",
    "        source = f'{base_path}/1min/{ccy}.parquet'\n",
    "        \n",
    "        try:\n",
    "            # Read source data\n",
    "            df = pd.read_parquet(source)\n",
    "            \n",
    "            # Prepare dataset with unbiased indicators\n",
    "            df_with_indicators = prepare_unbiased_dataset(\n",
    "                df=df,\n",
    "                indicator_manager=indicator_manager,\n",
    "                timeframe=indicator_timeframe,\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            # Normalize the data\n",
    "            print(\"\\nNormalizing data...\")\n",
    "            df_norm = processor.normalize_simple(df=df_with_indicators)\n",
    "            \n",
    "            # Save results\n",
    "            output_path = f'{base_path}/5min/unbiased/{ccy}_5T_indics_{indicator_timeframe}_norm_unbiased.parquet'\n",
    "            print(f\"Saving to {output_path}\")\n",
    "            df_norm.to_parquet(output_path)\n",
    "            \n",
    "            print(f\"Completed processing {ccy}\")\n",
    "            return df_norm\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ccy}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Example usage\n",
    "currencies = ['EUR_USD']  # Add more pairs as needed\n",
    "\n",
    "df = process_currency_pairs(\n",
    "    currencies=currencies,\n",
    "    indicator_timeframe='1H'  # or '1D' for daily indicators\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('/Volumes/ssd_fat2/ai6_trading_bot/datasets/5min/unbiased/EUR_USD_5T_indics_1D_norm_ubiased.parquet')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('/Volumes/ssd_fat2/ai6_trading_bot/datasets/5min/best_dataframes/CHF_JPY_5T_indics_1H_norm.parquet')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "from trading.environments.forex_env2_flat import ForexTradingEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "from data_management.indicator_manager import IndicatorManager\n",
    "from visualization.chart_manager import ChartManager\n",
    "from data_management.preprocessor import DataPreprocessor\n",
    "\n",
    "processor = DataPreprocessor()\n",
    "dataset_manager = DatasetManager()\n",
    "chart_manager = ChartManager()\n",
    "indicator_manager = IndicatorManager()\n",
    "\n",
    "currencies = [\n",
    "            'GBP_CHF', 'GBP_JPY', 'EUR_CHF', 'EUR_JPY', 'USD_CHF',\n",
    "            'EUR_CAD', 'EUR_USD', 'GBP_USD', 'EUR_GBP', 'USD_JPY',\n",
    "            'USD_CAD', 'AUD_USD', 'CHF_JPY', 'AUD_JPY', 'NZD_USD',\n",
    "            'NZD_JPY', 'XAU_USD', 'XAG_USD'\n",
    "        ]\n",
    "for ccy in currencies:\n",
    "    source = f'/Volumes/ssd_fat2/ai6_trading_bot/datasets/1min/{ccy}.parquet'\n",
    "    df = pd.read_parquet(source)\n",
    "    resampled_df = df.resample('5min').agg({\n",
    "                        'open': 'first',\n",
    "                        'high': 'max',\n",
    "                        'low': 'min',\n",
    "                        'close': 'last',\n",
    "                    }).dropna()\n",
    "    # Initialize result DataFrame\n",
    "    result_df = resampled_df.copy()\n",
    "    \n",
    "    # Group by date to process one day at a time\n",
    "    for date in resampled_df.index.date.unique():\n",
    "        day_data = resampled_df[resampled_df.index.date == date]\n",
    "        \n",
    "        # For each 5-minute candle in the day\n",
    "        for timestamp in day_data.index:\n",
    "            # Get only the data available up to this point\n",
    "            available_data = resampled_df[resampled_df.index <= timestamp]\n",
    "            \n",
    "            # Calculate daily candle using only available data\n",
    "            available_daily = available_data.resample('D').agg({\n",
    "                'open': 'first',\n",
    "                'high': 'max',\n",
    "                'low': 'min',\n",
    "                'close': 'last',\n",
    "                'volume': 'sum'\n",
    "            })\n",
    "            \n",
    "            # Calculate indicators using only available data\n",
    "            indicators = indicator_manager.calculate_indicators(available_daily)\n",
    "            \n",
    "            # Store the indicators for this specific timestamp\n",
    "            # result_df.loc[timestamp, indicator_columns] = indicators.iloc[-1]\n",
    "    df_norm = processor.normalize_simple(df=df_with_indicators)\n",
    "    df_with_indicators.to_parquet('/Volumes/ssd_fat2/ai6_trading_bot/datasets/5min/EUR_USD_5T_indics_1D.parquet')\n",
    "    df_with_indicators\n",
    "    \n",
    "    \n",
    "    df_with_indicators = indicator_manager.calculate_indicators(resampled_df, indicator_timeframe='1D')\n",
    "    # df_with_indicators.dropna(inplace=True)\n",
    "\n",
    "    # print(df_with_indicators)\n",
    "    df_norm = processor.normalize_simple(df=df_with_indicators)\n",
    "    df_with_indicators.to_parquet('/Volumes/ssd_fat2/ai6_trading_bot/datasets/5min/EUR_USD_5T_indics_1D.parquet')\n",
    "    df_with_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "from trading.environments.forex_env2_flat import ForexTradingEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "\n",
    "df_not_norm_dual_indic = '/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/EUR_USD_1H_indics_1D_and_1h.parquet'\n",
    "\n",
    "\n",
    "df = pd.read_parquet(df_not_norm_dual_indic)\n",
    "dataset_manager = DatasetManager()\n",
    "train_df, val_df, test_df = dataset_manager.split_dataset(df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)\n",
    "\n",
    "\n",
    "\n",
    "saving_path = f'./logs/29nov/no_norm_1H_and_1D_dual_indic/'\n",
    "os.makedirs(saving_path, exist_ok=True)\n",
    "\n",
    "def make_train_env():\n",
    "    env = ForexTradingEnv(\n",
    "        df=train_df,\n",
    "        pair='EUR_USD',\n",
    "\n",
    "    )\n",
    "    env = Monitor(env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=True)\n",
    "    return env\n",
    "\n",
    "def make_eval_env():\n",
    "    env = ForexTradingEnv(\n",
    "\n",
    "        df=val_df,\n",
    "        pair='EUR_USD',\n",
    "        # resample_interval='1h'\n",
    "    )\n",
    "    env = Monitor(env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=False)\n",
    "    env.training = False\n",
    "    return env\n",
    "\n",
    "train_env = make_train_env()\n",
    "eval_env = make_eval_env()\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=saving_path,\n",
    "    log_path=saving_path,\n",
    "    eval_freq=50_000,  # Adjust as needed\n",
    "    n_eval_episodes=5,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "model = PPO(\n",
    "    'MlpPolicy',\n",
    "    train_env,\n",
    "    verbose=0,\n",
    "    tensorboard_log=f'{saving_path}tensorboard/',\n",
    ")\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=3_000_000,  # Adjust as needed\n",
    "    callback=eval_callback\n",
    ")\n",
    "\n",
    "model.save(f'{saving_path}best_model.zip')\n",
    "train_env.save(f'{saving_path}vec_normalize.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EUR norm Hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "from trading.environments.forex_env2_flat import ForexTradingEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "\n",
    "pair = \"EUR_USD\"\n",
    "\n",
    "eur_norm = Path('/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/normalized/EUR_USD.parquet')\n",
    "df = pd.read_parquet(eur_norm)\n",
    "\n",
    "dataset_manager = DatasetManager()\n",
    "train_df, val_df, test_df = dataset_manager.split_dataset(df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)\n",
    "\n",
    "\n",
    "\n",
    "saving_path = f'./logs/27nov/norm_hstack/'\n",
    "os.makedirs(saving_path, exist_ok=True)\n",
    "\n",
    "def make_train_env():\n",
    "    env = ForexTradingEnv(\n",
    "        df=train_df,\n",
    "        pair='EUR_USD',\n",
    "\n",
    "    )\n",
    "    env = Monitor(env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=True)\n",
    "    return env\n",
    "\n",
    "def make_eval_env():\n",
    "    env = ForexTradingEnv(\n",
    "\n",
    "        df=val_df,\n",
    "        pair='EUR_USD',\n",
    "        # resample_interval='1h'\n",
    "    )\n",
    "    env = Monitor(env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=False)\n",
    "    env.training = False\n",
    "    return env\n",
    "\n",
    "train_env = make_train_env()\n",
    "eval_env = make_eval_env()\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=saving_path,\n",
    "    log_path=saving_path,\n",
    "    eval_freq=100_000,  # Adjust as needed\n",
    "    n_eval_episodes=5,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "model = PPO(\n",
    "    'MlpPolicy',\n",
    "    train_env,\n",
    "    verbose=0,\n",
    "    tensorboard_log=f'{saving_path}tensorboard/',\n",
    ")\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=5_000_000,  # Adjust as needed\n",
    "    callback=eval_callback\n",
    ")\n",
    "\n",
    "model.save(f'{saving_path}best_model.zip')\n",
    "train_env.save(f'{saving_path}vec_normalize.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "from trading.environments.forex_env2_flat import ForexTradingEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "\n",
    "pair = \"EUR_USD\"\n",
    "\n",
    "eur_norm = Path('/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/normalized/EUR_USD.parquet')\n",
    "df = pd.read_parquet(eur_norm)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "output_dir= \"/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/normalized/\"\n",
    "Path(output_dir)\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "\n",
    "from data_management.preprocessor import DataPreprocessor\n",
    "\n",
    "\n",
    "\n",
    "processor = DataPreprocessor()\n",
    "\n",
    "eur = pd.read_parquet(\"/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/EUR_USD.parquet\")\n",
    "\n",
    "eur_norm = processor.normalize_simple(df=eur)\n",
    "eur_norm.to_parquet('/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/normalized/EUR_USD.parquet')\n",
    "eur_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eur_norm.to_parquet('/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/normalized/EUR_USD.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SB5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
