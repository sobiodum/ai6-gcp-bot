{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from dataclasses import dataclass\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "import os, sys\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "log_file = \"optuna_trials4.log\"  # Path to log file\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()  # For console output\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "from trading.environments.forex_env2 import ForexTradingEnv\n",
    "\n",
    "pair = \"EUR_USD\"\n",
    "parquet_path = Path(\"/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h\") / f\"{pair}.parquet\"\n",
    "norm_robust_path = Path('/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/normalized/eur_norm_robut.parquet')\n",
    "df = pd.read_parquet(norm_robust_path)\n",
    "\n",
    "dataset_manager = DatasetManager()\n",
    "train_df, val_df, test_df = dataset_manager.split_dataset(df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RewardParams:\n",
    "    \"\"\"Parameters controlling the reward function behavior.\"\"\"\n",
    "    realized_pnl_weight: float = 1.1\n",
    "    unrealized_pnl_weight: float = 0.8\n",
    "    holding_time_threshold: int = 7*12  # hours\n",
    "    holding_penalty_factor: float = -0.00001\n",
    "    max_trades_per_day: int = 6 \n",
    "    overtrading_penalty_factor: float = -0.0001\n",
    "    win_rate_threshold: float = 0.4\n",
    "    win_rate_bonus_factor: float = 0.0005\n",
    "    drawdown_penalty_factor: float = -0.0001\n",
    "\n",
    "@dataclass\n",
    "class OptimizationResult:\n",
    "    \"\"\"Stores results of a single trial.\"\"\"\n",
    "    trial_number: int\n",
    "    params: Dict\n",
    "    final_balance: float\n",
    "    total_trades: int\n",
    "    win_rate: float\n",
    "    max_drawdown: float\n",
    "    training_time: float\n",
    "\n",
    "class RewardOptimizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        val_df: pd.DataFrame,\n",
    "        study_name: str = \"forex_reward_optimization_optimized_only_2_params_robust_norm\",\n",
    "        n_timesteps: int = 500_000\n",
    "    ):\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.study_name = study_name\n",
    "        self.n_timesteps = n_timesteps\n",
    "        \n",
    "        # Setup study with TPE sampler and Median pruner\n",
    "        self.study = optuna.create_study(\n",
    "            study_name=study_name,\n",
    "            storage=\"sqlite:///optuna_trials.db\",\n",
    "            load_if_exists=True,\n",
    "            sampler=TPESampler(seed=42),\n",
    "            pruner=MedianPruner(\n",
    "                n_startup_trials=5,\n",
    "                n_warmup_steps=100_000,\n",
    "                interval_steps=50_000\n",
    "            ),\n",
    "            direction=\"maximize\"\n",
    "        )\n",
    "\n",
    "    def _create_env(self, df: pd.DataFrame, params: Dict, is_eval: bool = False, n_envs: int = 3) -> VecNormalize:\n",
    "        \"\"\"Create vectorized and normalized environment with multiple subprocesses.\"\"\"\n",
    "        def make_env():\n",
    "            \"\"\"Returns an environment creation function for use with vectorized environments.\"\"\"\n",
    "            def _init():\n",
    "                try:\n",
    "                    env = ForexTradingEnv(\n",
    "                        df=df.copy(),\n",
    "                        pair='EUR_USD',\n",
    "                        initial_balance=1_000_000,\n",
    "                        trade_size=100_000,\n",
    "                        reward_params=RewardParams(**params),\n",
    "                        sequence_length=10,\n",
    "                        random_start=False\n",
    "                    )\n",
    "                    env = Monitor(env)\n",
    "                    return env\n",
    "                except Exception as e:\n",
    "                    print(f\"Error creating environment: {str(e)}\")\n",
    "                    raise\n",
    "            return _init\n",
    "\n",
    "        try:\n",
    "            # Create environment builders with appropriate seeds\n",
    "            if is_eval:\n",
    "                envs = [make_env()]  # Single env for evaluation\n",
    "                vec_env = DummyVecEnv(envs)\n",
    "            else:\n",
    "                envs = [make_env() for _ in range(n_envs)]\n",
    "                vec_env = SubprocVecEnv(envs)\n",
    "\n",
    "            # Apply normalization\n",
    "            env = VecNormalize(\n",
    "                vec_env,\n",
    "                norm_obs=True,\n",
    "                norm_reward=not is_eval,\n",
    "                clip_obs=10.0,\n",
    "                clip_reward=10.0,\n",
    "                gamma=1.0,\n",
    "                epsilon=1e-08\n",
    "            )\n",
    "\n",
    "            # Set training mode appropriately\n",
    "            if is_eval:\n",
    "                env.training = False\n",
    "                env.norm_reward = False\n",
    "\n",
    "            return env\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in environment creation: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def objective(self, trial: optuna.Trial) -> float:\n",
    "        \"\"\"Optimization objective function.\"\"\"\n",
    "        def get_env_attribute(env, attr_name):\n",
    "            \"\"\"\n",
    "            Fetch an attribute from the environment, supporting SubprocVecEnv and single environments.\n",
    "            Args:\n",
    "                env: The environment (SubprocVecEnv, DummyVecEnv, or wrapped env).\n",
    "                attr_name (str): The name of the attribute to fetch.\n",
    "            Returns:\n",
    "                The attribute value, or None if not found.\n",
    "            \"\"\"\n",
    "            if hasattr(env, \"envs\"):  # SubprocVecEnv or DummyVecEnv\n",
    "                # Get the attribute from the first environment in the vectorized stack\n",
    "                try:\n",
    "                    print(f\"Fetching attribute {attr_name} : {env.get_attr(attr_name, indices=0)} from the first environment in the vectorized stack.\")\n",
    "                    return env.get_attr(attr_name, indices=0)\n",
    "                except AttributeError:\n",
    "                    print(f\"Attribute {attr_name} : {env.get_attr(attr_name, indices=0)} not found in the first environment.\")\n",
    "                    # Use the latest Gymnasium recommendation: get_wrapper_attr\n",
    "                    return env.get_wrapper_attr(attr_name)\n",
    "            else:  # Single unwrapped environment\n",
    "                print(f\"Fetching attribute {attr_name} : {getattr(env.unwrapped, attr_name, None)} from the unwrapped environment.\")\n",
    "                return getattr(env.unwrapped, attr_name, None)\n",
    "        try:\n",
    "            # Sample parameters\n",
    "            params = self._sample_parameters(trial)\n",
    "            \n",
    "\n",
    "            # Create environments\n",
    "            train_env = self._create_env(self.train_df, params, is_eval=False, n_envs=3)\n",
    "            eval_env = self._create_env(self.val_df, params, is_eval=True, n_envs=1)\n",
    "            \n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            # Create model\n",
    "            model = PPO(\n",
    "                \"MultiInputPolicy\",\n",
    "                train_env,\n",
    "                verbose=0,\n",
    "                tensorboard_log=f\"./tensorboard4/short_trial_{trial.number}\"\n",
    "            )\n",
    "\n",
    "            # Setup evaluation callback\n",
    "            eval_callback = EvalCallback(\n",
    "                eval_env,\n",
    "                best_model_save_path=f\"./models/short_trial_{trial.number}\",\n",
    "                log_path=f\"./logs/trial_{trial.number}\",\n",
    "                eval_freq=100_000,\n",
    "                deterministic=True,\n",
    "                render=False\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            model.learn(\n",
    "                total_timesteps=self.n_timesteps,\n",
    "                callback=eval_callback\n",
    "            )\n",
    "      \n",
    "            try:\n",
    "                save_path = f'./optuna2/best_model_trial_{trial.number}/'\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                train_env.save(os.path.join(save_path, 'vecnormalize.pkl'))\n",
    "                logging.info(f\"Saved VecNormalize to {os.path.join(save_path, 'vecnormalize.pkl')}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to save VecNormalize: {e}\")\n",
    "                raise\n",
    "            \n",
    "            # Get final balance from eval environment\n",
    "            #! Below use oudated code, but worked before\n",
    "      \n",
    "            try:\n",
    "                final_balance = eval_env.get_attr('balance')[0]\n",
    "                total_trades = eval_env.get_attr('total_trades')[0]\n",
    "                win_rate = eval_env.get_attr('winning_trades')[0] / max(1, total_trades)\n",
    "                # final_balance = float(eval_env.env_method('get_attr', 'balance')[0][0])\n",
    "                # total_trades = int(eval_env.env_method('get_attr', 'total_trades')[0][0])\n",
    "                # winning_trades = int(eval_env.env_method('get_attr', 'winning_trades')[0][0])\n",
    "                # win_rate = winning_trades / max(1, total_trades)\n",
    "                \n",
    "                # Log results\n",
    "                training_time = (datetime.now() - start_time).total_seconds()\n",
    "                print(f\"\\nTrial {trial.number} completed:\")\n",
    "                print(f\"Final Balance: ${final_balance:,.2f}\")\n",
    "                print(f\"Total Trades: {total_trades}\")\n",
    "                print(f\"Win Rate: {win_rate:.2%}\")\n",
    "                print(f\"Training Time: {training_time:.1f}s\")\n",
    "                print(\"-\" * 80)\n",
    "                \n",
    "                return final_balance\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error getting evaluation metrics: {str(e)}\")\n",
    "                return float('-inf')\n",
    "                \n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Trial {trial.number} failed: {str(e)}\")\n",
    "            return float('-inf')\n",
    "        finally:\n",
    "            if 'train_env' in locals():\n",
    "                train_env.close()\n",
    "            if 'eval_env' in locals():\n",
    "                eval_env.close()\n",
    "\n",
    "    def _sample_parameters(self, trial: optuna.Trial) -> Dict:\n",
    "        \"\"\"Sample reward parameters for trial.\"\"\"\n",
    "        return {\n",
    "            'realized_pnl_weight': trial.suggest_float('realized_pnl_weight', 1.0, 1.5),\n",
    "            'unrealized_pnl_weight': trial.suggest_float('unrealized_pnl_weight', 0.5, 1.0),\n",
    "            # 'holding_time_threshold': trial.suggest_int('holding_time_threshold', 24, 96),\n",
    "            # 'holding_penalty_factor': trial.suggest_float('holding_penalty_factor', -0.0001, 0.0),\n",
    "            # 'max_trades_per_day': trial.suggest_int('max_trades_per_day', 3, 12),\n",
    "            # 'overtrading_penalty_factor': trial.suggest_float('overtrading_penalty_factor', -0.0001, 0.0),\n",
    "            # 'win_rate_threshold': trial.suggest_float('win_rate_threshold', 0.3, 0.5),\n",
    "            # 'win_rate_bonus_factor': trial.suggest_float('win_rate_bonus_factor', 0.0001, 0.001, log=True),\n",
    "            # 'drawdown_penalty_factor': trial.suggest_float('drawdown_penalty_factor', -0.001, 0.0)\n",
    "        }\n",
    "\n",
    "    def optimize(self, n_trials: int = 100, n_jobs: int = 6) -> None:\n",
    "        \"\"\"Run optimization using Optuna's built-in parallelization.\"\"\"\n",
    "        self.study.optimize(\n",
    "            self.objective,\n",
    "            n_trials=n_trials,\n",
    "            n_jobs=n_jobs,  # Number of parallel jobs\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "            \n",
    "        # Print best trial after completion\n",
    "        print(\"\\nOptimization completed!\")\n",
    "        print(\"\\nBest trial:\")\n",
    "        trial = self.study.best_trial\n",
    "        print(f\"Value: ${trial.value:,.2f}\")\n",
    "        print(\"Best parameters:\")\n",
    "        for key, value in trial.params.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "\n",
    "\n",
    "optimizer = RewardOptimizer(\n",
    "    train_df=train_df,\n",
    "    val_df=val_df,\n",
    "    n_timesteps=1_000_000\n",
    ")\n",
    "\n",
    "optimizer.optimize(n_trials=10, n_jobs=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SB5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
