{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 18:33:56,666 - setting up API-client for environment practice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split sizes:\n",
      "Training: 101768 samples (70.0%)\n",
      "Validation: 21808 samples (15.0%)\n",
      "Test: 21808 samples (15.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-22 18:33:57,031] A new study created in RDB with name: forex_reward_optimization_optimized_only_2_params_robust_norm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode Summary:\n",
      "Final Return: 0.67%\n",
      "Total PnL: 6685.37\n",
      "Total Trades: 1773\n",
      "Winning Trades: 689\n",
      "Win Rate: 38.86%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 1006685.37\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: 0.43%\n",
      "Total PnL: 4250.91\n",
      "Total Trades: 1342\n",
      "Winning Trades: 536\n",
      "Win Rate: 39.94%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 1004250.91\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: 0.67%\n",
      "Total PnL: 6685.37\n",
      "Total Trades: 1773\n",
      "Winning Trades: 689\n",
      "Win Rate: 38.86%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 1006685.37\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: 0.43%\n",
      "Total PnL: 4250.91\n",
      "Total Trades: 1342\n",
      "Winning Trades: 536\n",
      "Win Rate: 39.94%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 1004250.91\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: 0.67%\n",
      "Total PnL: 6685.37\n",
      "Total Trades: 1773\n",
      "Winning Trades: 689\n",
      "Win Rate: 38.86%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 1006685.37\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: 0.43%\n",
      "Total PnL: 4250.91\n",
      "Total Trades: 1342\n",
      "Winning Trades: 536\n",
      "Win Rate: 39.94%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 1004250.91\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: 0.67%\n",
      "Total PnL: 6685.37\n",
      "Total Trades: 1773\n",
      "Winning Trades: 689\n",
      "Win Rate: 38.86%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 1006685.37\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: 0.43%\n",
      "Total PnL: 4250.91\n",
      "Total Trades: 1342\n",
      "Winning Trades: 536\n",
      "Win Rate: 39.94%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 1004250.91\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: 0.67%\n",
      "Total PnL: 6685.37\n",
      "Total Trades: 1773\n",
      "Winning Trades: 689\n",
      "Win Rate: 38.86%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 1006685.37\n",
      "--------------------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=28.47 +/- 0.00\n",
      "Episode length: 21797.00 +/- 0.00\n",
      "New best mean reward!\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: 0.43%\n",
      "Total PnL: 4250.91\n",
      "Total Trades: 1342\n",
      "Winning Trades: 536\n",
      "Win Rate: 39.94%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 1004250.91\n",
      "--------------------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=505.12 +/- 0.00\n",
      "Episode length: 21797.00 +/- 0.00\n",
      "New best mean reward!\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -21.85%\n",
      "Total PnL: -218508.03\n",
      "Total Trades: 33413\n",
      "Winning Trades: 13852\n",
      "Win Rate: 41.46%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 781491.97\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -25.30%\n",
      "Total PnL: -253001.35\n",
      "Total Trades: 33579\n",
      "Winning Trades: 13779\n",
      "Win Rate: 41.03%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 746998.65\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -28.10%\n",
      "Total PnL: -280967.32\n",
      "Total Trades: 33494\n",
      "Winning Trades: 13633\n",
      "Win Rate: 40.70%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 719032.68\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -23.08%\n",
      "Total PnL: -230809.18\n",
      "Total Trades: 34863\n",
      "Winning Trades: 14180\n",
      "Win Rate: 40.67%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 769190.82\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -22.56%\n",
      "Total PnL: -225626.54\n",
      "Total Trades: 34506\n",
      "Winning Trades: 14156\n",
      "Win Rate: 41.02%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 774373.46\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -18.44%\n",
      "Total PnL: -184426.95\n",
      "Total Trades: 34992\n",
      "Winning Trades: 14378\n",
      "Win Rate: 41.09%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 815573.05\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -0.33%\n",
      "Total PnL: -3349.92\n",
      "Total Trades: 226\n",
      "Winning Trades: 77\n",
      "Win Rate: 34.07%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 996650.08\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -0.33%\n",
      "Total PnL: -3349.92\n",
      "Total Trades: 226\n",
      "Winning Trades: 77\n",
      "Win Rate: 34.07%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 996650.08\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -0.31%\n",
      "Total PnL: -3061.79\n",
      "Total Trades: 601\n",
      "Winning Trades: 236\n",
      "Win Rate: 39.27%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 996938.21\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -0.33%\n",
      "Total PnL: -3349.92\n",
      "Total Trades: 226\n",
      "Winning Trades: 77\n",
      "Win Rate: 34.07%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 996650.08\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -0.31%\n",
      "Total PnL: -3061.79\n",
      "Total Trades: 601\n",
      "Winning Trades: 236\n",
      "Win Rate: 39.27%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 996938.21\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -0.33%\n",
      "Total PnL: -3349.92\n",
      "Total Trades: 226\n",
      "Winning Trades: 77\n",
      "Win Rate: 34.07%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 996650.08\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -0.31%\n",
      "Total PnL: -3061.79\n",
      "Total Trades: 601\n",
      "Winning Trades: 236\n",
      "Win Rate: 39.27%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 996938.21\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -0.33%\n",
      "Total PnL: -3349.92\n",
      "Total Trades: 226\n",
      "Winning Trades: 77\n",
      "Win Rate: 34.07%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 996650.08\n",
      "--------------------------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=467.01 +/- 0.00\n",
      "Episode length: 21797.00 +/- 0.00\n",
      "New best mean reward!\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -0.31%\n",
      "Total PnL: -3061.79\n",
      "Total Trades: 601\n",
      "Winning Trades: 236\n",
      "Win Rate: 39.27%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 996938.21\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -0.31%\n",
      "Total PnL: -3061.79\n",
      "Total Trades: 601\n",
      "Winning Trades: 236\n",
      "Win Rate: 39.27%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 996938.21\n",
      "--------------------------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=566.60 +/- 0.00\n",
      "Episode length: 21797.00 +/- 0.00\n",
      "New best mean reward!\n",
      "\n",
      "Episode Summary:\n",
      "Episode Summary:\n",
      "\n",
      "Final Return: -20.81%\n",
      "Total PnL: -208090.34\n",
      "Final Return: -18.46%Total Trades: 26432\n",
      "\n",
      "Winning Trades: 10767Total PnL: -184579.18\n",
      "\n",
      "Episode Summary:Total Trades: 26234\n",
      "\n",
      "Winning Trades: 10754\n",
      "Win Rate: 40.73%\n",
      "Win Rate: 40.99%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 791909.66\n",
      "--------------------------------------------------\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 815420.82\n",
      "--------------------------------------------------\n",
      "\n",
      "Final Return: -18.65%\n",
      "Total PnL: -186506.67\n",
      "Total Trades: 26072\n",
      "Winning Trades: 10693\n",
      "Win Rate: 41.01%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 813493.33\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -18.85%\n",
      "Total PnL: -188539.13\n",
      "Total Trades: 28418\n",
      "Winning Trades: 11899\n",
      "Win Rate: 41.87%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 811460.87\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -19.85%\n",
      "Total PnL: -198479.25\n",
      "Total Trades: 27948\n",
      "Winning Trades: 11604\n",
      "Win Rate: 41.52%\n",
      "Final Return: -17.62%\n",
      "Total PnL: -176193.61Initial Balance: 1000000.00\n",
      "\n",
      "Total Trades: 27083\n",
      "Final Balance: 801520.75\n",
      "Winning Trades: 11254--------------------------------------------------\n",
      "\n",
      "Win Rate: 41.55%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 823806.39\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: 0.94%\n",
      "Total PnL: 9413.97\n",
      "Total Trades: 1061\n",
      "Winning Trades: 419\n",
      "Win Rate: 39.49%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 1009413.97\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: 0.94%\n",
      "Total PnL: 9413.97\n",
      "Total Trades: 1061\n",
      "Winning Trades: 419\n",
      "Win Rate: 39.49%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 1009413.97\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: 0.82%\n",
      "Total PnL: 8204.49\n",
      "Total Trades: 2296\n",
      "Winning Trades: 910\n",
      "Win Rate: 39.63%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 1008204.49\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: 0.94%\n",
      "Total PnL: 9413.97\n",
      "Total Trades: 1061\n",
      "Winning Trades: 419\n",
      "Win Rate: 39.49%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 1009413.97\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: 0.82%\n",
      "Total PnL: 8204.49\n",
      "Total Trades: 2296\n",
      "Winning Trades: 910\n",
      "Win Rate: 39.63%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 1008204.49\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: 0.94%\n",
      "Total PnL: 9413.97\n",
      "Total Trades: 1061\n",
      "Winning Trades: 419\n",
      "Win Rate: 39.49%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 1009413.97\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: 0.82%\n",
      "Total PnL: 8204.49\n",
      "Total Trades: 2296\n",
      "Winning Trades: 910\n",
      "Win Rate: 39.63%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 1008204.49\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: 0.94%\n",
      "Total PnL: 9413.97\n",
      "Total Trades: 1061\n",
      "Winning Trades: 419\n",
      "Win Rate: 39.49%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 1009413.97\n",
      "--------------------------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=35.01 +/- 0.00\n",
      "Episode length: 21797.00 +/- 0.00\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: 0.82%\n",
      "Total PnL: 8204.49\n",
      "Total Trades: 2296\n",
      "Winning Trades: 910\n",
      "Win Rate: 39.63%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 1008204.49\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: 0.82%\n",
      "Total PnL: 8204.49\n",
      "Total Trades: 2296\n",
      "Winning Trades: 910\n",
      "Win Rate: 39.63%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 1008204.49\n",
      "--------------------------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=15.80 +/- 0.00\n",
      "Episode length: 21797.00 +/- 0.00\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -13.10%\n",
      "Total PnL: -131017.59\n",
      "Total Trades: 24542\n",
      "Winning Trades: 10098\n",
      "Win Rate: 41.15%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 868982.41\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -12.38%\n",
      "Total PnL: -123827.50\n",
      "Total Trades: 25819\n",
      "Winning Trades: 10723\n",
      "Win Rate: 41.53%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 876172.50\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -10.75%\n",
      "Total PnL: -107549.58\n",
      "Total Trades: 25853\n",
      "Winning Trades: 10630\n",
      "Win Rate: 41.12%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 892450.42\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Episode Summary:\n",
      "\n",
      "Final Return: -24.96%\n",
      "Total PnL: -249582.65\n",
      "Final Return: -26.48%Total Trades: 25124\n",
      "\n",
      "Winning Trades: 10310\n",
      "Total PnL: -264759.45\n",
      "Win Rate: 41.04%\n",
      "Total Trades: 23122\n",
      "Winning Trades: 9291\n",
      "Win Rate: 40.18%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 750417.35\n",
      "Initial Balance: 1000000.00\n",
      "--------------------------------------------------\n",
      "Final Balance: 735240.55\n",
      "--------------------------------------------------\n",
      "\n",
      "Episode Summary:\n",
      "Final Return: -33.35%\n",
      "Total PnL: -333530.19\n",
      "Total Trades: 25948\n",
      "Winning Trades: 10634\n",
      "Win Rate: 40.98%\n",
      "Initial Balance: 1000000.00\n",
      "Final Balance: 666469.81\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 19:09:15,251 - Saved VecNormalize to ./optuna2/best_model_trial_1/vecnormalize.pkl\n",
      "/Users/floriankockler/Code/anaconda_env/SB5/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.balance to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.balance` for environment variables or `env.get_wrapper_attr('balance')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/floriankockler/Code/anaconda_env/SB5/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.total_trades to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.total_trades` for environment variables or `env.get_wrapper_attr('total_trades')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/floriankockler/Code/anaconda_env/SB5/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.winning_trades to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.winning_trades` for environment variables or `env.get_wrapper_attr('winning_trades')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "[I 2024-11-22 19:09:15,385] Trial 1 finished with value: 1000000.0 and parameters: {'realized_pnl_weight': 1.3020894401093006, 'unrealized_pnl_weight': 0.5997286355509384}. Best is trial 1 with value: 1000000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial 1 completed:\n",
      "Final Balance: $1,000,000.00\n",
      "Total Trades: 0\n",
      "Win Rate: 0.00%\n",
      "Training Time: 2102.6s\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 19:09:19,941 - Saved VecNormalize to ./optuna2/best_model_trial_0/vecnormalize.pkl\n",
      "[I 2024-11-22 19:09:20,019] Trial 0 finished with value: 1000000.0 and parameters: {'realized_pnl_weight': 1.1510216046112962, 'unrealized_pnl_weight': 0.9511521415268924}. Best is trial 0 with value: 1000000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial 0 completed:\n",
      "Final Balance: $1,000,000.00\n",
      "Total Trades: 0\n",
      "Win Rate: 0.00%\n",
      "Training Time: 2107.3s\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from dataclasses import dataclass\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "import os, sys\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "log_file = \"optuna_trials4.log\"  # Path to log file\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()  # For console output\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "from trading.environments.forex_env2 import ForexTradingEnv\n",
    "\n",
    "pair = \"EUR_USD\"\n",
    "parquet_path = Path(\"/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h\") / f\"{pair}.parquet\"\n",
    "norm_robust_path = Path('/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/normalized/eur_norm_robut.parquet')\n",
    "df = pd.read_parquet(norm_robust_path)\n",
    "\n",
    "dataset_manager = DatasetManager()\n",
    "train_df, val_df, test_df = dataset_manager.split_dataset(df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RewardParams:\n",
    "    \"\"\"Parameters controlling the reward function behavior.\"\"\"\n",
    "    realized_pnl_weight: float = 1.1\n",
    "    unrealized_pnl_weight: float = 0.8\n",
    "    holding_time_threshold: int = 7*12  # hours\n",
    "    holding_penalty_factor: float = -0.00001\n",
    "    max_trades_per_day: int = 6 \n",
    "    overtrading_penalty_factor: float = -0.0001\n",
    "    win_rate_threshold: float = 0.4\n",
    "    win_rate_bonus_factor: float = 0.0005\n",
    "    drawdown_penalty_factor: float = -0.0001\n",
    "\n",
    "@dataclass\n",
    "class OptimizationResult:\n",
    "    \"\"\"Stores results of a single trial.\"\"\"\n",
    "    trial_number: int\n",
    "    params: Dict\n",
    "    final_balance: float\n",
    "    total_trades: int\n",
    "    win_rate: float\n",
    "    max_drawdown: float\n",
    "    training_time: float\n",
    "\n",
    "class RewardOptimizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        val_df: pd.DataFrame,\n",
    "        study_name: str = \"forex_reward_optimization_optimized_only_2_params_robust_norm\",\n",
    "        n_timesteps: int = 500_000\n",
    "    ):\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.study_name = study_name\n",
    "        self.n_timesteps = n_timesteps\n",
    "        \n",
    "        # Setup study with TPE sampler and Median pruner\n",
    "        self.study = optuna.create_study(\n",
    "            study_name=study_name,\n",
    "            storage=\"sqlite:///optuna_trials.db\",\n",
    "            load_if_exists=True,\n",
    "            sampler=TPESampler(seed=42),\n",
    "            pruner=MedianPruner(\n",
    "                n_startup_trials=5,\n",
    "                n_warmup_steps=100_000,\n",
    "                interval_steps=50_000\n",
    "            ),\n",
    "            direction=\"maximize\"\n",
    "        )\n",
    "\n",
    "    def _create_env(self, df: pd.DataFrame, params: Dict, is_eval: bool = False, n_envs: int = 3) -> VecNormalize:\n",
    "        \"\"\"Create vectorized and normalized environment with multiple subprocesses.\"\"\"\n",
    "        def make_env():\n",
    "            \"\"\"Returns an environment creation function for use with vectorized environments.\"\"\"\n",
    "            def _init():\n",
    "                try:\n",
    "                    env = ForexTradingEnv(\n",
    "                        df=df.copy(),\n",
    "                        pair='EUR_USD',\n",
    "                        initial_balance=1_000_000,\n",
    "                        trade_size=100_000,\n",
    "                        reward_params=RewardParams(**params),\n",
    "                        sequence_length=10,\n",
    "                        random_start=False\n",
    "                    )\n",
    "                    env = Monitor(env)\n",
    "                    return env\n",
    "                except Exception as e:\n",
    "                    print(f\"Error creating environment: {str(e)}\")\n",
    "                    raise\n",
    "            return _init\n",
    "\n",
    "        try:\n",
    "            # Create environment builders with appropriate seeds\n",
    "            if is_eval:\n",
    "                envs = [make_env()]  # Single env for evaluation\n",
    "                vec_env = DummyVecEnv(envs)\n",
    "            else:\n",
    "                envs = [make_env() for _ in range(n_envs)]\n",
    "                vec_env = SubprocVecEnv(envs)\n",
    "\n",
    "            # Apply normalization\n",
    "            env = VecNormalize(\n",
    "                vec_env,\n",
    "                norm_obs=True,\n",
    "                norm_reward=not is_eval,\n",
    "                clip_obs=10.0,\n",
    "                clip_reward=10.0,\n",
    "                gamma=1.0,\n",
    "                epsilon=1e-08\n",
    "            )\n",
    "\n",
    "            # Set training mode appropriately\n",
    "            if is_eval:\n",
    "                env.training = False\n",
    "                env.norm_reward = False\n",
    "\n",
    "            return env\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in environment creation: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def objective(self, trial: optuna.Trial) -> float:\n",
    "        \"\"\"Optimization objective function.\"\"\"\n",
    "        def get_env_attribute(env, attr_name):\n",
    "            \"\"\"\n",
    "            Fetch an attribute from the environment, supporting SubprocVecEnv and single environments.\n",
    "            Args:\n",
    "                env: The environment (SubprocVecEnv, DummyVecEnv, or wrapped env).\n",
    "                attr_name (str): The name of the attribute to fetch.\n",
    "            Returns:\n",
    "                The attribute value, or None if not found.\n",
    "            \"\"\"\n",
    "            if hasattr(env, \"envs\"):  # SubprocVecEnv or DummyVecEnv\n",
    "                # Get the attribute from the first environment in the vectorized stack\n",
    "                try:\n",
    "                    print(f\"Fetching attribute {attr_name} : {env.get_attr(attr_name, indices=0)} from the first environment in the vectorized stack.\")\n",
    "                    return env.get_attr(attr_name, indices=0)\n",
    "                except AttributeError:\n",
    "                    print(f\"Attribute {attr_name} : {env.get_attr(attr_name, indices=0)} not found in the first environment.\")\n",
    "                    # Use the latest Gymnasium recommendation: get_wrapper_attr\n",
    "                    return env.get_wrapper_attr(attr_name)\n",
    "            else:  # Single unwrapped environment\n",
    "                print(f\"Fetching attribute {attr_name} : {getattr(env.unwrapped, attr_name, None)} from the unwrapped environment.\")\n",
    "                return getattr(env.unwrapped, attr_name, None)\n",
    "        try:\n",
    "            # Sample parameters\n",
    "            params = self._sample_parameters(trial)\n",
    "            \n",
    "\n",
    "            # Create environments\n",
    "            train_env = self._create_env(self.train_df, params, is_eval=False, n_envs=3)\n",
    "            eval_env = self._create_env(self.val_df, params, is_eval=True, n_envs=1)\n",
    "            \n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            # Create model\n",
    "            model = PPO(\n",
    "                \"MultiInputPolicy\",\n",
    "                train_env,\n",
    "                verbose=0,\n",
    "                tensorboard_log=f\"./tensorboard4/short_trial_{trial.number}\"\n",
    "            )\n",
    "\n",
    "            # Setup evaluation callback\n",
    "            eval_callback = EvalCallback(\n",
    "                eval_env,\n",
    "                best_model_save_path=f\"./models/short_trial_{trial.number}\",\n",
    "                log_path=f\"./logs/trial_{trial.number}\",\n",
    "                eval_freq=100_000,\n",
    "                deterministic=True,\n",
    "                render=False\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            model.learn(\n",
    "                total_timesteps=self.n_timesteps,\n",
    "                callback=eval_callback\n",
    "            )\n",
    "      \n",
    "            try:\n",
    "                save_path = f'./optuna2/best_model_trial_{trial.number}/'\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                train_env.save(os.path.join(save_path, 'vecnormalize.pkl'))\n",
    "                logging.info(f\"Saved VecNormalize to {os.path.join(save_path, 'vecnormalize.pkl')}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to save VecNormalize: {e}\")\n",
    "                raise\n",
    "            \n",
    "            # Get final balance from eval environment\n",
    "            #! Below use oudated code, but worked before\n",
    "      \n",
    "            try:\n",
    "                final_balance = eval_env.get_attr('balance')[0]\n",
    "                total_trades = eval_env.get_attr('total_trades')[0]\n",
    "                win_rate = eval_env.get_attr('winning_trades')[0] / max(1, total_trades)\n",
    "                # final_balance = float(eval_env.env_method('get_attr', 'balance')[0][0])\n",
    "                # total_trades = int(eval_env.env_method('get_attr', 'total_trades')[0][0])\n",
    "                # winning_trades = int(eval_env.env_method('get_attr', 'winning_trades')[0][0])\n",
    "                # win_rate = winning_trades / max(1, total_trades)\n",
    "                \n",
    "                # Log results\n",
    "                training_time = (datetime.now() - start_time).total_seconds()\n",
    "                print(f\"\\nTrial {trial.number} completed:\")\n",
    "                print(f\"Final Balance: ${final_balance:,.2f}\")\n",
    "                print(f\"Total Trades: {total_trades}\")\n",
    "                print(f\"Win Rate: {win_rate:.2%}\")\n",
    "                print(f\"Training Time: {training_time:.1f}s\")\n",
    "                print(\"-\" * 80)\n",
    "                \n",
    "                return final_balance\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error getting evaluation metrics: {str(e)}\")\n",
    "                return float('-inf')\n",
    "                \n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Trial {trial.number} failed: {str(e)}\")\n",
    "            return float('-inf')\n",
    "        finally:\n",
    "            if 'train_env' in locals():\n",
    "                train_env.close()\n",
    "            if 'eval_env' in locals():\n",
    "                eval_env.close()\n",
    "\n",
    "    def _sample_parameters(self, trial: optuna.Trial) -> Dict:\n",
    "        \"\"\"Sample reward parameters for trial.\"\"\"\n",
    "        return {\n",
    "            'realized_pnl_weight': trial.suggest_float('realized_pnl_weight', 1.0, 1.5),\n",
    "            'unrealized_pnl_weight': trial.suggest_float('unrealized_pnl_weight', 0.5, 1.0),\n",
    "            # 'holding_time_threshold': trial.suggest_int('holding_time_threshold', 24, 96),\n",
    "            # 'holding_penalty_factor': trial.suggest_float('holding_penalty_factor', -0.0001, 0.0),\n",
    "            # 'max_trades_per_day': trial.suggest_int('max_trades_per_day', 3, 12),\n",
    "            # 'overtrading_penalty_factor': trial.suggest_float('overtrading_penalty_factor', -0.0001, 0.0),\n",
    "            # 'win_rate_threshold': trial.suggest_float('win_rate_threshold', 0.3, 0.5),\n",
    "            # 'win_rate_bonus_factor': trial.suggest_float('win_rate_bonus_factor', 0.0001, 0.001, log=True),\n",
    "            # 'drawdown_penalty_factor': trial.suggest_float('drawdown_penalty_factor', -0.001, 0.0)\n",
    "        }\n",
    "\n",
    "    def optimize(self, n_trials: int = 100, n_jobs: int = 6) -> None:\n",
    "        \"\"\"Run optimization using Optuna's built-in parallelization.\"\"\"\n",
    "        self.study.optimize(\n",
    "            self.objective,\n",
    "            n_trials=n_trials,\n",
    "            n_jobs=n_jobs,  # Number of parallel jobs\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "            \n",
    "        # Print best trial after completion\n",
    "        print(\"\\nOptimization completed!\")\n",
    "        print(\"\\nBest trial:\")\n",
    "        trial = self.study.best_trial\n",
    "        print(f\"Value: ${trial.value:,.2f}\")\n",
    "        print(\"Best parameters:\")\n",
    "        for key, value in trial.params.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "\n",
    "\n",
    "optimizer = RewardOptimizer(\n",
    "    train_df=train_df,\n",
    "    val_df=val_df,\n",
    "    n_timesteps=1_000_000\n",
    ")\n",
    "\n",
    "optimizer.optimize(n_trials=10, n_jobs=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SB5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
