{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import random\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "from trading.environments.forex_env2_flat_simple import ForexTradingEnv\n",
    "# from trading.environments.forex_env2_flat_simple import ForexTradingEnv2 as ForexTradingEnv\n",
    "# from trading.environments.forex_env_flat_multi_pair import MultipairForexTradingEnv\n",
    "\n",
    "from stable_baselines3.common.callbacks import EvalCallback, BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "\n",
    "th.set_num_threads(3)\n",
    "N_ENVS = 3  # Number of parallel environments\n",
    "EVAL_FREUQENCY = 500_000\n",
    "EVAL_FREQ_ADJUSTED = int(EVAL_FREUQENCY / N_ENVS)\n",
    "\n",
    "hourly_dir = \"/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/unbiased/not_norm/train2/\"\n",
    "source_path = '/Volumes/ssd_fat2/ai6_trading_bot/datasets/5min/df_with_all_indics_unbiased/not_norm/train2/'\n",
    "source_dfs = [os.path.join(hourly_dir, f) for f in os.listdir(hourly_dir) if f.endswith('.parquet') and not f.startswith('.') and 'validate' not in f]\n",
    "\n",
    "eval_path = '/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/unbiased/not_norm/train2/EUR_GBP_validate.parquet'\n",
    "sequence = 5\n",
    "saving_path = f'/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/unbiased/not_norm/train2/results/'\n",
    "os.makedirs(saving_path, exist_ok=True)\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    th.manual_seed(seed)\n",
    "    th.backends.cudnn.deterministic = True\n",
    "\n",
    "set_all_seeds(42)\n",
    "\n",
    "class ForexTensorboardCallback(BaseCallback):\n",
    "    \"\"\"Custom callback for logging Forex trading metrics to tensorboard.\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_returns = []  # Track episode returns for averaging\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"Called after each step in the environment.\"\"\"\n",
    "        # infos is a list of dictionaries, one from each parallel environment\n",
    "        for info in self.locals['infos']:\n",
    "            if info is None:  # Skip if no info (can happen at episode boundaries)\n",
    "                continue\n",
    "                \n",
    "            # Log account metrics\n",
    "            self.logger.record(\"metrics/balance\", info['balance'])\n",
    "            # self.logger.record(\"metrics/total_return_pct\", info['total_return_pct'])\n",
    "            # self.logger.record(\"metrics/net_profit\", info['net_profit'])\n",
    "            \n",
    "            # Log trade metrics\n",
    "            # self.logger.record(\"metrics/total_pnl\", info['total_pnl'])\n",
    "            # self.logger.record(\"metrics/total_trades\", info['total_trades'])\n",
    "            # self.logger.record(\"metrics/win_rate\", info['win_rate'])\n",
    "            \n",
    "            # Log cost metrics\n",
    "            self.logger.record(\"metrics/transaction_costs\", info['transaction_costs'])\n",
    "            # self.logger.record(\"metrics/transaction_costs_pct\", info['transaction_costs_pct'])\n",
    "            \n",
    "            # Log position metrics\n",
    "            self.logger.record(\"metrics/position_size_pct\", info['position_size_pct'])\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"Called at the end of a rollout.\"\"\"\n",
    "        # Episode metrics are handled automatically by stable-baselines3\n",
    "        pass\n",
    "\n",
    "class DetailedEvalCallback(EvalCallback):\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        Performs evaluation with detailed metric logging throughout the evaluation episodes.\n",
    "        \"\"\"\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            # Store episode rewards for calculating mean\n",
    "            episode_rewards = []\n",
    "            episode_lengths = []\n",
    "            \n",
    "            # For each evaluation episode\n",
    "            for _ in range(self.n_eval_episodes):\n",
    "                episode_reward = 0\n",
    "                episode_length = 0\n",
    "                done = False\n",
    "                # VecEnv reset returns just the obs\n",
    "                obs = self.eval_env.reset()\n",
    "                \n",
    "                # Run episode until done\n",
    "                while not done:\n",
    "                    # Get deterministic action\n",
    "                    action, _ = self.model.predict(obs, deterministic=True)\n",
    "                    # VecEnv step returns (obs, reward, done, info)\n",
    "                    obs, reward, done, info = self.eval_env.step(action)\n",
    "                    episode_reward += reward[0]  # reward is a numpy array\n",
    "                    episode_length += 1\n",
    "                    \n",
    "                    # Log metrics at each step\n",
    "                    if info[0] is not None:  # info is a list of dicts\n",
    "                        info = info[0]  # Get info dict from first env\n",
    "                        self.logger.record(\"eval/balance\", info.get('balance', 0))\n",
    "                        self.logger.record(\"eval/total_pnl\", info.get('total_pnl', 0))\n",
    "                        # self.logger.record(\"eval/total_trades\", info.get('total_trades', 0))\n",
    "                        # self.logger.record(\"eval/win_rate\", info.get('win_rate', 0))\n",
    "                        self.logger.record(\"eval/transaction_costs\", info.get('transaction_costs', 0))\n",
    "                        # Dump metrics at each step\n",
    "                        self.logger.dump(self.n_calls)\n",
    "                \n",
    "                episode_rewards.append(episode_reward)\n",
    "                episode_lengths.append(episode_length)\n",
    "\n",
    "            # Calculate mean metrics across episodes\n",
    "            mean_reward = np.mean(episode_rewards)\n",
    "            mean_length = np.mean(episode_lengths)\n",
    "            \n",
    "            self.logger.record(\"eval/mean_reward\", mean_reward)\n",
    "            self.logger.record(\"eval/mean_episode_length\", mean_length)\n",
    "\n",
    "            # Update best model if needed\n",
    "            if self.best_model_save_path is not None:\n",
    "                if self.verbose >= 1:\n",
    "                    print(f\"Evaluating the current model: {mean_reward:.2f}\")\n",
    "                \n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    if self.verbose >= 1:\n",
    "                        print(f\"New best mean reward: {mean_reward:.2f} \"\n",
    "                              f\"(previous: {self.best_mean_reward:.2f})\")\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    self.model.save(self.best_model_save_path)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _get_eval_info(self):\n",
    "        \"\"\"Helper method to get the last info dict from eval environment.\"\"\"\n",
    "        try:\n",
    "            # Try to get info directly from environment\n",
    "            if hasattr(self.eval_env, 'get_info'):\n",
    "                return self.eval_env.get_info()\n",
    "            # If that's not available, try to get it from the unwrapped env\n",
    "            elif hasattr(self.eval_env, 'envs'):\n",
    "                return self.eval_env.envs[0].get_info()\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not get eval info: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "def make_train_env(rank):\n",
    "    def _init():\n",
    "        env = ForexTradingEnv(\n",
    "            df_paths=source_dfs,\n",
    "            eval_mode=False,\n",
    "            sequence_length=sequence,\n",
    "        )\n",
    "        env = Monitor(env)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "\n",
    "train_env = SubprocVecEnv([make_train_env(i) for i in range(N_ENVS)])\n",
    "train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)\n",
    "\n",
    "\n",
    "\n",
    "def make_eval_env():\n",
    "    env = ForexTradingEnv(\n",
    "        df_paths=source_dfs,\n",
    "        eval_path=eval_path,\n",
    "        eval_mode=True,\n",
    "        pair='EUR_GBP',\n",
    "        sequence_length=sequence,\n",
    "\n",
    "\n",
    "    )\n",
    "    env = Monitor(env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=False)\n",
    "    env.training = False\n",
    "    return env\n",
    "\n",
    "\n",
    "eval_env = make_eval_env()\n",
    "\n",
    "eval_callback = DetailedEvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=f'{saving_path}eval_best_model_new_reward/',\n",
    "    log_path=saving_path,\n",
    "    eval_freq=EVAL_FREQ_ADJUSTED,\n",
    "    n_eval_episodes=5,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "# eval_callback = EvalCallback(\n",
    "#     eval_env,\n",
    "#     best_model_save_path=saving_path,\n",
    "#     log_path=saving_path,\n",
    "#     eval_freq=EVAL_FREQ_ADJUSTED,  # Adjust as needed\n",
    "#     n_eval_episodes=5,\n",
    "#     deterministic=True,\n",
    "#     render=False\n",
    "# )\n",
    "\n",
    "# model = PPO(\n",
    "#     'MlpPolicy',\n",
    "#     train_env,\n",
    "#     verbose=0,\n",
    "#     tensorboard_log=f'{saving_path}sequence_{sequence}__PPO_1h_no_costs_50k_balance_reduced_LSTM',\n",
    "# )\n",
    "# Define policy kwargs for the LSTM configuration\n",
    "# policy_kwargs = dict(\n",
    "#     # Network Architecture\n",
    "#     net_arch=dict(\n",
    "#         # Actor (policy) network\n",
    "#         pi=[256, 128],  # Larger first layer to process high-dimensional input\n",
    "#         # Critic (value) network\n",
    "#         vf=[256, 128]   # Match actor architecture for balanced learning\n",
    "#     ),\n",
    "    \n",
    "#     # LSTM Configuration\n",
    "#     lstm_hidden_size=256,      # Larger hidden size to capture complex patterns\n",
    "#     n_lstm_layers=2,           # Multiple layers for hierarchical feature learning\n",
    "#     enable_critic_lstm=True,   # Share temporal understanding between actor and critic\n",
    "    \n",
    "#     # LSTM specific parameters\n",
    "#     lstm_kwargs=dict(\n",
    "#         dropout=0.2            # Slightly higher dropout for regularization\n",
    "#     )\n",
    "# )\n",
    "\n",
    "policy_kwargs_complex = dict(\n",
    "    net_arch=dict(\n",
    "        pi=[512, 256, 128],\n",
    "        vf=[512, 256, 128]\n",
    "    ),\n",
    "    lstm_hidden_size=512,\n",
    "    n_lstm_layers=3,\n",
    "    enable_critic_lstm=True,\n",
    "    lstm_kwargs=dict(\n",
    "        dropout=0.25\n",
    "    )\n",
    ")\n",
    "\n",
    "policy_kwargs_memory_efficient = dict(\n",
    "    net_arch=dict(\n",
    "        pi=[256, 128],\n",
    "        vf=[256, 128]\n",
    "    ),\n",
    "    lstm_hidden_size=256,\n",
    "    n_lstm_layers=1,\n",
    "    lstm_kwargs=dict(\n",
    "        dropout=0.1\n",
    "    )\n",
    ")\n",
    "\n",
    "model = RecurrentPPO(\n",
    "    'MlpLstmPolicy',\n",
    "    train_env,\n",
    "    verbose=0,\n",
    "    seed=42,\n",
    "    tensorboard_log=f'{saving_path}sequence_{sequence}__PPO_1h_no_costs_50k_balance_reduced_LSTM2/',\n",
    "    policy_kwargs=policy_kwargs_memory_efficient,\n",
    ")\n",
    "callbacks = [\n",
    "    ForexTensorboardCallback(),\n",
    "    eval_callback\n",
    "]\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=10_000_000,  # Adjust as needed\n",
    "    callback=callbacks\n",
    ")\n",
    "\n",
    "model.save(f'{saving_path}{sequence}_best_model_core.zip')\n",
    "train_env.save(f'{saving_path}{sequence}_vec_normalize_core.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "from typing import List, Optional\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Optional\n",
    "import logging\n",
    "\n",
    "def convert_5min_to_hourly(source_path: str, output_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert 5-minute data with daily indicators to hourly data while preserving indicator values.\n",
    "    \n",
    "    Args:\n",
    "        source_path: Path to source 5-minute parquet file\n",
    "        output_path: Path to save the hourly data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with hourly data\n",
    "    \"\"\"\n",
    "    # Read 5-minute data\n",
    "    df_5min = pd.read_parquet(source_path)\n",
    "    \n",
    "    # Resample OHLC data to hourly\n",
    "    df_hourly = df_5min.resample('1H').agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last'\n",
    "    })\n",
    "    \n",
    "    # For daily indicators, we can take the last value of each hour\n",
    "    # since they only change once per day anyway\n",
    "    indicator_columns = [col for col in df_5min.columns \n",
    "                        if col not in ['open', 'high', 'low', 'close']]\n",
    "    \n",
    "    for col in indicator_columns:\n",
    "        df_hourly[col] = df_5min[col].resample('1H').last()\n",
    "    \n",
    "    # Remove any NaN rows\n",
    "    df_hourly = df_hourly.dropna()\n",
    "    \n",
    "    # Save to parquet\n",
    "    df_hourly.to_parquet(output_path)\n",
    "    \n",
    "    return df_hourly\n",
    "\n",
    "def process_currency_pairs(currency_pairs: List[str], \n",
    "                         source_dir: str,\n",
    "                         output_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Process multiple currency pairs from 5min to hourly data.\n",
    "    \n",
    "    Args:\n",
    "        currency_pairs: List of currency pairs to process\n",
    "        source_dir: Directory containing 5min data files\n",
    "        output_dir: Directory to save hourly data files\n",
    "    \"\"\"\n",
    "    for pair in currency_pairs:\n",
    "        try:\n",
    "            source_path = f\"{source_dir}/{pair}_5min_1D_not_norm_10dec.parquet\"\n",
    "            output_path = f\"{output_dir}/{pair}_1h_1D_not_norm_unbiased.parquet\"\n",
    "            \n",
    "            print(f\"Processing {pair}...\")\n",
    "            df_hourly = convert_5min_to_hourly(source_path, output_path)\n",
    "            print(f\"Completed {pair}. Shape: {df_hourly.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pair}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    source_dir = \"/Volumes/ssd_fat2/ai6_trading_bot/datasets/5min/df_with_all_indics_unbiased/not_norm\"\n",
    "    output_dir = \"/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/unbiased/not_norm\"\n",
    "    \n",
    "    currency_pairs = [\n",
    "        'XAU_USD', 'XAG_USD', \n",
    "    ]\n",
    "    \n",
    "    process_currency_pairs(currency_pairs, source_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/unbiased/not_norm/EUR_USD_1h_1D_not_norm_unbiased.parquet\"\n",
    "df = pd.read_parquet(output_dir)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "from typing import List, Optional\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from unbiased_data import process_currency_pairs, prepare_unbiased_dataset_row_by_row\n",
    "\n",
    "from data_management.indicator_manager import IndicatorManager\n",
    "from data_management.preprocessor import DataPreprocessor\n",
    "\n",
    "indicator_manager = IndicatorManager()\n",
    "processor = DataPreprocessor()\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('dataset_prep.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger('dataset_prep')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# currencies_1 = [\n",
    "#             'GBP_CHF', 'GBP_JPY', 'EUR_CHF', \n",
    " \n",
    "#         ]\n",
    "currencies_2 = [\n",
    "\n",
    "            'EUR_CAD', 'EUR_USD', 'GBP_USD', \n",
    "    \n",
    "        ]\n",
    "currencies_3 = [\n",
    "\n",
    "            'AUD_USD', 'CHF_JPY', \n",
    " \n",
    "        ]\n",
    "# currencies_4 = [\n",
    "\n",
    "#             'NZD_JPY', 'XAU_USD', 'XAG_USD', \n",
    "#         ]\n",
    "currencies_5 = [\n",
    "\n",
    "            'USD_CHF', 'USD_JPY', 'AUD_JPY', \n",
    "        ]\n",
    "# currencies_6 = [\n",
    "\n",
    "#             'EUR_JPY', 'EUR_GBP', 'NZD_USD',\n",
    "#         ]\n",
    "\n",
    "eur_only = ['EUR_USD']\n",
    "\n",
    "for ccy in currencies_2:\n",
    "\n",
    "    logger.info(f'Starting processing for {ccy} at {pd.Timestamp.now()}')\n",
    "    df = pd.read_parquet(f'/Volumes/ssd_fat2/ai6_trading_bot/datasets/1min/{ccy}.parquet')\n",
    "    # df = df.head(3_000_000)\n",
    "    \n",
    "    df_with_indicators = prepare_unbiased_dataset_row_by_row(\n",
    "                df=df,\n",
    "                indicator_manager=indicator_manager,\n",
    "                indicator_timeframe='D',\n",
    "                verbose=True\n",
    "            )\n",
    "    df_with_indicators = df_with_indicators.dropna()\n",
    "    \n",
    "    output_path_not_norm = f'/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/unbiased/not_norm/{ccy}_1h_1D_not_norm_unbiased.parquet'\n",
    "    df_with_indicators.to_parquet(output_path_not_norm)\n",
    "    \n",
    "    # df_norm = processor.normalize_simple(df=df_with_indicators)\n",
    "    \n",
    "    # output_path = f'./{ccy}_5min_1D_norm_unbiased_dll_indics_09dec.parquet'\n",
    "    # df_norm.to_parquet(output_path)\n",
    "    \n",
    "    logger.info(f'Finished processing for {ccy} at {pd.Timestamp.now()}')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "from typing import List, Optional\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from visualization.chart_manager import ChartManager\n",
    "chart_manager = ChartManager()\n",
    "\n",
    "\n",
    "df = pd.read_parquet('EUR_USD_5min_1D_all_indic_not_norm_unbiased.parquet')\n",
    "# print(\"Contains inf:\", df.isin([float('inf'), float('-inf')]).any().any())\n",
    "# print(\"Contains NaN:\", df.isna().any().any())\n",
    "start_time = pd.Timestamp('2022-09-27 08:00').tz_localize('UTC')\n",
    "end_time = pd.Timestamp('2024-11-27 09:00').tz_localize('UTC')\n",
    "\n",
    "# chart_manager.chart(df, start_time, end_time)\n",
    "# df\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True, \n",
    "                    vertical_spacing=0.1, \n",
    "                    subplot_titles=('Ichimoku Cloud', 'MACD'))\n",
    "\n",
    "# Add traces for first subplot\n",
    "fig.add_trace(go.Scatter(x=df.index, y=df['close'], name='Close', line=dict(color='blue')), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=df.index, y=df['senkou_span_a'], name='Senkou Span A', line=dict(color='green')), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=df.index, y=df['senkou_span_b'], name='Senkou Span B', line=dict(color='red')), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=df.index, y=df['tenkan_sen'], name='Tenkan Sen', line=dict(color='orange')), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=df.index, y=df['kijun_sen'], name='Kijun Sen', line=dict(color='purple')), row=1, col=1)\n",
    "\n",
    "# Add traces for second subplot\n",
    "fig.add_trace(go.Scatter(x=df.index, y=df['macd_signal'], name='MACD Signal', line=dict(color='orange')), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=df.index, y=df['macd'], name='MACD', line=dict(color='blue')), row=2, col=1)\n",
    "fig.add_trace(go.Bar(x=df.index, y=df['macd_hist'], name='MACD Histogram', marker_color='gray'), row=2, col=1)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(height=800, width=1200, showlegend=True)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('/Volumes/ssd_fat2/ai6_trading_bot/datasets/5min/df_with_all_indics_unbiased/not_norm/GBP_JPY_5min_1D_not_norm_unbiased_dll_indics_09dec.parquet')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Define start and end dates for the chart\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2023-12-31'\n",
    "\n",
    "# Read and filter data\n",
    "df = pd.read_parquet('/Volumes/ssd_fat2/ai6_trading_bot/datasets/5min/df_with_all_indics_unbiased/not_norm/AUD_USD_5min_1D_not_norm_unbiased_dll_indics_09dec.parquet')\n",
    "df_filtered = df.loc[start_date:end_date]\n",
    "\n",
    "# Create the figure and subplots\n",
    "# figsize=(12, 8) creates a figure 12 inches wide and 8 inches tall\n",
    "fig, (ax1, ax2, ax3, ax4, ax5,ax6,ax7,ax8,ax9,ax10,ax11,ax12) = plt.subplots(12, 1, figsize=(12, 40), height_ratios=[2, 1,1,1,1,1,1,1,1,1,1,1])\n",
    "\n",
    "# Plot Ichimoku Cloud on the first subplot\n",
    "ax1.plot(df_filtered.index, df_filtered['close'], label='Close', color='blue')\n",
    "ax1.plot(df_filtered.index, df_filtered['senkou_span_a'], label='Senkou Span A', color='green')\n",
    "ax1.plot(df_filtered.index, df_filtered['senkou_span_b'], label='Senkou Span B', color='red')\n",
    "ax1.plot(df_filtered.index, df_filtered['tenkan_sen'], label='Tenkan Sen', color='orange')\n",
    "ax1.plot(df_filtered.index, df_filtered['kijun_sen'], label='Kijun Sen', color='purple')\n",
    "\n",
    "\n",
    "\n",
    "# Plot MACD on the second subplot\n",
    "ax2.plot(df_filtered.index, df_filtered['macd'], label='MACD', color='blue')\n",
    "ax2.plot(df_filtered.index, df_filtered['macd_signal'], label='Signal', color='orange')\n",
    "ax2.bar(df_filtered.index, df_filtered['macd_hist'], label='Histogram', color='gray', alpha=0.3)\n",
    "\n",
    "# Plot roc_10 \n",
    "ax3.plot(df_filtered.index, df_filtered['roc_10'], label='roc_10', color='blue')\n",
    "\n",
    "# Plot stoch_rsi \n",
    "ax4.plot(df_filtered.index, df_filtered['stoch_rsi'], label='stoch_rsi', color='blue')\n",
    "\n",
    "# Plot stoch \n",
    "ax5.plot(df_filtered.index, df_filtered['stoch_k'], label='stoch_k', color='blue',linewidth=0.3)\n",
    "ax5.plot(df_filtered.index, df_filtered['stoch_d'], label='stoch_d', color='green',linewidth=0.3)\n",
    "\n",
    "# Plot bollinger \n",
    "ax6.plot(df_filtered.index, df_filtered['bb_upper'], label='bb_upper', color='blue', linewidth=0.3)\n",
    "ax6.plot(df_filtered.index, df_filtered['bb_middle'], label='bb_middle', color='green',linewidth=0.3)\n",
    "ax6.plot(df_filtered.index, df_filtered['bb_lower'], label='bb_lower', color='red',linewidth=0.3)\n",
    "ax6.plot(df_filtered.index, df_filtered['close'], label='close', color='black', linewidth=0.5)\n",
    "\n",
    "# Plot bb_bandwidth \n",
    "ax7.plot(df_filtered.index, df_filtered['bb_bandwidth'], label='bb_bandwidth', color='black')\n",
    "\n",
    "# Plot bb_percent \n",
    "ax8.plot(df_filtered.index, df_filtered['bb_percent'], label='bb_percent', color='black')\n",
    "\n",
    "# Plot atr \n",
    "ax9.plot(df_filtered.index, df_filtered['atr'], label='atr', color='black')\n",
    "\n",
    "# Plot DMI \n",
    "ax10.plot(df_filtered.index, df_filtered['plus_di'], label='plus_di', color='green',linewidth=0.3)\n",
    "ax10.plot(df_filtered.index, df_filtered['minus_di'], label='minus_di', color='red',linewidth=0.3)\n",
    "\n",
    "\n",
    "\n",
    "# Plot adx \n",
    "ax11.plot(df_filtered.index, df_filtered['adx'], label='adx', color='black')\n",
    "# Plot rsi \n",
    "ax12.plot(df_filtered.index, df_filtered['rsi'], label='rsi', color='black')\n",
    "\n",
    "# Customize the appearance\n",
    "ax1.set_title('Ichimoku Cloud')\n",
    "ax2.set_title('MACD')\n",
    "ax3.set_title('roc_10')\n",
    "ax4.set_title('stoch_rsi')\n",
    "ax5.set_title('stoch')\n",
    "ax6.set_title('bollinger')\n",
    "ax7.set_title('bb_bandwidth')\n",
    "ax8.set_title('bb_percent')\n",
    "ax9.set_title('atr')\n",
    "ax10.set_title('dmi')\n",
    "ax11.set_title('adx')\n",
    "ax12.set_title('rsi')\n",
    "\n",
    "# Add legends\n",
    "ax1.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "ax2.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "ax3.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Adjust the layout to prevent overlapping\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from unbiased_data import process_currency_pairs, prepare_unbiased_dataset_row_by_row\n",
    "\n",
    "from data_management.indicator_manager import IndicatorManager\n",
    "from data_management.preprocessor import DataPreprocessor\n",
    "\n",
    "indicator_manager = IndicatorManager()\n",
    "processor = DataPreprocessor()\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('dataset_prep.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger('dataset_prep')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "currencies_1 = [\n",
    "            'GBP_CHF', 'GBP_JPY', 'EUR_CHF', \n",
    " \n",
    "        ]\n",
    "currencies_2 = [\n",
    "\n",
    "            'EUR_CAD', 'EUR_USD', 'GBP_USD', \n",
    "    \n",
    "        ]\n",
    "currencies_3 = [\n",
    "\n",
    "            'USD_CAD', 'AUD_USD', 'CHF_JPY', \n",
    " \n",
    "        ]\n",
    "currencies_4 = [\n",
    "\n",
    "            'NZD_JPY', 'XAU_USD', 'XAG_USD', \n",
    "        ]\n",
    "currencies_5 = [\n",
    "\n",
    "            'USD_CHF', 'USD_JPY', 'AUD_JPY', \n",
    "        ]\n",
    "currencies_6 = [\n",
    "\n",
    "            'EUR_JPY', 'EUR_GBP', 'NZD_USD',\n",
    "        ]\n",
    "\n",
    "eur_only = ['EUR_USD']\n",
    "\n",
    "for ccy in eur_only:\n",
    "\n",
    "    logger.info(f'Starting processing for {ccy} at {pd.Timestamp.now()}')\n",
    "    df = pd.read_parquet(f'/Users/floriankockler/Library/CloudStorage/OneDrive-kockler/usb_stick_6dec/1min_source/{ccy}.parquet')\n",
    "    # df = df.head(3_000_000)\n",
    "    \n",
    "    df_with_indicators = prepare_unbiased_dataset_row_by_row(\n",
    "                df=df,\n",
    "                indicator_manager=indicator_manager,\n",
    "                indicator_timeframe='D',\n",
    "                verbose=True\n",
    "            )\n",
    "    df_with_indicators = df_with_indicators.dropna()\n",
    "    \n",
    "    output_path_not_norm = f'./{ccy}_5min_1D_indic_not_norm_unbiased_full.parquet'\n",
    "    df_with_indicators.to_parquet(output_path_not_norm)\n",
    "    \n",
    "    df_norm = processor.normalize_simple(df=df_with_indicators)\n",
    "    \n",
    "    output_path = f'./{ccy}_5min_1D_norm_unbiased_full.parquet'\n",
    "    df_norm.to_parquet(output_path)\n",
    "    \n",
    "    logger.info(f'Finished processing for {ccy} at {pd.Timestamp.now()}')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = f'/Users/floriankockler/Code/GitHub.nosync/ai6-gcp-bot/forex_trading_system/notebooks/EUR_USD_5min_1H_norm_unbiased.parquet'\n",
    "df = pd.read_parquet(train_set)\n",
    "# df.isna().any()\n",
    "# np.isinf(df).any()\n",
    "\n",
    "df['bb_percent'] = df['bb_percent'].replace([np.inf, -np.inf], [1, 0])\n",
    "df.to_parquet(\"/Users/floriankockler/Code/GitHub.nosync/ai6-gcp-bot/forex_trading_system/notebooks/EUR_USD_5min_1H_norm_unbiased1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_norm = pd.read_parquet(f'/Volumes/ssd_fat2/ai6_trading_bot/datasets/1min/{ccy}.parquet')\n",
    "not_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = pd.read_parquet(f'./{ccy}_5min_1H_norm_unbiased.parquet')\n",
    "norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_parquet(f'/Volumes/ssd_fat2/ai6_trading_bot/datasets/1min/EUR_USD.parquet')\n",
    "# df_test = df.head(5000)\n",
    "# df_test.to_parquet(f'/Volumes/ssd_fat2/ai6_trading_bot/datasets/1min/EUR_USD_test.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SB5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
