{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sb3-contrib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import random\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "from trading.environments.forex_env2_flat_simple import ForexTradingEnv\n",
    "# from trading.environments.forex_env2_flat_simple import ForexTradingEnv2 as ForexTradingEnv\n",
    "# from trading.environments.forex_env_flat_multi_pair import MultipairForexTradingEnv\n",
    "\n",
    "from stable_baselines3.common.callbacks import EvalCallback, BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "\n",
    "th.set_num_threads(3)\n",
    "N_ENVS = 3  # Number of parallel environments\n",
    "EVAL_FREUQENCY = 500_000\n",
    "EVAL_FREQ_ADJUSTED = int(EVAL_FREUQENCY / N_ENVS)\n",
    "\n",
    "hourly_dir = \"/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/unbiased/not_norm/train2/\"\n",
    "source_path = '/Volumes/ssd_fat2/ai6_trading_bot/datasets/5min/df_with_all_indics_unbiased/not_norm/train2/'\n",
    "source_dfs = [os.path.join(hourly_dir, f) for f in os.listdir(hourly_dir) if f.endswith('.parquet') and not f.startswith('.') and 'validate' not in f]\n",
    "\n",
    "eval_path = '/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/unbiased/not_norm/train2/EUR_GBP_validate.parquet'\n",
    "sequence = 5\n",
    "saving_path = f'/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/unbiased/not_norm/train2/results/'\n",
    "os.makedirs(saving_path, exist_ok=True)\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    th.manual_seed(seed)\n",
    "    th.backends.cudnn.deterministic = True\n",
    "\n",
    "set_all_seeds(42)\n",
    "\n",
    "class ForexTensorboardCallback(BaseCallback):\n",
    "    \"\"\"Custom callback for logging Forex trading metrics to tensorboard.\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_returns = []  # Track episode returns for averaging\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"Called after each step in the environment.\"\"\"\n",
    "        # infos is a list of dictionaries, one from each parallel environment\n",
    "        for info in self.locals['infos']:\n",
    "            if info is None:  # Skip if no info (can happen at episode boundaries)\n",
    "                continue\n",
    "                \n",
    "            # Log account metrics\n",
    "            self.logger.record(\"metrics/balance\", info['balance'])\n",
    "            # self.logger.record(\"metrics/total_return_pct\", info['total_return_pct'])\n",
    "            # self.logger.record(\"metrics/net_profit\", info['net_profit'])\n",
    "            \n",
    "            # Log trade metrics\n",
    "            # self.logger.record(\"metrics/total_pnl\", info['total_pnl'])\n",
    "            # self.logger.record(\"metrics/total_trades\", info['total_trades'])\n",
    "            # self.logger.record(\"metrics/win_rate\", info['win_rate'])\n",
    "            \n",
    "            # Log cost metrics\n",
    "            self.logger.record(\"metrics/transaction_costs\", info['transaction_costs'])\n",
    "            # self.logger.record(\"metrics/transaction_costs_pct\", info['transaction_costs_pct'])\n",
    "            \n",
    "            # Log position metrics\n",
    "            self.logger.record(\"metrics/position_size_pct\", info['position_size_pct'])\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"Called at the end of a rollout.\"\"\"\n",
    "        # Episode metrics are handled automatically by stable-baselines3\n",
    "        pass\n",
    "\n",
    "class DetailedEvalCallback(EvalCallback):\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        Performs evaluation with detailed metric logging throughout the evaluation episodes.\n",
    "        \"\"\"\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            # Store episode rewards for calculating mean\n",
    "            episode_rewards = []\n",
    "            episode_lengths = []\n",
    "            \n",
    "            # For each evaluation episode\n",
    "            for _ in range(self.n_eval_episodes):\n",
    "                episode_reward = 0\n",
    "                episode_length = 0\n",
    "                done = False\n",
    "                # VecEnv reset returns just the obs\n",
    "                obs = self.eval_env.reset()\n",
    "                \n",
    "                # Run episode until done\n",
    "                while not done:\n",
    "                    # Get deterministic action\n",
    "                    action, _ = self.model.predict(obs, deterministic=True)\n",
    "                    # VecEnv step returns (obs, reward, done, info)\n",
    "                    obs, reward, done, info = self.eval_env.step(action)\n",
    "                    episode_reward += reward[0]  # reward is a numpy array\n",
    "                    episode_length += 1\n",
    "                    \n",
    "                    # Log metrics at each step\n",
    "                    if info[0] is not None:  # info is a list of dicts\n",
    "                        info = info[0]  # Get info dict from first env\n",
    "                        self.logger.record(\"eval/balance\", info.get('balance', 0))\n",
    "                        self.logger.record(\"eval/total_pnl\", info.get('total_pnl', 0))\n",
    "                        # self.logger.record(\"eval/total_trades\", info.get('total_trades', 0))\n",
    "                        # self.logger.record(\"eval/win_rate\", info.get('win_rate', 0))\n",
    "                        self.logger.record(\"eval/transaction_costs\", info.get('transaction_costs', 0))\n",
    "                        # Dump metrics at each step\n",
    "                        self.logger.dump(self.n_calls)\n",
    "                \n",
    "                episode_rewards.append(episode_reward)\n",
    "                episode_lengths.append(episode_length)\n",
    "\n",
    "            # Calculate mean metrics across episodes\n",
    "            mean_reward = np.mean(episode_rewards)\n",
    "            mean_length = np.mean(episode_lengths)\n",
    "            \n",
    "            self.logger.record(\"eval/mean_reward\", mean_reward)\n",
    "            self.logger.record(\"eval/mean_episode_length\", mean_length)\n",
    "\n",
    "            # Update best model if needed\n",
    "            if self.best_model_save_path is not None:\n",
    "                if self.verbose >= 1:\n",
    "                    print(f\"Evaluating the current model: {mean_reward:.2f}\")\n",
    "                \n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    if self.verbose >= 1:\n",
    "                        print(f\"New best mean reward: {mean_reward:.2f} \"\n",
    "                              f\"(previous: {self.best_mean_reward:.2f})\")\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    self.model.save(self.best_model_save_path)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _get_eval_info(self):\n",
    "        \"\"\"Helper method to get the last info dict from eval environment.\"\"\"\n",
    "        try:\n",
    "            # Try to get info directly from environment\n",
    "            if hasattr(self.eval_env, 'get_info'):\n",
    "                return self.eval_env.get_info()\n",
    "            # If that's not available, try to get it from the unwrapped env\n",
    "            elif hasattr(self.eval_env, 'envs'):\n",
    "                return self.eval_env.envs[0].get_info()\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not get eval info: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "def make_train_env(rank):\n",
    "    def _init():\n",
    "        env = ForexTradingEnv(\n",
    "            df_paths=source_dfs,\n",
    "            eval_mode=False,\n",
    "            sequence_length=sequence,\n",
    "        )\n",
    "        env = Monitor(env)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "\n",
    "train_env = SubprocVecEnv([make_train_env(i) for i in range(N_ENVS)])\n",
    "train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)\n",
    "\n",
    "\n",
    "\n",
    "def make_eval_env():\n",
    "    env = ForexTradingEnv(\n",
    "        df_paths=source_dfs,\n",
    "        eval_path=eval_path,\n",
    "        eval_mode=True,\n",
    "        pair='EUR_GBP',\n",
    "        sequence_length=sequence,\n",
    "\n",
    "\n",
    "    )\n",
    "    env = Monitor(env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=False)\n",
    "    env.training = False\n",
    "    return env\n",
    "\n",
    "\n",
    "eval_env = make_eval_env()\n",
    "\n",
    "eval_callback = DetailedEvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=f'{saving_path}eval_best_model_new_reward/',\n",
    "    log_path=saving_path,\n",
    "    eval_freq=EVAL_FREQ_ADJUSTED,\n",
    "    n_eval_episodes=5,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "# eval_callback = EvalCallback(\n",
    "#     eval_env,\n",
    "#     best_model_save_path=saving_path,\n",
    "#     log_path=saving_path,\n",
    "#     eval_freq=EVAL_FREQ_ADJUSTED,  # Adjust as needed\n",
    "#     n_eval_episodes=5,\n",
    "#     deterministic=True,\n",
    "#     render=False\n",
    "# )\n",
    "\n",
    "# model = PPO(\n",
    "#     'MlpPolicy',\n",
    "#     train_env,\n",
    "#     verbose=0,\n",
    "#     tensorboard_log=f'{saving_path}sequence_{sequence}__PPO_1h_no_costs_50k_balance_reduced_LSTM',\n",
    "# )\n",
    "# Define policy kwargs for the LSTM configuration\n",
    "# policy_kwargs = dict(\n",
    "#     # Network Architecture\n",
    "#     net_arch=dict(\n",
    "#         # Actor (policy) network\n",
    "#         pi=[256, 128],  # Larger first layer to process high-dimensional input\n",
    "#         # Critic (value) network\n",
    "#         vf=[256, 128]   # Match actor architecture for balanced learning\n",
    "#     ),\n",
    "    \n",
    "#     # LSTM Configuration\n",
    "#     lstm_hidden_size=256,      # Larger hidden size to capture complex patterns\n",
    "#     n_lstm_layers=2,           # Multiple layers for hierarchical feature learning\n",
    "#     enable_critic_lstm=True,   # Share temporal understanding between actor and critic\n",
    "    \n",
    "#     # LSTM specific parameters\n",
    "#     lstm_kwargs=dict(\n",
    "#         dropout=0.2            # Slightly higher dropout for regularization\n",
    "#     )\n",
    "# )\n",
    "\n",
    "policy_kwargs_complex = dict(\n",
    "    net_arch=dict(\n",
    "        pi=[512, 256, 128],\n",
    "        vf=[512, 256, 128]\n",
    "    ),\n",
    "    lstm_hidden_size=512,\n",
    "    n_lstm_layers=3,\n",
    "    enable_critic_lstm=True,\n",
    "    lstm_kwargs=dict(\n",
    "        dropout=0.25\n",
    "    )\n",
    ")\n",
    "\n",
    "policy_kwargs_memory_efficient = dict(\n",
    "    net_arch=dict(\n",
    "        pi=[256, 128],\n",
    "        vf=[256, 128]\n",
    "    ),\n",
    "    lstm_hidden_size=256,\n",
    "    n_lstm_layers=1,\n",
    "    lstm_kwargs=dict(\n",
    "        dropout=0.1\n",
    "    )\n",
    ")\n",
    "\n",
    "model = RecurrentPPO(\n",
    "    'MlpLstmPolicy',\n",
    "    train_env,\n",
    "    verbose=0,\n",
    "    seed=42,\n",
    "    tensorboard_log=f'{saving_path}sequence_{sequence}__PPO_1h_no_costs_50k_balance_reduced_LSTM2/',\n",
    "    policy_kwargs=policy_kwargs_memory_efficient,\n",
    ")\n",
    "callbacks = [\n",
    "    ForexTensorboardCallback(),\n",
    "    eval_callback\n",
    "]\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=10_000_000,  # Adjust as needed\n",
    "    callback=callbacks\n",
    ")\n",
    "\n",
    "model.save(f'{saving_path}{sequence}_best_model_core.zip')\n",
    "train_env.save(f'{saving_path}{sequence}_vec_normalize_core.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import torch as th\n",
    "import numpy as np\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "from trading.environments.forex_env2_flat_simple import ForexTradingEnv\n",
    "# from trading.environments.forex_env2_flat_simple import ForexTradingEnv2 as ForexTradingEnv\n",
    "# from trading.environments.forex_env_flat_multi_pair import MultipairForexTradingEnv\n",
    "\n",
    "from stable_baselines3.common.callbacks import EvalCallback, BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "\n",
    "th.set_num_threads(1)\n",
    "N_ENVS = 1  # Number of parallel environments\n",
    "EVAL_FREUQENCY = 200_000\n",
    "EVAL_FREQ_ADJUSTED = int(EVAL_FREUQENCY / N_ENVS)\n",
    "\n",
    "hourly_dir = \"/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/unbiased/not_norm/train2/\"\n",
    "source_path = '/Volumes/ssd_fat2/ai6_trading_bot/datasets/5min/df_with_all_indics_unbiased/not_norm/train2/'\n",
    "source_dfs = [os.path.join(hourly_dir, f) for f in os.listdir(hourly_dir) if f.endswith('.parquet') and not f.startswith('.') and 'validate' not in f]\n",
    "\n",
    "eval_path = '/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/unbiased/not_norm/train2/EUR_GBP_validate.parquet'\n",
    "sequence = 5\n",
    "saving_path = f'/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/unbiased/not_norm/train2/results'\n",
    "os.makedirs(saving_path, exist_ok=True)\n",
    "\n",
    "\n",
    "class ForexTensorboardCallback(BaseCallback):\n",
    "    \"\"\"Custom callback for logging Forex trading metrics to tensorboard.\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_returns = []  # Track episode returns for averaging\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"Called after each step in the environment.\"\"\"\n",
    "        # infos is a list of dictionaries, one from each parallel environment\n",
    "        for info in self.locals['infos']:\n",
    "            if info is None:  # Skip if no info (can happen at episode boundaries)\n",
    "                continue\n",
    "                \n",
    "            # Log account metrics\n",
    "            self.logger.record(\"metrics/balance\", info['balance'])\n",
    "            # self.logger.record(\"metrics/total_return_pct\", info['total_return_pct'])\n",
    "            # self.logger.record(\"metrics/net_profit\", info['net_profit'])\n",
    "            \n",
    "            # Log trade metrics\n",
    "            # self.logger.record(\"metrics/total_pnl\", info['total_pnl'])\n",
    "            self.logger.record(\"metrics/total_trades\", info['total_trades'])\n",
    "            # self.logger.record(\"metrics/win_rate\", info['win_rate'])\n",
    "            \n",
    "            # Log cost metrics\n",
    "            self.logger.record(\"metrics/transaction_costs\", info['transaction_costs'])\n",
    "            # self.logger.record(\"metrics/transaction_costs_pct\", info['transaction_costs_pct'])\n",
    "            \n",
    "            # Log position metrics\n",
    "            self.logger.record(\"metrics/position_size_pct\", info['position_size_pct'])\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"Called at the end of a rollout.\"\"\"\n",
    "        # Episode metrics are handled automatically by stable-baselines3\n",
    "        pass\n",
    "\n",
    "class DetailedEvalCallback(EvalCallback):\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        Performs evaluation with detailed metric logging throughout the evaluation episodes.\n",
    "        \"\"\"\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            # Store episode rewards for calculating mean\n",
    "            episode_rewards = []\n",
    "            episode_lengths = []\n",
    "            \n",
    "            # For each evaluation episode\n",
    "            for _ in range(self.n_eval_episodes):\n",
    "                episode_reward = 0\n",
    "                episode_length = 0\n",
    "                done = False\n",
    "                # VecEnv reset returns just the obs\n",
    "                obs = self.eval_env.reset()\n",
    "                \n",
    "                # Run episode until done\n",
    "                while not done:\n",
    "                    # Get deterministic action\n",
    "                    action, _ = self.model.predict(obs, deterministic=True)\n",
    "                    # VecEnv step returns (obs, reward, done, info)\n",
    "                    obs, reward, done, info = self.eval_env.step(action)\n",
    "                    episode_reward += reward[0]  # reward is a numpy array\n",
    "                    episode_length += 1\n",
    "                    \n",
    "                    # Log metrics at each step\n",
    "                    if info[0] is not None:  # info is a list of dicts\n",
    "                        info = info[0]  # Get info dict from first env\n",
    "                        self.logger.record(\"eval/balance\", info.get('balance', 0))\n",
    "                        self.logger.record(\"eval/total_pnl\", info.get('total_pnl', 0))\n",
    "                        self.logger.record(\"eval/total_trades\", info.get('total_trades', 0))\n",
    "                        # self.logger.record(\"eval/win_rate\", info.get('win_rate', 0))\n",
    "                        self.logger.record(\"eval/transaction_costs\", info.get('transaction_costs', 0))\n",
    "                        # Dump metrics at each step\n",
    "                        self.logger.dump(self.n_calls)\n",
    "                \n",
    "                episode_rewards.append(episode_reward)\n",
    "                episode_lengths.append(episode_length)\n",
    "\n",
    "            # Calculate mean metrics across episodes\n",
    "            mean_reward = np.mean(episode_rewards)\n",
    "            mean_length = np.mean(episode_lengths)\n",
    "            \n",
    "            self.logger.record(\"eval/mean_reward\", mean_reward)\n",
    "            self.logger.record(\"eval/mean_episode_length\", mean_length)\n",
    "\n",
    "            # Update best model if needed\n",
    "            if self.best_model_save_path is not None:\n",
    "                if self.verbose >= 1:\n",
    "                    print(f\"Evaluating the current model: {mean_reward:.2f}\")\n",
    "                \n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    if self.verbose >= 1:\n",
    "                        print(f\"New best mean reward: {mean_reward:.2f} \"\n",
    "                              f\"(previous: {self.best_mean_reward:.2f})\")\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    self.model.save(self.best_model_save_path)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _get_eval_info(self):\n",
    "        \"\"\"Helper method to get the last info dict from eval environment.\"\"\"\n",
    "        try:\n",
    "            # Try to get info directly from environment\n",
    "            if hasattr(self.eval_env, 'get_info'):\n",
    "                return self.eval_env.get_info()\n",
    "            # If that's not available, try to get it from the unwrapped env\n",
    "            elif hasattr(self.eval_env, 'envs'):\n",
    "                return self.eval_env.envs[0].get_info()\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not get eval info: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "def make_train_env(rank):\n",
    "    def _init():\n",
    "        env = ForexTradingEnv(\n",
    "            df_paths=source_dfs,\n",
    "            eval_mode=False,\n",
    "            sequence_length=sequence,\n",
    "        )\n",
    "        env = Monitor(env)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "\n",
    "train_env = SubprocVecEnv([make_train_env(i) for i in range(N_ENVS)])\n",
    "train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)\n",
    "\n",
    "\n",
    "\n",
    "def make_eval_env():\n",
    "    env = ForexTradingEnv(\n",
    "        df_paths=source_dfs,\n",
    "        eval_path=eval_path,\n",
    "        eval_mode=True,\n",
    "        pair='EUR_GBP',\n",
    "        sequence_length=sequence,\n",
    "\n",
    "\n",
    "    )\n",
    "    env = Monitor(env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=False)\n",
    "    env.training = False\n",
    "    return env\n",
    "\n",
    "\n",
    "eval_env = make_eval_env()\n",
    "\n",
    "eval_callback = DetailedEvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=f'{saving_path}eval_best_model_new_reward/',\n",
    "    log_path=saving_path,\n",
    "    eval_freq=EVAL_FREQ_ADJUSTED,\n",
    "    n_eval_episodes=5,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "# eval_callback = EvalCallback(\n",
    "#     eval_env,\n",
    "#     best_model_save_path=saving_path,\n",
    "#     log_path=saving_path,\n",
    "#     eval_freq=EVAL_FREQ_ADJUSTED,  # Adjust as needed\n",
    "#     n_eval_episodes=5,\n",
    "#     deterministic=True,\n",
    "#     render=False\n",
    "# )\n",
    "\n",
    "# model = PPO(\n",
    "#     'MlpPolicy',\n",
    "#     train_env,\n",
    "#     verbose=0,\n",
    "#     tensorboard_log=f'{saving_path}sequence_{sequence}__PPO_1h_no_costs_50k_balance_reduced_indics2',\n",
    "# )\n",
    "\n",
    "model = PPO(\n",
    "    'MlpPolicy',\n",
    "    train_env,\n",
    "    learning_rate=5e-5,  # Reduced from 3e-4\n",
    "    n_steps=4096,        # Increased from 2048\n",
    "    batch_size=256,      # Increased from 64\n",
    "    n_epochs=20,         # Increased from 10\n",
    "    ent_coef=0.01,      # Added to encourage exploration\n",
    "    # Clip range is important for stability\n",
    "    clip_range=0.1,      # Reduced from 0.2 default\n",
    "    # Add value function clipping\n",
    "    clip_range_vf=0.1,\n",
    "    verbose=0,\n",
    "    tensorboard_log=f'{saving_path}sequence_{sequence}__PPO_1h_no_costs_50k_balance_reduced_indics_claude_params/',\n",
    ")\n",
    "# Define policy kwargs for the LSTM configuration\n",
    "# policy_kwargs = dict(\n",
    "#     # Network Architecture\n",
    "#     net_arch=dict(\n",
    "#         # Actor (policy) network\n",
    "#         pi=[256, 128],  # Larger first layer to process high-dimensional input\n",
    "#         # Critic (value) network\n",
    "#         vf=[256, 128]   # Match actor architecture for balanced learning\n",
    "#     ),\n",
    "    \n",
    "#     # LSTM Configuration\n",
    "#     lstm_hidden_size=256,      # Larger hidden size to capture complex patterns\n",
    "#     n_lstm_layers=2,           # Multiple layers for hierarchical feature learning\n",
    "#     enable_critic_lstm=True,   # Share temporal understanding between actor and critic\n",
    "    \n",
    "#     # LSTM specific parameters\n",
    "#     lstm_kwargs=dict(\n",
    "#         dropout=0.2            # Slightly higher dropout for regularization\n",
    "#     )\n",
    "# )\n",
    "\n",
    "policy_kwargs_complex = dict(\n",
    "    net_arch=dict(\n",
    "        pi=[512, 256, 128],\n",
    "        vf=[512, 256, 128]\n",
    "    ),\n",
    "    lstm_hidden_size=512,\n",
    "    n_lstm_layers=3,\n",
    "    enable_critic_lstm=True,\n",
    "    lstm_kwargs=dict(\n",
    "        dropout=0.25\n",
    "    )\n",
    ")\n",
    "\n",
    "policy_kwargs_memory_efficient = dict(\n",
    "    net_arch=dict(\n",
    "        pi=[256, 128],\n",
    "        vf=[256, 128]\n",
    "    ),\n",
    "    lstm_hidden_size=256,\n",
    "    n_lstm_layers=1,\n",
    "    lstm_kwargs=dict(\n",
    "        dropout=0.1\n",
    "    )\n",
    ")\n",
    "\n",
    "# model = RecurrentPPO(\n",
    "#     'MlpLstmPolicy',\n",
    "#     train_env,\n",
    "#     verbose=0,\n",
    "#     tensorboard_log=f'{saving_path}sequence_{sequence}_RecurrentPPO_memory efficient/',\n",
    "#     policy_kwargs=policy_kwargs_memory_efficient,\n",
    "# )\n",
    "callbacks = [\n",
    "    ForexTensorboardCallback(),\n",
    "    eval_callback\n",
    "]\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=10_000_000,  # Adjust as needed\n",
    "    callback=callbacks\n",
    ")\n",
    "\n",
    "model.save(f'{saving_path}{sequence}_best_model_core.zip')\n",
    "train_env.save(f'{saving_path}{sequence}_vec_normalize_core.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import torch as th\n",
    "import numpy as np\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "from trading.environments.forex_env2_flat_simple import ForexTradingEnv\n",
    "# from trading.environments.forex_env2_flat_simple import ForexTradingEnv2 as ForexTradingEnv\n",
    "# from trading.environments.forex_env_flat_multi_pair import MultipairForexTradingEnv\n",
    "\n",
    "from stable_baselines3.common.callbacks import EvalCallback, BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "\n",
    "th.set_num_threads(3)\n",
    "N_ENVS = 3  # Number of parallel environments\n",
    "EVAL_FREUQENCY = 500_000\n",
    "EVAL_FREQ_ADJUSTED = int(EVAL_FREUQENCY / N_ENVS)\n",
    "\n",
    "hourly_dir = \"/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/unbiased/not_norm/train2/\"\n",
    "source_path = '/Volumes/ssd_fat2/ai6_trading_bot/datasets/5min/df_with_all_indics_unbiased/not_norm/train2/'\n",
    "source_dfs = [os.path.join(hourly_dir, f) for f in os.listdir(hourly_dir) if f.endswith('.parquet') and not f.startswith('.') and 'validate' not in f]\n",
    "\n",
    "eval_path = '/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/unbiased/not_norm/train2/EUR_GBP_validate.parquet'\n",
    "sequence = 5\n",
    "saving_path = f'/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/unbiased/not_norm/train2/results/'\n",
    "os.makedirs(saving_path, exist_ok=True)\n",
    "\n",
    "\n",
    "class ForexTensorboardCallback(BaseCallback):\n",
    "    \"\"\"Custom callback for logging Forex trading metrics to tensorboard.\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_returns = []  # Track episode returns for averaging\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"Called after each step in the environment.\"\"\"\n",
    "        # infos is a list of dictionaries, one from each parallel environment\n",
    "        for info in self.locals['infos']:\n",
    "            if info is None:  # Skip if no info (can happen at episode boundaries)\n",
    "                continue\n",
    "                \n",
    "            # Log account metrics\n",
    "            self.logger.record(\"metrics/balance\", info['balance'])\n",
    "            # self.logger.record(\"metrics/total_return_pct\", info['total_return_pct'])\n",
    "            # self.logger.record(\"metrics/net_profit\", info['net_profit'])\n",
    "            \n",
    "            # Log trade metrics\n",
    "            # self.logger.record(\"metrics/total_pnl\", info['total_pnl'])\n",
    "            self.logger.record(\"metrics/total_trades\", info['total_trades'])\n",
    "            # self.logger.record(\"metrics/win_rate\", info['win_rate'])\n",
    "            \n",
    "            # Log cost metrics\n",
    "            self.logger.record(\"metrics/transaction_costs\", info['transaction_costs'])\n",
    "            # self.logger.record(\"metrics/transaction_costs_pct\", info['transaction_costs_pct'])\n",
    "            \n",
    "            # Log position metrics\n",
    "            self.logger.record(\"metrics/position_size_pct\", info['position_size_pct'])\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"Called at the end of a rollout.\"\"\"\n",
    "        # Episode metrics are handled automatically by stable-baselines3\n",
    "        pass\n",
    "\n",
    "class DetailedEvalCallback(EvalCallback):\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        Performs evaluation with detailed metric logging throughout the evaluation episodes.\n",
    "        \"\"\"\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            # Store episode rewards for calculating mean\n",
    "            episode_rewards = []\n",
    "            episode_lengths = []\n",
    "            \n",
    "            # For each evaluation episode\n",
    "            for _ in range(self.n_eval_episodes):\n",
    "                episode_reward = 0\n",
    "                episode_length = 0\n",
    "                done = False\n",
    "                # VecEnv reset returns just the obs\n",
    "                obs = self.eval_env.reset()\n",
    "                \n",
    "                # Run episode until done\n",
    "                while not done:\n",
    "                    # Get deterministic action\n",
    "                    action, _ = self.model.predict(obs, deterministic=True)\n",
    "                    # VecEnv step returns (obs, reward, done, info)\n",
    "                    obs, reward, done, info = self.eval_env.step(action)\n",
    "                    episode_reward += reward[0]  # reward is a numpy array\n",
    "                    episode_length += 1\n",
    "                    \n",
    "                    # Log metrics at each step\n",
    "                    if info[0] is not None:  # info is a list of dicts\n",
    "                        info = info[0]  # Get info dict from first env\n",
    "                        self.logger.record(\"eval/balance\", info.get('balance', 0))\n",
    "                        self.logger.record(\"eval/total_pnl\", info.get('total_pnl', 0))\n",
    "                        self.logger.record(\"eval/total_trades\", info.get('total_trades', 0))\n",
    "                        # self.logger.record(\"eval/win_rate\", info.get('win_rate', 0))\n",
    "                        self.logger.record(\"eval/transaction_costs\", info.get('transaction_costs', 0))\n",
    "                        # Dump metrics at each step\n",
    "                        self.logger.dump(self.n_calls)\n",
    "                \n",
    "                episode_rewards.append(episode_reward)\n",
    "                episode_lengths.append(episode_length)\n",
    "\n",
    "            # Calculate mean metrics across episodes\n",
    "            mean_reward = np.mean(episode_rewards)\n",
    "            mean_length = np.mean(episode_lengths)\n",
    "            \n",
    "            self.logger.record(\"eval/mean_reward\", mean_reward)\n",
    "            self.logger.record(\"eval/mean_episode_length\", mean_length)\n",
    "\n",
    "            # Update best model if needed\n",
    "            if self.best_model_save_path is not None:\n",
    "                if self.verbose >= 1:\n",
    "                    print(f\"Evaluating the current model: {mean_reward:.2f}\")\n",
    "                \n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    if self.verbose >= 1:\n",
    "                        print(f\"New best mean reward: {mean_reward:.2f} \"\n",
    "                              f\"(previous: {self.best_mean_reward:.2f})\")\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    self.model.save(self.best_model_save_path)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _get_eval_info(self):\n",
    "        \"\"\"Helper method to get the last info dict from eval environment.\"\"\"\n",
    "        try:\n",
    "            # Try to get info directly from environment\n",
    "            if hasattr(self.eval_env, 'get_info'):\n",
    "                return self.eval_env.get_info()\n",
    "            # If that's not available, try to get it from the unwrapped env\n",
    "            elif hasattr(self.eval_env, 'envs'):\n",
    "                return self.eval_env.envs[0].get_info()\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not get eval info: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "def make_train_env(rank):\n",
    "    def _init():\n",
    "        env = ForexTradingEnv(\n",
    "            df_paths=source_dfs,\n",
    "            eval_mode=False,\n",
    "            sequence_length=sequence,\n",
    "        )\n",
    "        env = Monitor(env)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "\n",
    "train_env = SubprocVecEnv([make_train_env(i) for i in range(N_ENVS)])\n",
    "train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)\n",
    "\n",
    "\n",
    "\n",
    "def make_eval_env():\n",
    "    env = ForexTradingEnv(\n",
    "        df_paths=source_dfs,\n",
    "        eval_path=eval_path,\n",
    "        eval_mode=True,\n",
    "        pair='EUR_GBP',\n",
    "        sequence_length=sequence,\n",
    "\n",
    "\n",
    "    )\n",
    "    env = Monitor(env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=False)\n",
    "    env.training = False\n",
    "    return env\n",
    "\n",
    "\n",
    "eval_env = make_eval_env()\n",
    "\n",
    "eval_callback = DetailedEvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=f'{saving_path}eval_best_model_new_reward/',\n",
    "    log_path=saving_path,\n",
    "    eval_freq=EVAL_FREQ_ADJUSTED,\n",
    "    n_eval_episodes=5,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "# eval_callback = EvalCallback(\n",
    "#     eval_env,\n",
    "#     best_model_save_path=saving_path,\n",
    "#     log_path=saving_path,\n",
    "#     eval_freq=EVAL_FREQ_ADJUSTED,  # Adjust as needed\n",
    "#     n_eval_episodes=5,\n",
    "#     deterministic=True,\n",
    "#     render=False\n",
    "# )\n",
    "\n",
    "# model = PPO(\n",
    "#     'MlpPolicy',\n",
    "#     train_env,\n",
    "#     verbose=0,\n",
    "#     tensorboard_log=f'{saving_path}sequence_{sequence}__PPO_1h_no_costs_50k_balance_reduced_indics2',\n",
    "# )\n",
    "\n",
    "model = PPO(\n",
    "    'MlpPolicy',\n",
    "    train_env,\n",
    "    learning_rate=5e-5,  # Reduced from 3e-4\n",
    "    n_steps=4096,        # Increased from 2048\n",
    "    batch_size=256,      # Increased from 64\n",
    "    n_epochs=20,         # Increased from 10\n",
    "    ent_coef=0.01,      # Added to encourage exploration\n",
    "    # Clip range is important for stability\n",
    "    clip_range=0.1,      # Reduced from 0.2 default\n",
    "    # Add value function clipping\n",
    "    clip_range_vf=0.1,\n",
    "    verbose=0,\n",
    "    tensorboard_log=f'{saving_path}sequence_{sequence}__PPO_1h_no_costs_50k_balance_reduced_indics_claude_params/',\n",
    ")\n",
    "# Define policy kwargs for the LSTM configuration\n",
    "# policy_kwargs = dict(\n",
    "#     # Network Architecture\n",
    "#     net_arch=dict(\n",
    "#         # Actor (policy) network\n",
    "#         pi=[256, 128],  # Larger first layer to process high-dimensional input\n",
    "#         # Critic (value) network\n",
    "#         vf=[256, 128]   # Match actor architecture for balanced learning\n",
    "#     ),\n",
    "    \n",
    "#     # LSTM Configuration\n",
    "#     lstm_hidden_size=256,      # Larger hidden size to capture complex patterns\n",
    "#     n_lstm_layers=2,           # Multiple layers for hierarchical feature learning\n",
    "#     enable_critic_lstm=True,   # Share temporal understanding between actor and critic\n",
    "    \n",
    "#     # LSTM specific parameters\n",
    "#     lstm_kwargs=dict(\n",
    "#         dropout=0.2            # Slightly higher dropout for regularization\n",
    "#     )\n",
    "# )\n",
    "\n",
    "policy_kwargs_complex = dict(\n",
    "    net_arch=dict(\n",
    "        pi=[512, 256, 128],\n",
    "        vf=[512, 256, 128]\n",
    "    ),\n",
    "    lstm_hidden_size=512,\n",
    "    n_lstm_layers=3,\n",
    "    enable_critic_lstm=True,\n",
    "    lstm_kwargs=dict(\n",
    "        dropout=0.25\n",
    "    )\n",
    ")\n",
    "\n",
    "policy_kwargs_memory_efficient = dict(\n",
    "    net_arch=dict(\n",
    "        pi=[256, 128],\n",
    "        vf=[256, 128]\n",
    "    ),\n",
    "    lstm_hidden_size=256,\n",
    "    n_lstm_layers=1,\n",
    "    lstm_kwargs=dict(\n",
    "        dropout=0.1\n",
    "    )\n",
    ")\n",
    "\n",
    "# model = RecurrentPPO(\n",
    "#     'MlpLstmPolicy',\n",
    "#     train_env,\n",
    "#     verbose=0,\n",
    "#     tensorboard_log=f'{saving_path}sequence_{sequence}_RecurrentPPO_memory efficient/',\n",
    "#     policy_kwargs=policy_kwargs_memory_efficient,\n",
    "# )\n",
    "callbacks = [\n",
    "    ForexTensorboardCallback(),\n",
    "    eval_callback\n",
    "]\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=10_000_000,  # Adjust as needed\n",
    "    callback=callbacks\n",
    ")\n",
    "\n",
    "model.save(f'{saving_path}{sequence}_best_model_core.zip')\n",
    "train_env.save(f'{saving_path}{sequence}_vec_normalize_core.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import torch as th\n",
    "import numpy as np\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "from trading.environments.forex_env2_flat_simple import ForexTradingEnv\n",
    "# from trading.environments.forex_env2_flat_simple import ForexTradingEnv2 as ForexTradingEnv\n",
    "# from trading.environments.forex_env_flat_multi_pair import MultipairForexTradingEnv\n",
    "\n",
    "from stable_baselines3.common.callbacks import EvalCallback, BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "\n",
    "th.set_num_threads(3)\n",
    "N_ENVS = 3  # Number of parallel environments\n",
    "EVAL_FREUQENCY = 500_000\n",
    "EVAL_FREQ_ADJUSTED = int(EVAL_FREUQENCY / N_ENVS)\n",
    "\n",
    "hourly_dir = \"/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/unbiased/not_norm/train3_minimal_indic/\"\n",
    "source_path = '/Volumes/ssd_fat2/ai6_trading_bot/datasets/5min/df_with_all_indics_unbiased/not_norm/train2/'\n",
    "source_dfs = [os.path.join(hourly_dir, f) for f in os.listdir(hourly_dir) if f.endswith('.parquet') and not f.startswith('.') and 'validate' not in f]\n",
    "\n",
    "eval_path = '/Volumes/ssd_fat2/ai6_trading_bot/datasets/5min/df_with_all_indics_unbiased/not_norm/train2/EUR_GBP_validate.parquet'\n",
    "sequence = 5\n",
    "saving_path = f'/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/unbiased/not_norm/train2/results/'\n",
    "os.makedirs(saving_path, exist_ok=True)\n",
    "\n",
    "\n",
    "class ForexTensorboardCallback(BaseCallback):\n",
    "    \"\"\"Custom callback for logging Forex trading metrics to tensorboard.\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_returns = []  # Track episode returns for averaging\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"Called after each step in the environment.\"\"\"\n",
    "        # infos is a list of dictionaries, one from each parallel environment\n",
    "        for info in self.locals['infos']:\n",
    "            if info is None:  # Skip if no info (can happen at episode boundaries)\n",
    "                continue\n",
    "                \n",
    "            # Log account metrics\n",
    "            self.logger.record(\"metrics/balance\", info['balance'])\n",
    "            # self.logger.record(\"metrics/total_return_pct\", info['total_return_pct'])\n",
    "            # self.logger.record(\"metrics/net_profit\", info['net_profit'])\n",
    "            \n",
    "            # Log trade metrics\n",
    "            # self.logger.record(\"metrics/total_pnl\", info['total_pnl'])\n",
    "            # self.logger.record(\"metrics/total_trades\", info['total_trades'])\n",
    "            # self.logger.record(\"metrics/win_rate\", info['win_rate'])\n",
    "            \n",
    "            # Log cost metrics\n",
    "            self.logger.record(\"metrics/transaction_costs\", info['transaction_costs'])\n",
    "            # self.logger.record(\"metrics/transaction_costs_pct\", info['transaction_costs_pct'])\n",
    "            \n",
    "            # Log position metrics\n",
    "            self.logger.record(\"metrics/position_size_pct\", info['position_size_pct'])\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"Called at the end of a rollout.\"\"\"\n",
    "        # Episode metrics are handled automatically by stable-baselines3\n",
    "        pass\n",
    "\n",
    "class DetailedEvalCallback(EvalCallback):\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        Performs evaluation with detailed metric logging throughout the evaluation episodes.\n",
    "        \"\"\"\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            # Store episode rewards for calculating mean\n",
    "            episode_rewards = []\n",
    "            episode_lengths = []\n",
    "            \n",
    "            # For each evaluation episode\n",
    "            for _ in range(self.n_eval_episodes):\n",
    "                episode_reward = 0\n",
    "                episode_length = 0\n",
    "                done = False\n",
    "                # VecEnv reset returns just the obs\n",
    "                obs = self.eval_env.reset()\n",
    "                \n",
    "                # Run episode until done\n",
    "                while not done:\n",
    "                    # Get deterministic action\n",
    "                    action, _ = self.model.predict(obs, deterministic=True)\n",
    "                    # VecEnv step returns (obs, reward, done, info)\n",
    "                    obs, reward, done, info = self.eval_env.step(action)\n",
    "                    episode_reward += reward[0]  # reward is a numpy array\n",
    "                    episode_length += 1\n",
    "                    \n",
    "                    # Log metrics at each step\n",
    "                    if info[0] is not None:  # info is a list of dicts\n",
    "                        info = info[0]  # Get info dict from first env\n",
    "                        self.logger.record(\"eval/balance\", info.get('balance', 0))\n",
    "                        self.logger.record(\"eval/total_pnl\", info.get('total_pnl', 0))\n",
    "                        # self.logger.record(\"eval/total_trades\", info.get('total_trades', 0))\n",
    "                        # self.logger.record(\"eval/win_rate\", info.get('win_rate', 0))\n",
    "                        self.logger.record(\"eval/transaction_costs\", info.get('transaction_costs', 0))\n",
    "                        # Dump metrics at each step\n",
    "                        self.logger.dump(self.n_calls)\n",
    "                \n",
    "                episode_rewards.append(episode_reward)\n",
    "                episode_lengths.append(episode_length)\n",
    "\n",
    "            # Calculate mean metrics across episodes\n",
    "            mean_reward = np.mean(episode_rewards)\n",
    "            mean_length = np.mean(episode_lengths)\n",
    "            \n",
    "            self.logger.record(\"eval/mean_reward\", mean_reward)\n",
    "            self.logger.record(\"eval/mean_episode_length\", mean_length)\n",
    "\n",
    "            # Update best model if needed\n",
    "            if self.best_model_save_path is not None:\n",
    "                if self.verbose >= 1:\n",
    "                    print(f\"Evaluating the current model: {mean_reward:.2f}\")\n",
    "                \n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    if self.verbose >= 1:\n",
    "                        print(f\"New best mean reward: {mean_reward:.2f} \"\n",
    "                              f\"(previous: {self.best_mean_reward:.2f})\")\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    self.model.save(self.best_model_save_path)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _get_eval_info(self):\n",
    "        \"\"\"Helper method to get the last info dict from eval environment.\"\"\"\n",
    "        try:\n",
    "            # Try to get info directly from environment\n",
    "            if hasattr(self.eval_env, 'get_info'):\n",
    "                return self.eval_env.get_info()\n",
    "            # If that's not available, try to get it from the unwrapped env\n",
    "            elif hasattr(self.eval_env, 'envs'):\n",
    "                return self.eval_env.envs[0].get_info()\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not get eval info: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "def make_train_env(rank):\n",
    "    def _init():\n",
    "        env = ForexTradingEnv(\n",
    "            df_paths=source_dfs,\n",
    "            eval_mode=False,\n",
    "            sequence_length=sequence,\n",
    "        )\n",
    "        env = Monitor(env)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "\n",
    "train_env = SubprocVecEnv([make_train_env(i) for i in range(N_ENVS)])\n",
    "train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)\n",
    "\n",
    "\n",
    "\n",
    "def make_eval_env():\n",
    "    env = ForexTradingEnv(\n",
    "        df_paths=source_dfs,\n",
    "        eval_path=eval_path,\n",
    "        eval_mode=True,\n",
    "        pair='EUR_GBP',\n",
    "        sequence_length=sequence,\n",
    "\n",
    "\n",
    "    )\n",
    "    env = Monitor(env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=False)\n",
    "    env.training = False\n",
    "    return env\n",
    "\n",
    "\n",
    "eval_env = make_eval_env()\n",
    "\n",
    "eval_callback = DetailedEvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=f'{saving_path}eval_best_model_new_reward/',\n",
    "    log_path=saving_path,\n",
    "    eval_freq=EVAL_FREQ_ADJUSTED,\n",
    "    n_eval_episodes=5,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "# eval_callback = EvalCallback(\n",
    "#     eval_env,\n",
    "#     best_model_save_path=saving_path,\n",
    "#     log_path=saving_path,\n",
    "#     eval_freq=EVAL_FREQ_ADJUSTED,  # Adjust as needed\n",
    "#     n_eval_episodes=5,\n",
    "#     deterministic=True,\n",
    "#     render=False\n",
    "# )\n",
    "\n",
    "model = PPO(\n",
    "    'MlpPolicy',\n",
    "    train_env,\n",
    "    verbose=0,\n",
    "    tensorboard_log=f'{saving_path}sequence_{sequence}__PPO_1h_no_costs_50k_balance_minimal_indic',\n",
    ")\n",
    "# Define policy kwargs for the LSTM configuration\n",
    "# policy_kwargs = dict(\n",
    "#     # Network Architecture\n",
    "#     net_arch=dict(\n",
    "#         # Actor (policy) network\n",
    "#         pi=[256, 128],  # Larger first layer to process high-dimensional input\n",
    "#         # Critic (value) network\n",
    "#         vf=[256, 128]   # Match actor architecture for balanced learning\n",
    "#     ),\n",
    "    \n",
    "#     # LSTM Configuration\n",
    "#     lstm_hidden_size=256,      # Larger hidden size to capture complex patterns\n",
    "#     n_lstm_layers=2,           # Multiple layers for hierarchical feature learning\n",
    "#     enable_critic_lstm=True,   # Share temporal understanding between actor and critic\n",
    "    \n",
    "#     # LSTM specific parameters\n",
    "#     lstm_kwargs=dict(\n",
    "#         dropout=0.2            # Slightly higher dropout for regularization\n",
    "#     )\n",
    "# )\n",
    "\n",
    "policy_kwargs_complex = dict(\n",
    "    net_arch=dict(\n",
    "        pi=[512, 256, 128],\n",
    "        vf=[512, 256, 128]\n",
    "    ),\n",
    "    lstm_hidden_size=512,\n",
    "    n_lstm_layers=3,\n",
    "    enable_critic_lstm=True,\n",
    "    lstm_kwargs=dict(\n",
    "        dropout=0.25\n",
    "    )\n",
    ")\n",
    "\n",
    "policy_kwargs_memory_efficient = dict(\n",
    "    net_arch=dict(\n",
    "        pi=[256, 128],\n",
    "        vf=[256, 128]\n",
    "    ),\n",
    "    lstm_hidden_size=256,\n",
    "    n_lstm_layers=1,\n",
    "    lstm_kwargs=dict(\n",
    "        dropout=0.1\n",
    "    )\n",
    ")\n",
    "\n",
    "# model = RecurrentPPO(\n",
    "#     'MlpLstmPolicy',\n",
    "#     train_env,\n",
    "#     verbose=0,\n",
    "#     tensorboard_log=f'{saving_path}sequence_{sequence}_RecurrentPPO_memory efficient/',\n",
    "#     policy_kwargs=policy_kwargs_memory_efficient,\n",
    "# )\n",
    "callbacks = [\n",
    "    ForexTensorboardCallback(),\n",
    "    eval_callback\n",
    "]\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=10_000_000,  # Adjust as needed\n",
    "    callback=callbacks\n",
    ")\n",
    "\n",
    "model.save(f'{saving_path}{sequence}_best_model_core.zip')\n",
    "train_env.save(f'{saving_path}{sequence}_vec_normalize_core.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SB5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
