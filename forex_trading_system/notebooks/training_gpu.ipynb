{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121  # For CUDA 12.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/sobiodum/ai6-gcp-bot.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import random\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "from trading.environments.forex_env2_flat_simple import ForexTradingEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Custom learning rate schedule implementation\n",
    "def custom_lr_schedule(progress_remaining: float, initial_value: float = 3e-4, final_value: float = 1e-4) -> float:\n",
    "    \"\"\"Linear learning rate schedule.\"\"\"\n",
    "    return final_value + (initial_value - final_value) * progress_remaining\n",
    "\n",
    "# Optimize for GPU usage\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    th.backends.cudnn.benchmark = True  # Enable cuDNN auto-tuner\n",
    "    th.set_float32_matmul_precision('high')  # Use TF32 on Ampere GPUs\n",
    "\n",
    "# Optimized number of environments for your CPU (Ryzen 9 5950X)\n",
    "N_ENVS = 14  # Slightly less than physical cores to avoid overhead\n",
    "EVAL_FREQUENCY = 500_000\n",
    "EVAL_FREQ_ADJUSTED = int(EVAL_FREQUENCY / N_ENVS)\n",
    "\n",
    "# Your existing paths setup\n",
    "hourly_dir = \"./train2/\"\n",
    "source_dfs = [os.path.join(hourly_dir, f) for f in os.listdir(hourly_dir) \n",
    "              if f.endswith('.parquet') and not f.startswith('.') and 'validate' not in f]\n",
    "eval_path = './train/EUR_GBP_validate.parquet'\n",
    "sequence = 5\n",
    "saving_path = f'/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h/unbiased/not_norm/train2/results/'\n",
    "os.makedirs(saving_path, exist_ok=True)\n",
    "\n",
    "# Enhanced seed setting for better reproducibility\n",
    "def set_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    th.manual_seed(seed)\n",
    "    if th.cuda.is_available():\n",
    "        th.cuda.manual_seed(seed)\n",
    "        th.cuda.manual_seed_all(seed)\n",
    "    th.backends.cudnn.deterministic = True\n",
    "    th.backends.cudnn.benchmark = False\n",
    "\n",
    "set_all_seeds(42)\n",
    "\n",
    "class ForexTensorboardCallback(BaseCallback):\n",
    "    \"\"\"Custom callback for logging Forex trading metrics to tensorboard.\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_returns = []  # Track episode returns for averaging\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"Called after each step in the environment.\"\"\"\n",
    "        # infos is a list of dictionaries, one from each parallel environment\n",
    "        for info in self.locals['infos']:\n",
    "            if info is None:  # Skip if no info (can happen at episode boundaries)\n",
    "                continue\n",
    "                \n",
    "            # Log account metrics\n",
    "            self.logger.record(\"metrics/balance\", info['balance'])\n",
    "            # self.logger.record(\"metrics/total_return_pct\", info['total_return_pct'])\n",
    "            # self.logger.record(\"metrics/net_profit\", info['net_profit'])\n",
    "            \n",
    "            # Log trade metrics\n",
    "            self.logger.record(\"metrics/total_pnl\", info['total_pnl'])\n",
    "            # self.logger.record(\"metrics/total_trades\", info['total_trades'])\n",
    "            # self.logger.record(\"metrics/win_rate\", info['win_rate'])\n",
    "            \n",
    "            # Log cost metrics\n",
    "            self.logger.record(\"metrics/transaction_costs\", info['transaction_costs'])\n",
    "            # self.logger.record(\"metrics/transaction_costs_pct\", info['transaction_costs_pct'])\n",
    "            \n",
    "            # Log position metrics\n",
    "            self.logger.record(\"metrics/position_size_pct\", info['position_size_pct'])\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"Called at the end of a rollout.\"\"\"\n",
    "        # Episode metrics are handled automatically by stable-baselines3\n",
    "        pass\n",
    "\n",
    "class DetailedEvalCallback(EvalCallback):\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        Performs evaluation with detailed metric logging throughout the evaluation episodes.\n",
    "        \"\"\"\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            # Store episode rewards for calculating mean\n",
    "            episode_rewards = []\n",
    "            episode_lengths = []\n",
    "            \n",
    "            # For each evaluation episode\n",
    "            for _ in range(self.n_eval_episodes):\n",
    "                episode_reward = 0\n",
    "                episode_length = 0\n",
    "                done = False\n",
    "                # VecEnv reset returns just the obs\n",
    "                obs = self.eval_env.reset()\n",
    "                \n",
    "                # Run episode until done\n",
    "                while not done:\n",
    "                    # Get deterministic action\n",
    "                    action, _ = self.model.predict(obs, deterministic=True)\n",
    "                    # VecEnv step returns (obs, reward, done, info)\n",
    "                    obs, reward, done, info = self.eval_env.step(action)\n",
    "                    episode_reward += reward[0]  # reward is a numpy array\n",
    "                    episode_length += 1\n",
    "                    \n",
    "                    # Log metrics at each step\n",
    "                    if info[0] is not None:  # info is a list of dicts\n",
    "                        info = info[0]  # Get info dict from first env\n",
    "                        self.logger.record(\"eval/balance\", info.get('balance', 0))\n",
    "                        self.logger.record(\"eval/total_pnl\", info.get('total_pnl', 0))\n",
    "                        # self.logger.record(\"eval/total_trades\", info.get('total_trades', 0))\n",
    "                        # self.logger.record(\"eval/win_rate\", info.get('win_rate', 0))\n",
    "                        self.logger.record(\"eval/transaction_costs\", info.get('transaction_costs', 0))\n",
    "                        # Dump metrics at each step\n",
    "                        self.logger.dump(self.n_calls)\n",
    "                \n",
    "                episode_rewards.append(episode_reward)\n",
    "                episode_lengths.append(episode_length)\n",
    "\n",
    "            # Calculate mean metrics across episodes\n",
    "            mean_reward = np.mean(episode_rewards)\n",
    "            mean_length = np.mean(episode_lengths)\n",
    "            \n",
    "            self.logger.record(\"eval/mean_reward\", mean_reward)\n",
    "            self.logger.record(\"eval/mean_episode_length\", mean_length)\n",
    "\n",
    "            # Update best model if needed\n",
    "            if self.best_model_save_path is not None:\n",
    "                if self.verbose >= 1:\n",
    "                    print(f\"Evaluating the current model: {mean_reward:.2f}\")\n",
    "                \n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    if self.verbose >= 1:\n",
    "                        print(f\"New best mean reward: {mean_reward:.2f} \"\n",
    "                              f\"(previous: {self.best_mean_reward:.2f})\")\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    self.model.save(self.best_model_save_path)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _get_eval_info(self):\n",
    "        \"\"\"Helper method to get the last info dict from eval environment.\"\"\"\n",
    "        try:\n",
    "            # Try to get info directly from environment\n",
    "            if hasattr(self.eval_env, 'get_info'):\n",
    "                return self.eval_env.get_info()\n",
    "            # If that's not available, try to get it from the unwrapped env\n",
    "            elif hasattr(self.eval_env, 'envs'):\n",
    "                return self.eval_env.envs[0].get_info()\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not get eval info: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "def make_train_env(rank):\n",
    "    def _init():\n",
    "        env = ForexTradingEnv(\n",
    "            df_paths=source_dfs,\n",
    "            eval_mode=False,\n",
    "            sequence_length=sequence,\n",
    "        )\n",
    "        env = Monitor(env)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "\n",
    "train_env = SubprocVecEnv([make_train_env(i) for i in range(N_ENVS)])\n",
    "train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)\n",
    "\n",
    "\n",
    "\n",
    "def make_eval_env():\n",
    "    env = ForexTradingEnv(\n",
    "        df_paths=source_dfs,\n",
    "        eval_path=eval_path,\n",
    "        eval_mode=True,\n",
    "        pair='EUR_GBP',\n",
    "        sequence_length=sequence,\n",
    "\n",
    "\n",
    "    )\n",
    "    env = Monitor(env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=False)\n",
    "    env.training = False\n",
    "    return env\n",
    "\n",
    "\n",
    "eval_env = make_eval_env()\n",
    "\n",
    "eval_callback = DetailedEvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=f'{saving_path}eval_best_model_new_reward/',\n",
    "    log_path=saving_path,\n",
    "    eval_freq=EVAL_FREQ_ADJUSTED,\n",
    "    n_eval_episodes=5,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "\n",
    "# Optimized policy configuration for RTX 4070\n",
    "policy_kwargs_gpu = dict(\n",
    "    net_arch=dict(\n",
    "        pi=[512, 512, 256],\n",
    "        vf=[512, 512, 256]\n",
    "    ),\n",
    "    lstm_hidden_size=512,\n",
    "    n_lstm_layers=2,\n",
    "    lstm_kwargs=dict(\n",
    "        dropout=0.15,\n",
    "        batch_first=True,  # Important for GPU optimization\n",
    "    ),\n",
    "    optimizer_kwargs=dict(\n",
    "        eps=1e-5,\n",
    "        weight_decay=1e-5,  # L2 regularization\n",
    "        betas=(0.9, 0.999)  # Adam optimizer parameters\n",
    "    ),\n",
    "    share_features_extractor=False,\n",
    "    enable_critic_norm=True\n",
    ")\n",
    "\n",
    "# Training configuration optimized for your hardware\n",
    "training_kwargs = dict(\n",
    "    learning_rate=custom_lr_schedule,  # Custom schedule instead of default constant\n",
    "    batch_size=2048,                   # Increased for GPU efficiency\n",
    "    ent_coef=0.01,                     # Slightly increased exploration\n",
    "    device=device,                     # Explicit GPU usage\n",
    "    target_kl=0.015                    # Trading-specific stability\n",
    ")\n",
    "\n",
    "\n",
    "# Create model with optimized configuration\n",
    "model = RecurrentPPO(\n",
    "    'MlpLstmPolicy',\n",
    "    train_env,\n",
    "    verbose=1,  # Increased verbosity for better monitoring\n",
    "    seed=42,\n",
    "    tensorboard_log=f'{saving_path}sequence_{sequence}__PPO_1h_gpu_optimized/',\n",
    "    policy_kwargs=policy_kwargs_gpu,\n",
    "    **training_kwargs\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    ForexTensorboardCallback(),\n",
    "    eval_callback\n",
    "]\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=30_000_000,  # Adjust as needed\n",
    "    callback=callbacks\n",
    ")\n",
    "\n",
    "# Save the final model and normalizer\n",
    "model.save(f'{saving_path}{sequence}_best_model_gpu_optimized.zip')\n",
    "train_env.save(f'{saving_path}{sequence}_vec_normalize_gpu_optimized.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SB5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
