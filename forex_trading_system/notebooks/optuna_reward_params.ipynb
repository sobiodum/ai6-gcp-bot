{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from dataclasses import dataclass\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from optuna.integration.tensorboard import TensorBoardCallback\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "\n",
    "import os, sys\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "from trading.environments.forex_env2_flat import ForexTradingEnv\n",
    "\n",
    "from utils.logging_utils import setup_logging, get_logger\n",
    "setup_logging()\n",
    "logger = get_logger('optuna_optimize_window_size')\n",
    "\n",
    "pair = \"EUR_USD\"\n",
    "best_eur_df_so_far = '/Volumes/ssd_fat2/ai6_trading_bot/datasets/5min/best_dataframes/EUR_USD_5T_indics_1H_norm.parquet'\n",
    "df = pd.read_parquet(best_eur_df_so_far)\n",
    "\n",
    "\n",
    "best_model_path = '/Volumes/ssd_fat2/ai6_trading_bot/datasets/5min/best_dataframes/optuna'\n",
    "os.makedirs(best_model_path, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "dataset_manager = DatasetManager()\n",
    "train_df, val_df, test_df = dataset_manager.split_dataset(df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)\n",
    "\n",
    "\n",
    "\n",
    "class TrialEvalCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback used for evaluating and reporting a trial to Optuna.\n",
    "    \"\"\"\n",
    "    def __init__(self, eval_env, trial, n_eval_episodes=5, eval_freq=50_000, deterministic=True, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.eval_env = eval_env\n",
    "        self.trial = trial\n",
    "        self.n_eval_episodes = n_eval_episodes\n",
    "        self.eval_freq = eval_freq\n",
    "        self.deterministic = deterministic\n",
    "\n",
    "    def _init_callback(self):\n",
    "        # Initialize variables\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def _on_step(self):\n",
    "        # Check if it's time to evaluate\n",
    "        if self.n_calls % self.eval_freq == 0:\n",
    "            # Evaluate the policy\n",
    "            mean_reward, _ = evaluate_policy(\n",
    "                self.model,\n",
    "                self.eval_env,\n",
    "                n_eval_episodes=self.n_eval_episodes,\n",
    "                deterministic=self.deterministic,\n",
    "                return_episode_rewards=False\n",
    "            )\n",
    "            \n",
    "            # Report the intermediate result to the trial\n",
    "            self.trial.report(mean_reward, self.n_calls)\n",
    "            \n",
    "            # Check if the trial should be pruned\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                # Raise prune exception\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "                \n",
    "        return True  # Continue training\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
    "    # batch_size = trial.suggest_categorical('batch_size', [64, 128, 256])\n",
    "\n",
    "\n",
    "    def make_env():\n",
    "        env = ForexTradingEnv(\n",
    "            df=train_df,\n",
    "            pair='EUR_USD',\n",
    "      \n",
    "        )\n",
    "        env = Monitor(env)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        env = VecNormalize(env, norm_obs=True, norm_reward=True, epsilon=1e-08)\n",
    "        return env\n",
    "\n",
    "    train_env = make_env()\n",
    "\n",
    "    # Create the validation environment\n",
    "    def make_eval_env():\n",
    "        env = ForexTradingEnv(\n",
    "            df=val_df,\n",
    "            pair='EUR_USD',\n",
    "   \n",
    "        )\n",
    "        env = Monitor(env)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        env = VecNormalize(env, norm_obs=True, norm_reward=False, epsilon=1e-08)\n",
    "        env.training = False\n",
    "        return env\n",
    "\n",
    "    eval_env = make_eval_env()\n",
    "\n",
    "    eval_callback = TrialEvalCallback(\n",
    "        eval_env=eval_env,\n",
    "        trial=trial,\n",
    "        n_eval_episodes=5,\n",
    "        eval_freq=50_000,  # Adjust based on your needs\n",
    "        deterministic=True,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Initialize the model\n",
    "    model = PPO(\n",
    "        'MlpPolicy',\n",
    "        train_env,\n",
    "        learning_rate=learning_rate,\n",
    "        callback=eval_callback,\n",
    "        # batch_size=batch_size,\n",
    "        verbose=0,\n",
    "        tensorboard_log=f'./logs/optuna/tensorboard_logs/trial_{trial.number}'\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.learn(\n",
    "        total_timesteps=3_000_000,\n",
    "        callback=eval_callback\n",
    "    )\n",
    "    train_env.save(f'{best_model_path}vecnormalize_{trial.number}.pkl')\n",
    "\n",
    "\n",
    "    mean_reward, _ = evaluate_policy(model, eval_env, n_eval_episodes=5,deterministic=True)\n",
    "\n",
    "    return mean_reward\n",
    "\n",
    "\n",
    "def print_status(trial):\n",
    "    logger.info(f\"Trial {trial.number} completed with value: {trial.value}\")\n",
    "    if study.best_trial == trial:\n",
    "        logger.info(f\"New best trial: {trial.number} with value: {trial.value}\")\n",
    "\n",
    "\n",
    "tensorboard_callback = TensorBoardCallback(\n",
    "    \"./optuna_logs/\", metric_name=\"mean_reward\")\n",
    "\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(\n",
    "    logging.StreamHandler(sys.stdout))\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    storage=\"sqlite:///db.sqlite3\",\n",
    "    study_name='sequence_length2',\n",
    "    load_if_exists=True,\n",
    "    sampler=TPESampler(n_startup_trials=10),\n",
    "    pruner=MedianPruner(\n",
    "        n_startup_trials=5,    # Wait for 5 trials before pruning\n",
    "        n_warmup_steps=2,      # Wait for 2 evaluations within each trial\n",
    "        interval_steps=1  \n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "study.optimize(objective, n_trials=10, show_progress_bar=False, n_jobs=1,\n",
    "               callbacks=[tensorboard_callback])\n",
    "\n",
    "logger.info(\"Number of finished trials: \", len(study.trials))\n",
    "logger.info(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "logger.info(\"  Value: \", trial.value)\n",
    "logger.info(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    logger.info(f\"    {key}: {value}\")\n",
    "\n",
    "    # Evaluation callback\n",
    "    # eval_callback2 = EvalCallback(\n",
    "    #     eval_env,\n",
    "    #     best_model_save_path=f\"./logs/optuna/15nov/ppo_trading_model/trial_{trial.number}\",\n",
    "    #     log_path='./logs/',\n",
    "    #     eval_freq=50_000,\n",
    "    #     deterministic=True,\n",
    "    #     render=False,\n",
    "    #     n_eval_episodes=5\n",
    "    # )\n",
    "    # Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "complete code from claude\n",
    "dieser callback feeded anscheinend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "import optuna\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import numpy as np\n",
    "import logging\n",
    "import pandas as pd\n",
    "from trading.environments.forex_env2_flat import ForexTradingEnv\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "\n",
    "best_model_path = '/Volumes/ssd_fat2/ai6_trading_bot/datasets/5min/best_dataframes/optuna/'\n",
    "os.makedirs(best_model_path, exist_ok=True)\n",
    "\n",
    "pair = \"EUR_USD\"\n",
    "best_eur_df_so_far = '/Volumes/ssd_fat2/ai6_trading_bot/datasets/5min/best_dataframes/EUR_USD_5T_indics_1H_norm.parquet'\n",
    "df = pd.read_parquet(best_eur_df_so_far)\n",
    "\n",
    "dataset_manager = DatasetManager()\n",
    "train_df, val_df, test_df = dataset_manager.split_dataset(df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)\n",
    "\n",
    "class ForexTrialEvalCallback(BaseCallback):\n",
    "    \"\"\"Callback for evaluating and reporting trial results to Optuna.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env,\n",
    "        trial: optuna.Trial,\n",
    "        eval_freq: int = 50_000,\n",
    "        deterministic: bool = True,\n",
    "        verbose: int = 0\n",
    "    ):\n",
    "        super().__init__(verbose)\n",
    "        self.eval_env = eval_env  # We'll set this later\n",
    "        self.trial = trial\n",
    "        self.eval_freq = eval_freq\n",
    "        self.deterministic = deterministic\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"Called after each training step.\"\"\"\n",
    "        if self.n_calls % self.eval_freq == 0:\n",
    "            # Use the training environment's VecNormalize statistics\n",
    "            self.eval_env = self.training_env\n",
    "            self.eval_env.training = False\n",
    "            self.eval_env.norm_reward = False\n",
    "            \n",
    "            mean_reward, _ = evaluate_policy(\n",
    "                self.model,\n",
    "                self.eval_env,\n",
    "                n_eval_episodes=5,  # Increased from 1\n",
    "                deterministic=self.deterministic\n",
    "            )\n",
    "\n",
    "            # Report the result to Optuna\n",
    "            self.trial.report(mean_reward, self.n_calls)\n",
    "\n",
    "            # Optional: Save best model\n",
    "            if mean_reward > self.best_mean_reward:\n",
    "                self.best_mean_reward = mean_reward\n",
    "                model_save_path = f\"{best_model_path}best_model\"\n",
    "                self.model.save(model_save_path)\n",
    "                # Save VecNormalize statistics\n",
    "                vec_normalize_path = f\"{best_model_path}vec_normalize.pkl\"\n",
    "                self.eval_env.save(vec_normalize_path)\n",
    "                \n",
    "            # Check if we should prune the trial\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "            # Reset environment to training mode\n",
    "            self.eval_env.training = True\n",
    "            self.eval_env.norm_reward = True\n",
    "\n",
    "        return True\n",
    "\n",
    "def make_env(df, is_training: bool = True):\n",
    "    \"\"\"Create and wrap the forex trading environment.\"\"\"\n",
    "    def _init():\n",
    "        env = ForexTradingEnv(\n",
    "            df=df.copy(),\n",
    "            pair='EUR_USD',\n",
    "\n",
    "        )\n",
    "        return Monitor(env)\n",
    "    \n",
    "    env = DummyVecEnv([_init])\n",
    "    env = VecNormalize(\n",
    "        env,\n",
    "        norm_obs=True,\n",
    "        norm_reward=is_training,  # Only normalize rewards during training\n",
    "        clip_obs=10.0,\n",
    "        clip_reward=10.0\n",
    "    )\n",
    "    \n",
    "    if not is_training:\n",
    "        env.training = False\n",
    "        env.norm_reward = False\n",
    "        \n",
    "    return env\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    \"\"\"Optuna objective function for optimizing learning rate.\"\"\"\n",
    "    \n",
    "    # Suggest learning rate (narrowed range)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-4, log=True)\n",
    "    \n",
    "    # Calculate training parameters\n",
    "    total_timesteps = 500_000  # Adjust based on your dataset size\n",
    "    eval_freq = 50_000           # Evaluate every 50k steps\n",
    "    \n",
    "    try:\n",
    "        # Create training environment\n",
    "        train_env = make_env(train_df, is_training=True)\n",
    "        \n",
    "        # Create model\n",
    "        model = PPO(\n",
    "            \"MlpPolicy\",\n",
    "            train_env,\n",
    "            learning_rate=learning_rate,\n",
    "            verbose=0,  # Increased verbosity\n",
    "            tensorboard_log=f\"{best_model_path}tensorboard_logs/trial_{trial.number}\"\n",
    "        )\n",
    "        \n",
    "        # Setup evaluation callback\n",
    "        eval_callback = ForexTrialEvalCallback(\n",
    "            eval_env=None,  # We'll set this later\n",
    "            trial=trial,\n",
    "            eval_freq=eval_freq,\n",
    "            deterministic=True\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        model.learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            callback=eval_callback\n",
    "        )\n",
    "        \n",
    "        # Save the VecNormalize statistics\n",
    "        vec_normalize_path = f\"{best_model_path}vec_normalize.pkl\"\n",
    "        train_env.save(vec_normalize_path)\n",
    "        \n",
    "        # Load the saved VecNormalize statistics into a new evaluation environment\n",
    "        eval_env = make_env(val_df, is_training=False)\n",
    "        eval_env = VecNormalize.load(vec_normalize_path, eval_env)\n",
    "        eval_env.training = False\n",
    "        eval_env.norm_reward = False\n",
    "        \n",
    "        # Evaluate the policy\n",
    "        mean_reward, _ = evaluate_policy(\n",
    "            model,\n",
    "            eval_env,\n",
    "            n_eval_episodes=10,  # Increased from 5\n",
    "            deterministic=True\n",
    "        )\n",
    "        \n",
    "        # Clean up\n",
    "        train_env.close()\n",
    "        eval_env.close()\n",
    "        \n",
    "        return mean_reward\n",
    "        \n",
    "    except optuna.exceptions.TrialPruned:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed with error: {str(e)}\")\n",
    "        return float('-inf')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Create the study\n",
    "study = optuna.create_study(\n",
    "    study_name=\"forex_learning_rate_optimization6\",\n",
    "    storage=\"sqlite:///db.sqlite3\",\n",
    "    direction=\"maximize\",\n",
    "    load_if_exists=True,\n",
    "    sampler=optuna.samplers.TPESampler(seed=42),\n",
    "    pruner=optuna.pruners.MedianPruner(\n",
    "        n_startup_trials=10,  # Wait for 10 trials before pruning\n",
    "        n_warmup_steps=5      # Wait for 5 evaluations within each trial\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=10,\n",
    "    n_jobs=1,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"\\nBest trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(f\"  Params: \")\n",
    "print(f\"    learning_rate: {trial.params['learning_rate']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SB5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
