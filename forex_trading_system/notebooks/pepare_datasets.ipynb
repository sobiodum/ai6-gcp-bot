{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "from data_management.indicator_manager import IndicatorManager\n",
    "\n",
    "dataset_manager = DatasetManager()\n",
    "\n",
    "\n",
    "end_date = datetime.now(pytz.UTC)\n",
    "\n",
    "start_date = end_date - timedelta(days=365 * 30)  # 10 years\n",
    "\n",
    "pairs = [f.replace('.parquet', '') for f in os.listdir(dataset_manager.base_path) if f.endswith(\".parquet\") and not f.startswith(\".\")]\n",
    "\n",
    "for pair in pairs:\n",
    "\n",
    "    print(f\"\\nProcessing {pair}\")\n",
    "    \n",
    "    # Load and update data\n",
    "    print(\"Loading and updating data...\")\n",
    "    df = dataset_manager.load_and_update_dataset(\n",
    "        currency_pair=pair,\n",
    "        timeframe=\"1min\",  # Load 1-minute data first\n",
    "        start_time=start_date,\n",
    "        end_time=end_date,\n",
    "        normalize=False  # No normalization at this stage\n",
    "    )\n",
    "    print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UPDATE 1 MIN DF WITH NEW PRICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "from data_management.indicator_manager import IndicatorManager\n",
    "\n",
    "dataset_manager = DatasetManager()\n",
    "\n",
    "\n",
    "end_date = datetime.now(pytz.UTC)\n",
    "\n",
    "start_date = end_date - timedelta(days=365 * 30)  # 10 years\n",
    "\n",
    "pairs = [f.replace('.parquet', '') for f in os.listdir(dataset_manager.base_path) if f.endswith(\".parquet\") and not f.startswith(\".\")]\n",
    "\n",
    "for pair in pairs:\n",
    "\n",
    "    print(f\"\\nProcessing {pair}\")\n",
    "    \n",
    "    # Load and update data\n",
    "    print(\"Loading and updating data...\")\n",
    "    df = dataset_manager.load_and_update_dataset(\n",
    "        currency_pair=pair,\n",
    "        timeframe=\"1min\",  # Load 1-minute data first\n",
    "        start_time=start_date,\n",
    "        end_time=end_date,\n",
    "        normalize=False  # No normalization at this stage\n",
    "    )\n",
    "    print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "from data_management.indicator_manager import IndicatorManager\n",
    "\n",
    "dataset_manager = DatasetManager()\n",
    "\n",
    "\n",
    "end_date = datetime.now(pytz.UTC)\n",
    "\n",
    "start_date = end_date - timedelta(days=365 * 30)  # 10 years\n",
    "\n",
    "pairs = [f.replace('.parquet', '') for f in os.listdir(dataset_manager.base_path) if f.endswith(\".parquet\") and not f.startswith(\".\")]\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pair = \"EUR_USD\"\n",
    "parquet_path = Path(\"/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h\") / f\"{pair}.parquet\"\n",
    "df = pd.read_parquet(parquet_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_management.dataset_manager import DatasetManager\n",
    "from data_management.indicator_manager import IndicatorManager\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "dataset_manager = DatasetManager()\n",
    "indicator_manager = IndicatorManager()\n",
    "\n",
    "fx_pair = 'NZD_USD'\n",
    "\n",
    "all_ticker_list = dataset_manager.get_currency_pairs()\n",
    "\n",
    "for fx_pair in all_ticker_list:\n",
    "\n",
    "    df = dataset_manager.prepare_dataset(\n",
    "        data_source=fx_pair,\n",
    "        target_timeframe=\"1h\",\n",
    "        start_time=datetime(2000, 1, 1),\n",
    "        end_time=datetime(2024, 12, 31),\n",
    "        normalize=False,\n",
    "        add_indicators=True,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "    # df_w_indicators = indicator_manager.calculate_indicators(\n",
    "    #     df= df\n",
    "    # )\n",
    "    # df_w_indicators\n",
    "    parquet_path = Path(\"/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h\") / f\"{fx_pair}.parquet\"\n",
    "    df.to_parquet(parquet_path)\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_management.dataset_manager import DatasetManager\n",
    "from data_management.indicator_manager import IndicatorManager\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "dataset_manager = DatasetManager()\n",
    "indicator_manager = IndicatorManager()\n",
    "\n",
    "fx_pair = 'EUR_USD'\n",
    "\n",
    "df = dataset_manager.prepare_dataset(\n",
    "        data_source=fx_pair,\n",
    "        target_timeframe=\"1h\",\n",
    "        start_time=datetime(2000, 1, 1),\n",
    "        end_time=datetime(2024, 12, 31),\n",
    "        normalize=True,\n",
    "        add_indicators=True,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from visualization.chart_manager import ChartManager\n",
    "chart_manager = ChartManager()\n",
    "start_time = pd.Timestamp('2024-01-01').tz_localize('UTC')\n",
    "end_time = pd.Timestamp('2024-10-31').tz_localize('UTC')\n",
    "chart_manager.chart(df, start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pair = \"EUR_USD\"\n",
    "parquet_path = Path(\"/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h\") / f\"{pair}.parquet\"\n",
    "df = pd.read_parquet(parquet_path)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from visualization.chart_manager import ChartManager\n",
    "import pandas as pd\n",
    "output_dir= \"/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h\"\n",
    "Path(output_dir)\n",
    "# Add the project root to the Python path\n",
    "pair = \"EUR_USD\"\n",
    "parquet_path = Path(\"/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h\") / f\"{pair}.parquet\"\n",
    "df = pd.read_parquet(parquet_path)\n",
    "start_time = pd.Timestamp('2024-01-01').tz_localize('UTC')\n",
    "end_time = pd.Timestamp('2024-10-31').tz_localize('UTC')\n",
    "chart_manager = ChartManager()\n",
    "# chart_manager.create_charts(df, start_time=start_time, end_time=end_time)\n",
    "chart_manager.chart(df, start_time, end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: Load pre-generated Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_management.dataset_manager import DatasetManager\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "\n",
    "dataset_manager = DatasetManager()\n",
    "\n",
    "# Test Case 1: Load from 1h parquet with reasonable date range\n",
    "pair = \"EUR_USD\"\n",
    "parquet_path = Path(\"/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h\") / f\"{pair}.parquet\"\n",
    "df_1h = pd.read_parquet(parquet_path)\n",
    "\n",
    "# Print basic info about the loaded data\n",
    "print(\"\\nLoaded data info:\")\n",
    "print(f\"Date range: {df_1h.index[0]} to {df_1h.index[-1]}\")\n",
    "print(f\"Total rows: {len(df_1h)}\")\n",
    "\n",
    "# Use a more reasonable date range (e.g., last 2 years)\n",
    "start_time = datetime.now(pytz.UTC) - timedelta(days=365*5)\n",
    "end_time = datetime.now(pytz.UTC)\n",
    "\n",
    "print(f\"\\nProcessing data from {start_time} to {end_time}\")\n",
    "\n",
    "try:\n",
    "    processed_df = dataset_manager.prepare_dataset(\n",
    "        data_source=df_1h,\n",
    "        target_timeframe=\"1h\",\n",
    "        normalize=False,\n",
    "        add_indicators=False,\n",
    "        start_time=start_time,\n",
    "        end_time=end_time\n",
    "    )\n",
    "    \n",
    "    print(\"\\nProcessed data info:\")\n",
    "    print(f\"Shape: {processed_df.shape}\")\n",
    "    print(f\"Date range: {processed_df.index[0]} to {processed_df.index[-1]}\")\n",
    "    print(f\"Columns: {processed_df.columns.tolist()}\")\n",
    "    print(\"\\nSample data:\")\n",
    "    print(processed_df.tail().round(4))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error processing data: {str(e)}\")\n",
    "\n",
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = \"EUR_USD\"\n",
    "parquet_path = Path(\"/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h\") / f\"{pair}.parquet\"\n",
    "df_1h = pd.read_parquet(parquet_path)\n",
    "df_1h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Add the project root to the Python path so we can import our modules\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "from data_management.indicator_manager import IndicatorManager\n",
    "\n",
    "dataset_manager = DatasetManager()\n",
    "\n",
    "def load_and_process_data(\n",
    "    ticker: str = \"EUR_USD\",\n",
    "    timeframe: str = \"1h\",\n",
    "    days_back: int = 30,\n",
    "\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and process data for a given ticker.\n",
    "    \n",
    "    Args:\n",
    "        ticker: Currency pair to process\n",
    "        timeframe: Timeframe for the data\n",
    "        days_back: Number of days of data to load\n",
    "    \"\"\"\n",
    "    end_time = datetime.now(pytz.UTC)\n",
    "    start_time = end_time - timedelta(days=days_back)\n",
    "    \n",
    "    print(f\"Loading and updating data for {ticker}...\")\n",
    "    print(f\"Timeframe: {timeframe}\")\n",
    "    print(f\"Date range: {start_time} to {end_time}\")\n",
    "    \n",
    "    df = dataset_manager.load_and_update_dataset(\n",
    "        currency_pair=ticker,\n",
    "        timeframe=\"1min\",  # Always load 1-min data first\n",
    "        start_time=start_time,\n",
    "        end_time=end_time,\n",
    "        normalize=False,\n",
    "    )\n",
    "    \n",
    "    print(\"no fetching, move to calculate indicators for:\", ticker)\n",
    "    \n",
    "    # Aggregate to 1-hour timeframe\n",
    "    print(f\"Move to normalize for: {ticker}\")\n",
    "    \n",
    "    # Resample to 1-hour timeframe\n",
    "    hourly_df = df.resample('1H').agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last',\n",
    "        'volume': 'sum'\n",
    "    })\n",
    "    \n",
    "    # Drop NaN values instead of forward filling\n",
    "    hourly_df.dropna(inplace=True)\n",
    "    \n",
    "    print(\"\\nData Summary:\")\n",
    "    print(f\"Data range: {hourly_df.index[0]} to {hourly_df.index[-1]}\")\n",
    "    print(f\"Total rows: {len(hourly_df)}\")\n",
    "    print(\"\\nColumns available:\")\n",
    "    print(hourly_df.columns.tolist())\n",
    "\n",
    "    output_dir = \"/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h\"\n",
    "    output_path = Path(output_dir) / f\"{ticker}.parquet\"\n",
    "    hourly_df.to_parquet(output_path)\n",
    "    \n",
    "    return hourly_df\n",
    "\n",
    "\n",
    "# In Jupyter notebook, you would use it like this:\n",
    "\n",
    "ticker_list = [\n",
    "            'GBP_CHF',\n",
    "            'GBP_JPY',\n",
    "            'EUR_CHF',\n",
    "            'EUR_JPY',\n",
    "            'USD_CHF',\n",
    "            'EUR_CAD',\n",
    "         \n",
    "            'GBP_USD',\n",
    "            'EUR_GBP',\n",
    "       \n",
    "            'USD_CAD',\n",
    "            'AUD_USD',\n",
    "            'CHF_JPY',\n",
    "            'AUD_JPY',\n",
    "            'NZD_USD',\n",
    "            'NZD_JPY',\n",
    "            \n",
    "           ]\n",
    "ticker_list2 = [\n",
    "            'EUR_USD',\n",
    "\n",
    "            \n",
    "           ]\n",
    "\n",
    "for ticker in ticker_list2:\n",
    "    load_and_process_data(ticker=ticker, timeframe=\"1h\", days_back=100_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "from data_management.dataset_manager import DatasetManager\n",
    "from data_management.indicator_manager import IndicatorManager\n",
    "\n",
    "\n",
    "def setup_logging() -> logging.Logger:\n",
    "    \"\"\"Configure logging for dataset preparation.\"\"\"\n",
    "    logger = logging.getLogger('dataset_preparation')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(\n",
    "        logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    )\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    # File handler\n",
    "    log_dir = Path('logs')\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "    file_handler = logging.FileHandler(\n",
    "        log_dir / f'dataset_preparation_{datetime.now():%Y%m%d_%H%M%S}.log'\n",
    "    )\n",
    "    file_handler.setFormatter(\n",
    "        logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    )\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "\n",
    "def prepare_datasets(\n",
    "    output_dir: str = \"/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h\",\n",
    "    start_date: datetime = None,\n",
    "    end_date: datetime = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Prepare and save datasets for all currency pairs.\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Directory to save processed datasets\n",
    "        start_date: Start date for data (default: 10 years ago)\n",
    "        end_date: End date for data (default: now)\n",
    "    \"\"\"\n",
    "    logger = setup_logging()\n",
    "    \n",
    "    # Create output directory\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Initialize managers\n",
    "    dataset_manager = DatasetManager()\n",
    "    \n",
    "    # Set date range\n",
    "    if end_date is None:\n",
    "        end_date = datetime.now(pytz.UTC)\n",
    "    if start_date is None:\n",
    "        start_date = end_date - timedelta(days=365 * 30)  # 10 years\n",
    "    \n",
    "    # Get list of currency pairs\n",
    "    pairs = dataset_manager.get_currency_pairs()\n",
    "    logger.info(f\"Processing {len(pairs)} currency pairs from {start_date} to {end_date}\")\n",
    "    \n",
    "    # Process each pair\n",
    "    for pair in tqdm(pairs, desc=\"Processing pairs\"):\n",
    "        try:\n",
    "            logger.info(f\"\\nProcessing {pair}\")\n",
    "            \n",
    "            # Load and update data\n",
    "            logger.info(\"Loading and updating data...\")\n",
    "            df = dataset_manager.load_and_update_dataset(\n",
    "                currency_pair=pair,\n",
    "                timeframe=\"1min\",  # Load 1-minute data first\n",
    "                start_time=start_date,\n",
    "                end_time=end_date,\n",
    "                normalize=False  # No normalization at this stage\n",
    "            )\n",
    "            \n",
    "            if df.empty:\n",
    "                logger.warning(f\"No data available for {pair}\")\n",
    "                continue\n",
    "            \n",
    "            logger.info(f\"Loaded {len(df)} rows of 1-minute data\")\n",
    "            \n",
    "            # Resample to 1-hour timeframe\n",
    "            logger.info(\"Resampling to 1-hour timeframe...\")\n",
    "            hourly_df = df.resample('1H').agg({\n",
    "                'open': 'first',\n",
    "                'high': 'max',\n",
    "                'low': 'min',\n",
    "                'close': 'last',\n",
    "                'volume': 'sum'\n",
    "            })\n",
    "            \n",
    "            # Forward fill any missing values\n",
    "            hourly_df.fillna(method='ffill', inplace=True)\n",
    "            \n",
    "            # Calculate indicators for hourly data\n",
    "            logger.info(\"Calculating technical indicators...\")\n",
    "            indicator_manager = IndicatorManager(\n",
    "                config_path=\"config\",\n",
    "                cache_dir=\"cache\"\n",
    "            )\n",
    "            hourly_df = indicator_manager.calculate_indicators(hourly_df)\n",
    "            \n",
    "            # Save processed dataset\n",
    "            output_file = output_path / f\"{pair}.parquet\"\n",
    "            hourly_df.to_parquet(output_file)\n",
    "            \n",
    "            logger.info(f\"Saved processed dataset to {output_file}\")\n",
    "            logger.info(f\"Final shape: {hourly_df.shape}\")\n",
    "            logger.info(f\"Date range: {hourly_df.index[0]} to {hourly_df.index[-1]}\")\n",
    "            logger.info(\"Columns:\")\n",
    "            for col in hourly_df.columns:\n",
    "                logger.info(f\"  - {col}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {pair}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    logger.info(\"\\nDataset preparation completed!\")\n",
    "\n",
    "\n",
    "def verify_datasets(output_dir: str) -> None:\n",
    "    \"\"\"Verify the prepared datasets.\"\"\"\n",
    "    logger = logging.getLogger('dataset_preparation')\n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    logger.info(\"\\nVerifying prepared datasets:\")\n",
    "    \n",
    "    for file in output_path.glob(\"*.parquet\"):\n",
    "        try:\n",
    "            df = pd.read_parquet(file)\n",
    "            logger.info(f\"\\n{file.name}:\")\n",
    "            logger.info(f\"  Rows: {len(df)}\")\n",
    "            logger.info(f\"  Date range: {df.index[0]} to {df.index[-1]}\")\n",
    "            logger.info(f\"  Memory usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "            \n",
    "            # Check for missing values\n",
    "            missing = df.isnull().sum()\n",
    "            if missing.any():\n",
    "                logger.warning(f\"  Missing values found in {file.name}:\")\n",
    "                for col, count in missing[missing > 0].items():\n",
    "                    logger.warning(f\"    {col}: {count} missing values\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error verifying {file}: {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parse command line arguments if needed\n",
    "    output_dir = \"/Volumes/ssd_fat2/ai6_trading_bot/datasets/1h\"\n",
    "    \n",
    "    # Prepare datasets\n",
    "    prepare_datasets(output_dir)\n",
    "    \n",
    "    # Verify prepared datasets\n",
    "    verify_datasets(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SB5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
